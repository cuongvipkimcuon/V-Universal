# ai_engine.py - AI Service, Router, Context, Rule Mining
import json
import re
from collections import defaultdict
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Tuple, Any

import streamlit as st
from openai import OpenAI

from config import Config, init_services

try:
    from core.arc_service import ArcService
    from core.reverse_lookup import ReverseLookupAssembler
except ImportError:
    ArcService = None
    ReverseLookupAssembler = None


def _get_default_tool_model() -> str:
    """Model m·∫∑c ƒë·ªãnh cho Router, Planner v√† c√°c c√¥ng c·ª• (t·ª´ Settings > AI Model)."""
    try:
        model = st.session_state.get("default_ai_model") or getattr(Config, "DEFAULT_TOOL_MODEL", None)
        return model or Config.ROUTER_MODEL
    except Exception:
        return getattr(Config, "DEFAULT_TOOL_MODEL", None) or Config.ROUTER_MODEL


# ==========================================
# ü§ñ AI SERVICE
# ==========================================
class AIService:
    """D·ªãch v·ª• AI s·ª≠ d·ª•ng OpenAI client cho OpenRouter v·ªõi c√°c t√≠nh nƒÉng n√¢ng cao"""

    @staticmethod
    @st.cache_data(ttl=3600)
    def get_available_models():
        """L·∫•y danh s√°ch model c√≥ s·∫µn t·ª´ OpenRouter"""
        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY
            )
            return Config.AVAILABLE_MODELS
        except Exception:
            return Config.AVAILABLE_MODELS

    @staticmethod
    def call_openrouter(
        messages: List[Dict],
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 8000,
        stream: bool = False,
        response_format: Optional[Dict] = None
    ) -> Any:
        """G·ªçi OpenRouter API s·ª≠ d·ª•ng OpenAI client"""
        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY,
                default_headers={
                    "HTTP-Referer": "https://v-universe.streamlit.app",
                    "X-Title": "V-Universe AI Hub"
                }
            )

            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream,
                response_format=response_format
            )

            return response
        except Exception as e:
            raise Exception(f"OpenRouter API error: {str(e)}")

    @staticmethod
    def get_embedding(text: str) -> Optional[List[float]]:
        """L·∫•y embedding t·ª´ OpenRouter"""
        if not text or not isinstance(text, str) or not text.strip():
            return None

        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY
            )

            response = client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=text
            )

            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    @staticmethod
    def get_embeddings_batch(texts: List[str], batch_size: int = 100) -> List[Optional[List[float]]]:
        """L·∫•y embedding h√†ng lo·∫°t (nhi·ªÅu text trong √≠t request). Tr·∫£ v·ªÅ list c√πng th·ª© t·ª± v·ªõi texts; ph·∫ßn t·ª≠ l·ªói l√† None."""
        if not texts:
            return []
        out: List[Optional[List[float]]] = [None] * len(texts)
        valid_indices: List[int] = []
        valid_texts: List[str] = []
        for i, t in enumerate(texts):
            if t and isinstance(t, str) and t.strip():
                valid_indices.append(i)
                valid_texts.append(t.strip())
        if not valid_texts:
            return out
        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY
            )
            for start in range(0, len(valid_texts), batch_size):
                chunk = valid_texts[start:start + batch_size]
                chunk_indices = valid_indices[start:start + batch_size]
                response = client.embeddings.create(
                    model=Config.EMBEDDING_MODEL,
                    input=chunk
                )
                for j, emb_obj in enumerate(response.data):
                    idx = chunk_indices[j] if j < len(chunk_indices) else start + j
                    if idx < len(out) and emb_obj.embedding is not None:
                        out[idx] = emb_obj.embedding
        except Exception as e:
            print(f"Embedding batch error: {e}")
        return out

    @staticmethod
    def estimate_tokens(text: str) -> int:
        """∆Ø·ªõc t√≠nh s·ªë token"""
        if not text:
            return 0
        return len(text) // 4

    @staticmethod
    def calculate_cost(
        input_tokens: int,
        output_tokens: int,
        model: str
    ) -> float:
        """T√≠nh chi ph√≠ cho request"""
        model_costs = Config.MODEL_COSTS.get(model, {"input": 0.0, "output": 0.0})

        input_cost = (input_tokens / 1_000_000) * model_costs["input"]
        output_cost = (output_tokens / 1_000_000) * model_costs["output"]

        return round(input_cost + output_cost, 6)

    @staticmethod
    def clean_json_text(text):
        """L√†m s·∫°ch markdown (```json ... ```) tr∆∞·ªõc khi parse"""
        if not text:
            return "{}"
        text = text.replace("```json", "").replace("```", "").strip()
        start = text.find("{")
        end = text.rfind("}") + 1
        if start != -1 and end != 0:
            return text[start:end]
        return text


def cap_context_to_tokens(text: str, max_tokens: int) -> Tuple[str, int]:
    """Ki·ªÉm tra v√† c·∫Øt context sao cho kh√¥ng v∆∞·ª£t qu√° max_tokens. C·∫Øt t·ª´ cu·ªëi ƒë·ªÉ gi·ªØ ph·∫ßn ƒë·∫ßu (persona, rules...)."""
    if not text or max_tokens <= 0:
        return text or "", AIService.estimate_tokens(text or "")
    est = AIService.estimate_tokens(text)
    if est <= max_tokens:
        return text, est
    # ∆Ø·ªõc t√≠nh: estimate_tokens = len//4, n√™n target_chars ‚âà max_tokens * 4
    target_chars = max_tokens * 4
    out = text[:target_chars] if len(text) > target_chars else text
    est = AIService.estimate_tokens(out)
    while est > max_tokens and len(out) > 500:
        out = out[:-500]
        est = AIService.estimate_tokens(out)
    return out, est


# Gi·ªõi h·∫°n token cho l·ªãch s·ª≠ chat ƒë∆∞a v√†o Router/Planner (tr√°nh v∆∞·ª£t context window).
ROUTER_PLANNER_CHAT_HISTORY_MAX_TOKENS = 6000


def cap_chat_history_to_tokens(text: str, max_tokens: int = ROUTER_PLANNER_CHAT_HISTORY_MAX_TOKENS) -> str:
    """C·∫Øt l·ªãch s·ª≠ chat sao cho kh√¥ng v∆∞·ª£t max_tokens; gi·ªØ ph·∫ßn ƒëu√¥i (tin nh·∫Øn g·∫ßn nh·∫•t)."""
    if not text or max_tokens <= 0:
        return text or ""
    est = AIService.estimate_tokens(text)
    if est <= max_tokens:
        return text
    # Gi·ªØ ƒëu√¥i: c·∫Øt t·ª´ ƒë·∫ßu. ∆Ø·ªõc t√≠nh ~4 k√Ω t·ª±/token.
    target_chars = max_tokens * 4
    if len(text) <= target_chars:
        return text
    out = text[-target_chars:]
    while AIService.estimate_tokens(out) > max_tokens and len(out) > 500:
        out = out[500:]
    return out


# ==========================================
# üîç HYBRID SEARCH SYSTEM (V5 - Re-ranking + lookup stats)
# ==========================================
# Tr·ªçng s·ªë re-rank: VectorSim * 0.7 + RecencyBonus * 0.1 + ImportanceBias * 0.2
VECTOR_WEIGHT = 0.7
RECENCY_WEIGHT = 0.1
IMPORTANCE_WEIGHT = 0.2
RECENCY_BONUS_HOURS = 24


def _safe_float(value: Any, default: float = 0.5) -> float:
    """L·∫•y s·ªë th·ª±c an to√†n t·ª´ record (defensive)."""
    if value is None:
        return default
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


def _recency_bonus(last_lookup_at: Any) -> float:
    """RecencyBonus: 1.0 n·∫øu last_lookup_at trong v√≤ng 24h, else 0.0."""
    if last_lookup_at is None:
        return 0.0
    try:
        if isinstance(last_lookup_at, str):
            dt = datetime.fromisoformat(last_lookup_at.replace("Z", "+00:00"))
        else:
            dt = last_lookup_at
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        delta = now - dt
        return 1.0 if delta <= timedelta(hours=RECENCY_BONUS_HOURS) else 0.0
    except Exception:
        return 0.0


def _rerank_by_score(rows: List[Dict], top_k: int) -> List[Dict]:
    """T√≠nh Final Score v√† s·∫Øp x·∫øp l·∫°i: (VectorSim*0.7) + (RecencyBonus*0.1) + (ImportanceBias*0.2)."""
    for item in rows:
        vector_sim = _safe_float(item.get("similarity") or item.get("score"), 0.5)
        vector_sim = max(0.0, min(1.0, vector_sim))
        recency = _recency_bonus(item.get("last_lookup_at"))
        importance = _safe_float(item.get("importance_bias"), 0.5)
        importance = max(0.0, min(1.0, importance))
        item["_final_score"] = (vector_sim * VECTOR_WEIGHT) + (recency * RECENCY_WEIGHT) + (importance * IMPORTANCE_WEIGHT)
    sorted_rows = sorted(rows, key=lambda x: x.get("_final_score", 0.0), reverse=True)
    for item in sorted_rows:
        item.pop("_final_score", None)
    return sorted_rows[:top_k]


def _rerank_by_score_with_breakdown(rows: List[Dict], top_k: int) -> List[Dict]:
    """Gi·ªëng _rerank_by_score nh∆∞ng gi·ªØ l·∫°i score_vector, score_recency, score_bias, score_final ƒë·ªÉ hi·ªÉn th·ªã."""
    for item in rows:
        vector_sim = _safe_float(item.get("similarity") or item.get("score"), 0.5)
        vector_sim = max(0.0, min(1.0, vector_sim))
        recency = _recency_bonus(item.get("last_lookup_at"))
        importance = _safe_float(item.get("importance_bias"), 0.5)
        importance = max(0.0, min(1.0, importance))
        item["score_vector"] = round(vector_sim * VECTOR_WEIGHT, 4)
        item["score_recency"] = round(recency * RECENCY_WEIGHT, 4)
        item["score_bias"] = round(importance * IMPORTANCE_WEIGHT, 4)
        item["score_final"] = round(
            item["score_vector"] + item["score_recency"] + item["score_bias"], 4
        )
    sorted_rows = sorted(rows, key=lambda x: x.get("score_final", 0.0), reverse=True)
    return sorted_rows[:top_k]


class HybridSearch:
    """H·ªá th·ªëng t√¨m ki·∫øm k·∫øt h·ª£p vector v√† t·ª´ kh√≥a (V5: re-ranking, lookup_count, last_lookup_at)"""

    @staticmethod
    def smart_search_hybrid_raw(
        query_text: str,
        project_id: str,
        top_k: int = 10,
        inferred_prefixes: Optional[List[str]] = None,
    ) -> List[Dict]:
        """T√¨m ki·∫øm hybrid tr·∫£ v·ªÅ raw data; re-rank trong Python. N·∫øu inferred_prefixes c√≥ gi√° tr·ªã th√¨ d√πng prefix-aware rerank."""
        try:
            services = init_services()
            supabase = services["supabase"]
            query_vec = AIService.get_embedding(query_text)
            candidate_limit = max(top_k * 3, 30)

            if query_vec:
                try:
                    response = supabase.rpc("hybrid_search", {
                        "query_text": query_text,
                        "query_embedding": query_vec,
                        "match_threshold": 0.3,
                        "match_count": candidate_limit,
                        "story_id_input": project_id,
                    }).execute()
                    raw_list = response.data if response.data else []
                except Exception:
                    raw_list = []
                if not raw_list:
                    try:
                        response = supabase.table("story_bible").select("*").eq(
                            "story_id", project_id
                        ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                            candidate_limit
                        ).execute()
                        raw_list = response.data if response.data else []
                        for item in raw_list:
                            item["similarity"] = 0.5
                    except Exception:
                        raw_list = []
            else:
                try:
                    response = supabase.table("story_bible").select("*").eq(
                        "story_id", project_id
                    ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                        candidate_limit
                    ).execute()
                    raw_list = response.data if response.data else []
                    for item in raw_list:
                        item["similarity"] = 0.5
                except Exception:
                    raw_list = []

            if not raw_list:
                return []

            if inferred_prefixes:
                reranked = _rerank_by_score_with_prefix(raw_list, top_k, inferred_prefixes)
            else:
                reranked = _rerank_by_score(raw_list, top_k)
            return reranked

        except Exception as e:
            print(f"Search error: {e}")
            return []

    @staticmethod
    def smart_search_hybrid_raw_with_scores(query_text: str, project_id: str, top_k: int = 10) -> List[Dict]:
        """Gi·ªëng smart_search_hybrid_raw nh∆∞ng m·ªói item c√≥ th√™m score_vector, score_recency, score_bias, score_final."""
        try:
            services = init_services()
            supabase = services["supabase"]
            query_vec = AIService.get_embedding(query_text)
            candidate_limit = max(top_k * 3, 30)
            if query_vec:
                try:
                    response = supabase.rpc("hybrid_search", {
                        "query_text": query_text,
                        "query_embedding": query_vec,
                        "match_threshold": 0.3,
                        "match_count": candidate_limit,
                        "story_id_input": project_id,
                    }).execute()
                    raw_list = response.data if response.data else []
                except Exception:
                    raw_list = []
                if not raw_list:
                    try:
                        response = supabase.table("story_bible").select("*").eq(
                            "story_id", project_id
                        ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                            candidate_limit
                        ).execute()
                        raw_list = response.data if response.data else []
                        for item in raw_list:
                            item["similarity"] = 0.5
                    except Exception:
                        raw_list = []
            else:
                try:
                    response = supabase.table("story_bible").select("*").eq(
                        "story_id", project_id
                    ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                        candidate_limit
                    ).execute()
                    raw_list = response.data if response.data else []
                    for item in raw_list:
                        item["similarity"] = 0.5
                except Exception:
                    raw_list = []
            if not raw_list:
                return []
            return _rerank_by_score_with_breakdown(raw_list, top_k)
        except Exception as e:
            print(f"Search error: {e}")
            return []

    @staticmethod
    def update_lookup_stats(entity_id: Any) -> None:
        """TƒÉng lookup_count += 1 v√† c·∫≠p nh·∫≠t last_lookup_at = now() cho record v·ª´a ƒë∆∞·ª£c t√¨m th·∫•y. Defensive: kh√¥ng crash n·∫øu c·ªôt ch∆∞a c√≥."""
        if entity_id is None:
            return
        try:
            services = init_services()
            if not services:
                return
            supabase = services["supabase"]
            now_iso = datetime.now(timezone.utc).isoformat()
            try:
                row = supabase.table("story_bible").select("lookup_count").eq("id", entity_id).execute()
                current = 0
                if row.data and len(row.data) > 0:
                    current = _safe_float(row.data[0].get("lookup_count"), 0.0)
                new_count = int(current) + 1
                supabase.table("story_bible").update({
                    "lookup_count": new_count,
                    "last_lookup_at": now_iso,
                }).eq("id", entity_id).execute()
            except Exception:
                pass
        except Exception as e:
            print(f"update_lookup_stats error: {e}")

    @staticmethod
    def smart_search_hybrid(query_text: str, project_id: str, top_k: int = 10) -> str:
        """Wrapper tr·∫£ v·ªÅ string context (gi·ªØ t∆∞∆°ng th√≠ch)."""
        raw_data = HybridSearch.smart_search_hybrid_raw(query_text, project_id, top_k)
        results = []
        if raw_data:
            for item in raw_data:
                name = item.get("entity_name") or ""
                desc = item.get("description") or ""
                results.append(f"- [{name}]: {desc}")
        return "\n".join(results) if results else ""


# ==========================================
# üéØ SEMANTIC INTENT (tr∆∞·ªõc Router - kh·ªõp th√¨ b·ªè qua Router)
# ==========================================
def check_semantic_intent(
    query_text: str,
    project_id: str,
    threshold: float = 0.90,
) -> Optional[Dict]:
    """So s√°nh vector c√¢u h·ªèi v·ªõi semantic_intent. N·∫øu kh·ªõp >= threshold th√¨ tr·∫£ v·ªÅ row (related_data ch√≠nh), else None. Kh√¥ng c·∫ßn intent."""
    if not query_text or not project_id:
        return None
    try:
        services = init_services()
        if not services:
            return None
        supabase = services["supabase"]
        try:
            supabase.table("semantic_intent").select("id").limit(1).execute()
        except Exception:
            return None
        try:
            r = supabase.table("settings").select("value").eq("key", "semantic_intent_threshold").execute()
            if r.data and r.data[0]:
                t = r.data[0].get("value")
                threshold = max(0.85, min(1.0, float(t) / 100.0)) if t is not None else threshold
        except Exception:
            pass
        query_vec = AIService.get_embedding(query_text)
        if not query_vec:
            return None
        rows = supabase.table("semantic_intent").select("id, question_sample, intent, related_data, embedding").eq("story_id", project_id).execute()
        data = rows.data or []
        best_match = None
        best_sim = 0.0
        for row in data:
            emb = row.get("embedding")
            if emb is None:
                continue
            if isinstance(emb, str):
                try:
                    emb = json.loads(emb)
                except Exception:
                    continue
            try:
                import math
                dot = sum(a * b for a, b in zip(query_vec, emb))
                na = math.sqrt(sum(a * a for a in query_vec))
                nb = math.sqrt(sum(b * b for b in emb))
                sim = dot / (na * nb) if na and nb else 0
                sim = (sim + 1) / 2
                if sim >= threshold and sim > best_sim:
                    best_sim = sim
                    best_match = {**row, "similarity": sim}
            except Exception:
                pass
        return best_match
    except Exception as e:
        print(f"check_semantic_intent error: {e}")
        return None


# ==========================================
# üì¶ CHUNK SEARCH (vector + text, reverse lookup)
# ==========================================
def search_chunks_vector(
    query_text: str,
    project_id: str,
    arc_id: Optional[str] = None,
    top_k: int = 10,
) -> List[Dict]:
    """T√¨m chunks theo vector (n·∫øu c√≥ embedding) ho·∫∑c text fallback. Tr·∫£ v·ªÅ list chunk rows. N·∫øu c√≥ arc_id m√† kh√¥ng c√≥ k·∫øt qu·∫£ th√¨ th·ª≠ l·∫°i kh√¥ng l·ªçc arc."""
    try:
        services = init_services()
        if not services:
            return []
        supabase = services["supabase"]
        query_vec = AIService.get_embedding(query_text)
        q = supabase.table("chunks").select("id, chapter_id, arc_id, content, raw_content, meta_json, story_id").eq("story_id", project_id)
        if arc_id:
            q = q.eq("arc_id", arc_id)
        if query_vec:
            try:
                r = supabase.rpc("hybrid_chunk_search", {
                    "query_text": query_text,
                    "query_embedding": query_vec,
                    "story_id_input": project_id,
                    "match_threshold": 0.3,
                    "match_count": top_k,
                }).execute()
                rows = list(r.data) if r.data else []
                if arc_id and not rows and query_text and query_text.strip():
                    rows = search_chunks_vector(query_text, project_id, arc_id=None, top_k=top_k)
                return rows
            except Exception:
                pass
        if query_text and query_text.strip():
            pattern = "%" + str(query_text).strip() + "%"
            r = q.ilike("content", pattern).limit(top_k).execute()
            rows = list(r.data) if r.data else []
            if arc_id and not rows:
                rows = search_chunks_vector(query_text, project_id, arc_id=None, top_k=top_k)
            return rows
        return []
    except Exception as e:
        print(f"search_chunks_vector error: {e}")
        return []


# ==========================================
# üß≠ SMART AI ROUTER SYSTEM
# ==========================================


def get_chapter_list_for_router(project_id: str) -> str:
    """
    L·∫•y ƒë·ªß danh s√°ch ch∆∞∆°ng (s·ªë - t√™n) cho project ƒë·ªÉ inject v√†o Router/Planner.
    Kh√¥ng gi·ªõi h·∫°n ƒë·ªô d√†i ‚Äî c·∫ßn ƒë·ªß ƒë·ªÉ LLM map "ch∆∞∆°ng cu·ªëi", t√™n ch∆∞∆°ng, kho·∫£ng ch∆∞∆°ng.
    """
    if not project_id:
        return "(Tr·ªëng)"
    try:
        services = init_services()
        if not services:
            return "(Tr·ªëng)"
        r = (
            services["supabase"]
            .table("chapters")
            .select("chapter_number, title")
            .eq("story_id", project_id)
            .order("chapter_number")
            .execute()
        )
        rows = list(r.data) if r.data else []
        if not rows:
            return "(Tr·ªëng)"
        parts = []
        for row in rows:
            num = row.get("chapter_number") or 0
            title = (row.get("title") or "").strip() or f"Ch∆∞∆°ng {num}"
            parts.append(f"{num} - {title}")
        return ", ".join(parts)
    except Exception:
        return "(Tr·ªëng)"


def parse_chapter_range_from_query(query: str) -> Optional[Tuple[int, int]]:
    """
    Tr√≠ch s·ªë ch∆∞∆°ng t·ª´ c√¢u h·ªèi (ch∆∞∆°ng 1, chapter 5, ch∆∞∆°ng 5 ƒë·∫øn 10, t·ª´ ch∆∞∆°ng 3 t·ªõi 7...).
    Tr·∫£ v·ªÅ (start, end) ho·∫∑c None n·∫øu kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c. D√πng cho fallback read_full_content khi search_chunks kh√¥ng c√≥ s·ªë ch∆∞∆°ng trong chunk.
    """
    if not query or not isinstance(query, str) or not query.strip():
        return None
    q = query.strip().lower()
    # Kho·∫£ng: "ch∆∞∆°ng 5 ƒë·∫øn 10", "t·ª´ ch∆∞∆°ng 3 t·ªõi 7", "chapter 2 to 5"
    range_match = re.search(
        r"(?:ch∆∞∆°ng|chapter)\s*(\d+)\s*(?:ƒë·∫øn|t·ªõi|to|-)\s*(?:ch∆∞∆°ng|chapter)?\s*(\d+)",
        q,
        re.IGNORECASE,
    )
    if range_match:
        try:
            a, b = int(range_match.group(1)), int(range_match.group(2))
            return (min(a, b), max(a, b))
        except (ValueError, IndexError):
            pass
    # M·ªôt ch∆∞∆°ng: "ch∆∞∆°ng 1", "chapter 3", "ch∆∞∆°ng 5"
    single_match = re.search(r"(?:ch∆∞∆°ng|chapter)\s*(\d+)", q, re.IGNORECASE)
    if single_match:
        try:
            n = int(single_match.group(1))
            if n >= 1:
                return (n, n)
        except (ValueError, IndexError):
            pass
    return None


def is_multi_step_update_data_request(query: str) -> bool:
    """
    B·ªô l·ªçc nh·ªè: ph√°t hi·ªán c√¢u h·ªèi c√≥ y√™u c·∫ßu 2+ thao t√°c update_data (extract/update/delete bible, relation, timeline, chunking).
    D√πng cho V6: n·∫øu True th√¨ kh√¥ng th·ª±c hi·ªán m√† c·∫£nh b√°o user b·∫≠t V7.
    """
    if not query or not isinstance(query, str):
        return False
    q = query.strip().lower()
    if len(q) < 3:
        return False
    # C·ª•m t·ª´ g·ª£i √Ω "nhi·ªÅu b∆∞·ªõc" / "t·∫•t c·∫£"
    multi_phrases = [
        "t·∫•t c·∫£",
        "to√†n b·ªô",
        "c·∫£ 4",
        "c·∫£ b·ªën",
        "full",
        "m·ªçi b∆∞·ªõc",
        "t·∫•t c·∫£ c√°c b∆∞·ªõc",
        "data analyze",  # th∆∞·ªùng hi·ªÉu l√† full pipeline
        "ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß",
        "4 b∆∞·ªõc",
        "b·ªën b∆∞·ªõc",
        "bible v√† relation",
        "relation v√† timeline",
        "timeline v√† chunk",
        "bible, relation",
        "relation, timeline",
        "extract bible v√†",
        "tr√≠ch xu·∫•t bible v√†",
        "ch·∫°y ƒë·ªß",
        "l√†m ƒë·ªß",
        "th·ª±c hi·ªán ƒë·ªß",
    ]
    for phrase in multi_phrases:
        if phrase in q:
            return True
    # "bible" + "relation" (ho·∫∑c timeline, chunking) trong c√πng c√¢u
    targets = ["bible", "relation", "timeline", "chunking"]
    found = sum(1 for t in targets if t in q)
    if found >= 2:
        return True
    return False


def is_multi_intent_request(query: str) -> bool:
    """
    B·ªô l·ªçc: ph√°t hi·ªán c√¢u h·ªèi c√≥ v·∫ª c·∫ßn nhi·ªÅu intent (nhi·ªÅu b∆∞·ªõc x·ª≠ l√Ω kh√°c nhau) ƒë·ªÉ tr·∫£ l·ªùi ƒë·ªß.
    VD: "t√≥m t·∫Øt ch∆∞∆°ng 1 r·ªìi so s√°nh v·ªõi timeline", "tr√≠ch xu·∫•t bible v√† t√¨m quan h·ªá nh√¢n v·∫≠t A".
    D√πng cho V6: hi·ªÉn th·ªã l·ªùi nh·∫Øc b·∫≠t V7 khi True.
    """
    if not query or not isinstance(query, str):
        return False
    q = query.strip().lower()
    if len(q) < 5:
        return False
    # C·ª•m g·ª£i √Ω nhi·ªÅu thao t√°c / nhi·ªÅu lo·∫°i x·ª≠ l√Ω
    multi_intent_phrases = [
        " r·ªìi ",
        " sau ƒë√≥ ",
        " xong th√¨ ",
        " xong r·ªìi ",
        " r·ªìi so s√°nh",
        " r·ªìi t√¨m ",
        " r·ªìi ki·ªÉm tra",
        " r·ªìi tr√≠ch ",
        " v√† so s√°nh",
        " v√† t√¨m ",
        " v√† ki·ªÉm tra",
        " v√† tr√≠ch ",
        " t√≥m t·∫Øt r·ªìi",
        " t√≥m t·∫Øt v√† ",
        " tr√≠ch xu·∫•t r·ªìi",
        " tr√≠ch xu·∫•t v√† ",
        " extract r·ªìi",
        " extract v√† ",
        " ki·ªÉm tra .* v√† ",  # regex nh·∫π
        " v·ª´a .* v·ª´a ",
        " ƒë·ªìng th·ªùi ",
        " k·∫øt h·ª£p v·ªõi timeline",
        " k·∫øt h·ª£p v·ªõi bible",
        " so s√°nh v·ªõi timeline",
        " ƒë·ªëi chi·∫øu v·ªõi ",
        " r·ªìi ƒë·ªëi chi·∫øu",
        " sau khi .* th√¨ ",
    ]
    for phrase in multi_intent_phrases:
        if ".*" in phrase:
            if re.search(phrase.replace(".*", r".{2,40}"), q):
                return True
        elif phrase in q:
            return True
    return False


def get_v7_reminder_message() -> str:
    """L·ªùi nh·∫Øc th·ªëng nh·∫•t khi V6 ph√°t hi·ªán c√¢u h·ªèi c·∫ßn nhi·ªÅu b∆∞·ªõc / nhi·ªÅu intent."""
    return (
        "**Y√™u c·∫ßu c·ªßa b·∫°n c√≥ v·∫ª g·ªìm nhi·ªÅu thao t√°c ho·∫∑c nhi·ªÅu b∆∞·ªõc x·ª≠ l√Ω** (nhi·ªÅu intent). "
        "Ch·∫ø ƒë·ªô V6 ch·ªâ x·ª≠ l√Ω **m·ªôt** intent m·ªói l·∫ßn. "
        "Vui l√≤ng **b·∫≠t V7 Planner** (trong c√†i ƒë·∫∑t Chat) ƒë·ªÉ th·ª±c hi·ªán nhi·ªÅu b∆∞·ªõc trong m·ªôt l·∫ßn."
    )


def extract_prefix(name: str) -> Tuple[str, str]:
    """
    B√≥c t√°ch ti·ªÅn t·ªë: t√¨m n·ªôi dung trong [...] ·ªü ƒë·∫ßu chu·ªói.
    VD: "[V≈® KH√ç] Ki·∫øm Thi√™n" -> ("V≈® KH√ç", "Ki·∫øm Thi√™n"). Defensive: l·ªói -> ("", name g·ªëc).
    """
    if not name or not isinstance(name, str):
        return "", (name or "")
    s = name.strip()
    if not s:
        return "", name
    try:
        if s.startswith("["):
            idx = s.find("]")
            if idx > 0:
                prefix = s[1:idx].strip()
                rest = s[idx + 1:].strip()
                return prefix, rest if rest else s
        return "", s
    except Exception:
        return "", s


def _estimate_tokens(text: str) -> int:
    """∆Ø·ªõc l∆∞·ª£ng s·ªë token (~4 k√Ω t·ª±/token)."""
    if not text:
        return 0
    return max(1, len(text) // 4)


# Tr·ªçng s·ªë khi re-rank c√≥ prefix: vector 0.55, recency 0.1, bias 0.2, prefix 0.15
PREFIX_WEIGHT = 0.15
VECTOR_WEIGHT_WITH_PREFIX = 0.55
RECENCY_WEIGHT_UNCHANGED = 0.1
IMPORTANCE_WEIGHT_UNCHANGED = 0.2


def get_prefix_key_from_entity_name(entity_name: str) -> str:
    """L·∫•y prefix_key (vi·∫øt HOA, kh√¥ng ngo·∫∑c) t·ª´ entity_name. VD: '[CHARACTER] John' -> 'CHARACTER'."""
    if not entity_name or not isinstance(entity_name, str):
        return "OTHER"
    prefix, _ = extract_prefix(entity_name.strip())
    return (prefix or "OTHER").strip().upper().replace(" ", "_") or "OTHER"


def _rerank_by_score_with_prefix(
    rows: List[Dict],
    top_k: int,
    inferred_prefixes: Optional[List[str]] = None,
) -> List[Dict]:
    """Re-rank v·ªõi bonus cho entry c√≥ prefix n·∫±m trong inferred_prefixes. D√πng khi Router tr·∫£ v·ªÅ inferred_prefixes."""
    if not inferred_prefixes:
        return _rerank_by_score(rows, top_k)
    normalized_inferred = {str(p).strip().upper().replace(" ", "_") for p in inferred_prefixes if p}
    for item in rows:
        vector_sim = _safe_float(item.get("similarity") or item.get("score"), 0.5)
        vector_sim = max(0.0, min(1.0, vector_sim))
        recency = _recency_bonus(item.get("last_lookup_at"))
        importance = _safe_float(item.get("importance_bias"), 0.5)
        importance = max(0.0, min(1.0, importance))
        pk = get_prefix_key_from_entity_name(item.get("entity_name") or "")
        prefix_bonus = 1.0 if pk in normalized_inferred else 0.0
        item["_final_score"] = (
            (vector_sim * VECTOR_WEIGHT_WITH_PREFIX)
            + (recency * RECENCY_WEIGHT_UNCHANGED)
            + (importance * IMPORTANCE_WEIGHT_UNCHANGED)
            + (prefix_bonus * PREFIX_WEIGHT)
        )
    sorted_rows = sorted(rows, key=lambda x: x.get("_final_score", 0.0), reverse=True)
    for item in sorted_rows:
        item.pop("_final_score", None)
    return sorted_rows[:top_k]


def _get_prefix_section_order_and_labels() -> Tuple[List[str], Dict[str, str]]:
    """L·∫•y th·ª© t·ª± v√† nh√£n section t·ª´ DB (Config.get_prefix_setup()). Tr·∫£ v·ªÅ (order, label_map)."""
    setup = Config.get_prefix_setup()
    order = []
    labels: Dict[str, str] = {}
    for p in setup:
        pk = (p.get("prefix_key") or "").strip().upper().replace(" ", "_")
        if pk:
            order.append(pk)
            labels[pk] = pk
    return order, labels


def format_bible_context_by_sections(raw_list: List[Dict]) -> str:
    """Gom k·∫øt qu·∫£ Bible theo section theo prefix; th·ª© t·ª± v√† nh√£n l·∫•y t·ª´ DB (get_prefix_setup)."""
    if not raw_list:
        return ""
    grouped: Dict[str, List[Dict]] = defaultdict(list)
    for item in raw_list:
        pk = get_prefix_key_from_entity_name(item.get("entity_name") or "")
        grouped[pk].append(item)
    order, labels = _get_prefix_section_order_and_labels()
    seen = set(order)
    for pk in grouped:
        if pk not in seen:
            order.append(pk)
            if pk not in labels:
                labels[pk] = pk
    sections = []
    for pk in order:
        items = grouped.get(pk, [])
        if not items:
            continue
        label = labels.get(pk, pk)
        block = "\n".join(
            f"- [{e.get('entity_name', '')}]: {e.get('description', '')}"
            for e in items
        )
        sections.append(f"\n--- {label} ---\n{block}")
    return "\n".join(sections).strip()


def get_bible_index(story_id: str, max_tokens: int = 2000) -> str:
    """
    Danh s√°ch th√¥ cho Router: m·ªói d√≤ng "Entity: [LO·∫†I] T√™n" (gi·ªØ nguy√™n format [PREFIX] Name).
    Top 100 theo (lookup_count + importance_bias). C√≥ parent_id th√¨ g·ª£i √Ω th·ª±c th·ªÉ g·ªëc.
    """
    if not story_id:
        return ""
    try:
        services = init_services()
        if not services:
            return ""
        supabase = services["supabase"]
        try:
            rows = (
                supabase.table("story_bible")
                .select("entity_name, lookup_count, importance_bias, parent_id")
                .eq("story_id", story_id)
                .execute()
            )
        except Exception:
            try:
                rows = (
                    supabase.table("story_bible")
                    .select("entity_name, lookup_count, importance_bias")
                    .eq("story_id", story_id)
                    .execute()
                )
            except Exception:
                return ""
        data = list(rows.data) if rows.data else []
        for r in data:
            r.setdefault("parent_id", None)
        def _score(r):
            try:
                lk = int(r.get("lookup_count") or 0)
                bi = r.get("importance_bias")
                b = float(bi) if bi is not None else 0.0
                return lk + b
            except (TypeError, ValueError):
                return 0
        data.sort(key=_score, reverse=True)
        top100 = data[:100]
        parent_ids = [r["parent_id"] for r in top100 if r.get("parent_id")]
        parent_names: Dict[Any, str] = {}
        if parent_ids:
            try:
                ids = list(set(str(pid) for pid in parent_ids if pid is not None))
                if ids:
                    pr = supabase.table("story_bible").select("id, entity_name").in_("id", ids).execute()
                    if pr.data:
                        for row in pr.data:
                            try:
                                _, disp = extract_prefix(row.get("entity_name") or "")
                                parent_names[row.get("id")] = disp.strip() or "(g·ªëc)"
                            except Exception:
                                parent_names[row.get("id")] = (row.get("entity_name") or "").strip() or "(g·ªëc)"
            except Exception:
                pass
        lines = []
        for r in top100:
            name = r.get("entity_name")
            if not name:
                continue
            line = f"Entity: {name}"
            pid = r.get("parent_id")
            if pid is not None and parent_names.get(pid):
                line += f" (g·ªëc: {parent_names[pid]})"
            lines.append(line)
        out = "\n".join(lines) if lines else ""
        if _estimate_tokens(out) > max_tokens:
            out = out[: max(100, max_tokens * 4)]
        return out
    except Exception as e:
        print(f"get_bible_index error: {e}")
        return ""


def get_bible_entries(story_id: str) -> List[Dict[str, Any]]:
    """Tr·∫£ v·ªÅ danh s√°ch entity trong Bible c·ªßa story: [{id, entity_name}, ...]. ƒê·ªÉ resolve t√™n -> id khi ƒë·ªÅ xu·∫•t quan h·ªá."""
    if not story_id:
        return []
    try:
        services = init_services()
        if not services:
            return []
        services = init_services()
        supabase = services["supabase"] if services else None
        if not supabase:
            return []
        r = (
            supabase
            .table("story_bible")
            .select("id, entity_name")
            .eq("story_id", story_id)
            .execute()
        )
        return list(r.data) if r.data else []
    except Exception:
        return []


def get_timeline_events(project_id: str, limit: int = 50) -> List[Dict[str, Any]]:
    """L·∫•y s·ª± ki·ªán timeline c·ªßa project (b·∫£ng timeline_events V7). Tr·∫£ v·ªÅ [] n·∫øu b·∫£ng ch∆∞a c√≥ ho·∫∑c l·ªói."""
    if not project_id:
        return []
    try:
        services = init_services()
        if not services:
            return []
        supabase = services["supabase"]
        r = (
            supabase.table("timeline_events")
            .select("id, event_order, title, description, raw_date, event_type, chapter_id")
            .eq("story_id", project_id)
            .order("event_order")
            .limit(limit)
            .execute()
        )
        return list(r.data) if r.data else []
    except Exception as e:
        print(f"get_timeline_events error: {e}")
        return []


def suggest_relations(content: str, story_id: str) -> List[Dict[str, Any]]:
    """
    AI qu√©t n·ªôi dung (ch∆∞∆°ng/ƒëo·∫°n) v√† so kh·ªõp v·ªõi bible_index ƒë·ªÉ ƒë·ªÅ xu·∫•t:
    - Quan h·ªá gi·ªØa hai th·ª±c th·ªÉ: Source, Target, Relation_Type, Reason -> tr·∫£ v·ªÅ kind="relation".
    - Nh√¢n v·∫≠t ti·∫øn h√≥a (1-n): th·ª±c th·ªÉ m·ªõi c√πng g·ªëc -> g·ª£i √Ω parent_id, kind="parent".
    Output: list of {
      "kind": "relation" | "parent",
      "source_entity_id", "target_entity_id", "relation_type", "description" (reason), "story_id"  (cho relation),
      ho·∫∑c "entity_id", "parent_entity_id", "reason" (cho parent).
    }
    """
    if not content or not content.strip() or not story_id:
        return []
    entries = get_bible_entries(story_id)
    if not entries:
        return []
    name_to_id = {}
    for e in entries:
        name = (e.get("entity_name") or "").strip()
        if name:
            name_to_id[name] = e.get("id")
    index_text = "\n".join([f"- {e.get('entity_name', '')}" for e in entries[:150]])
    prompt = f"""B·∫°n l√† tr·ª£ l√Ω ph√¢n t√≠ch vƒÉn b·∫£n. Cho N·ªòI DUNG v√† DANH S√ÅCH TH·ª∞C TH·ªÇ (Bible) c·ªßa m·ªôt truy·ªán.

DANH S√ÅCH TH·ª∞C TH·ªÇ (ch√≠nh x√°c t·ª´ Bible):
{index_text}

N·ªòI DUNG (ƒëo·∫°n/ch∆∞∆°ng c·∫ßn ph√¢n t√≠ch):
---
{content[:15000]}
---

Nhi·ªám v·ª•:
1) QUAN H·ªÜ: T√¨m c√°c c·∫∑p th·ª±c th·ªÉ c√≥ t∆∞∆°ng t√°c/li√™n quan trong n·ªôi dung (v√≠ d·ª•: A l√† b·∫°n c·ªßa B, X ph·∫£n b·ªôi Y). V·ªõi m·ªói c·∫∑p, tr·∫£ v·ªÅ source (t√™n ƒë√∫ng nh∆∞ trong danh s√°ch), target, relation_type (ng·∫Øn g·ªçn: b·∫°n, k·∫ª th√π, ƒë·ªìng ƒë·ªôi, y√™u, cha-con...), reason (l√Ω do ng·∫Øn).
2) NH√ÇN V·∫¨T TI·∫æN H√ìA (1-n): N·∫øu trong n·ªôi dung c√≥ th·ª±c th·ªÉ m·ªõi m√† th·ª±c ch·∫•t l√† "phi√™n b·∫£n kh√°c" c·ªßa m·ªôt th·ª±c th·ªÉ ƒë√£ c√≥ (VD: "C∆∞·ªùng l√∫c nh·ªè" / "C∆∞·ªùng l√∫c l·ªõn", c√πng m·ªôt nh√¢n v·∫≠t ·ªü hai giai ƒëo·∫°n), KH√îNG t·∫°o quan h·ªá r·ªùi r·∫°c m√† g·ª£i √Ω ƒë·∫∑t parent: entity (t√™n th·ª±c th·ªÉ con/bi·∫øn th·ªÉ) v√† parent (t√™n th·ª±c th·ªÉ g·ªëc trong danh s√°ch), k√®m reason.

Tr·∫£ v·ªÅ ƒê√öNG m·ªôt JSON object v·ªõi hai key:
- "relations": [ {{ "source": "<t√™n trong danh s√°ch>", "target": "<t√™n trong danh s√°ch>", "relation_type": "...", "reason": "..." }} ]
- "parent_suggestions": [ {{ "entity": "<t√™n con/bi·∫øn th·ªÉ trong danh s√°ch>", "parent": "<t√™n g·ªëc trong danh s√°ch>", "reason": "..." }} ]

Ch·ªâ d√πng t√™n c√≥ trong DANH S√ÅCH TH·ª∞C TH·ªÇ. N·∫øu kh√¥ng c√≥ g√¨ ph√π h·ª£p, tr·∫£ v·ªÅ "relations": [] v√† "parent_suggestions": [].
Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m."""

    try:
        response = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=_get_default_tool_model(),
            temperature=0.2,
            max_tokens=2000,
        )
        text = (response.choices[0].message.content or "").strip()
        text = re.sub(r"^```\w*\n?", "", text).strip()
        text = re.sub(r"\n?```\s*$", "", text).strip()
        data = json.loads(text)
        relations_in = data.get("relations") or []
        parent_in = data.get("parent_suggestions") or []

        def resolve_name(name: str) -> Optional[Any]:
            n = (name or "").strip()
            if n in name_to_id:
                return name_to_id[n]
            for k, vid in name_to_id.items():
                if n in k or k in n:
                    return vid
            return None

        out = []
        for r in relations_in:
            src_id = resolve_name(r.get("source") or "")
            tgt_id = resolve_name(r.get("target") or "")
            if src_id and tgt_id and src_id != tgt_id:
                out.append({
                    "kind": "relation",
                    "source_entity_id": src_id,
                    "target_entity_id": tgt_id,
                    "relation_type": (r.get("relation_type") or "li√™n quan").strip(),
                    "description": (r.get("reason") or "").strip(),
                    "story_id": story_id,
                })
        for p in parent_in:
            child_id = resolve_name(p.get("entity") or "")
            parent_id = resolve_name(p.get("parent") or "")
            if child_id and parent_id and child_id != parent_id:
                out.append({
                    "kind": "parent",
                    "entity_id": child_id,
                    "parent_entity_id": parent_id,
                    "reason": (p.get("reason") or "").strip(),
                })
        return out
    except Exception as e:
        print(f"suggest_relations error: {e}")
        return []


class SmartAIRouter:
    """B·ªô ƒë·ªãnh tuy·∫øn AI th√¥ng minh v·ªõi hybrid search v√† bible index"""

    @staticmethod
    def ai_router_pro_v2(user_prompt: str, chat_history_text: str, project_id: str = None) -> Dict:
        """Router V2: Ph√¢n t√≠ch Intent v√† Target Files, c√≥ inject bible_index ƒë·ªÉ nh·∫≠n di·ªán √Ω ƒë·ªãnh.
        chat_history_text ƒë∆∞·ª£c gi·ªõi h·∫°n token ƒë·ªÉ kh√¥ng v∆∞·ª£t context window."""
        chat_history_text = cap_chat_history_to_tokens(chat_history_text or "")
        rules_context = ""
        bible_index = ""
        prefix_setup_str = ""
        if project_id:
            rules_context = ContextManager.get_mandatory_rules(project_id)
            bible_index = get_bible_index(project_id, max_tokens=2000)
        try:
            prefix_setup = Config.get_prefix_setup()
            if prefix_setup:
                prefix_setup_str = "\n".join(
                    f"- [{p.get('prefix_key', '')}]: {p.get('description', '')}" for p in prefix_setup
                )
            else:
                prefix_setup_str = "(Ch∆∞a c·∫•u h√¨nh lo·∫°i th·ª±c th·ªÉ trong Bible Prefix / b·∫£ng bible_prefix_config.)"
        except Exception:
            prefix_setup_str = "(Ch∆∞a c·∫•u h√¨nh lo·∫°i th·ª±c th·ªÉ trong Bible Prefix.)"

        chapter_list_str = get_chapter_list_for_router(project_id) if project_id else "(Tr·ªëng)"
        filter_multi = is_multi_step_update_data_request(user_prompt) or is_multi_intent_request(user_prompt)
        router_prompt = f"""
### VAI TR√í
B·∫°n l√† AI ƒêi·ªÅu Ph·ªëi Vi√™n (Router) cho h·ªá th·ªëng V7-Universal. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch Input c·ªßa User v√† quy·∫øt ƒë·ªãnh c√¥ng c·ª• (Intent) ch√≠nh x√°c nh·∫•t ƒë·ªÉ x·ª≠ l√Ω. Ch·ªâ tr·∫£ v·ªÅ JSON.

### 1. D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO
- QUY T·∫ÆC D·ª∞ √ÅN: {rules_context}
- B·∫¢NG PREFIX ENTITY: {prefix_setup_str}
- DANH S√ÅCH ENTITY (Bible): {bible_index if bible_index else "(Tr·ªëng)"}
- DANH S√ÅCH CH∆Ø∆†NG (s·ªë - t√™n): {chapter_list_str}
- L·ªäCH S·ª¨ CHAT: {chat_history_text}
- REFERENCE (b·ªô l·ªçc nhanh): C√¢u h·ªèi c√≥ th·ªÉ c·∫ßn **nhi·ªÅu b∆∞·ªõc / nhi·ªÅu intent**: {filter_multi}. Ch·ªâ d√πng l√†m tham kh·∫£o; b·∫°n c√≥ quy·ªÅn quy·∫øt ƒë·ªãnh cu·ªëi.

### 2. B·∫¢NG QUY T·∫ÆC CH·ªåN INTENT (∆ØU TI√äN T·ª™ TR√äN XU·ªêNG)

| INTENT | ƒêI·ªÄU KI·ªÜN K√çCH HO·∫†T (TRIGGER) | T·ª™ KH√ìA NH·∫¨N DI·ªÜN |
| :--- | :--- | :--- |
| **ask_user_clarification** | C√¢u h·ªèi qu√° ng·∫Øn, m∆° h·ªì, thi·∫øu ch·ªß ng·ªØ ho·∫∑c kh√¥ng r√µ ng·ªØ c·∫£nh. | "T√≠nh ƒëi", "N√≥ l√† ai", "C√°i ƒë√≥ sao r·ªìi" (khi kh√¥ng c√≥ history). |
| **web_search** | C·∫ßn th√¥ng tin **TH·ª∞C T·∫æ, TH·ªúI GIAN TH·ª∞C** b√™n ngo√†i d·ª± √°n. | "T·ª∑ gi√°", "Gi√° v√†ng", "Th·ªùi ti·∫øt", "Tin t·ª©c", "Th√¥ng s·ªë s√∫ng Glock ngo√†i ƒë·ªùi", "m·ªõi nh·∫•t", "tra c·ª©u". |
| **numerical_calculation** | Y√™u c·∫ßu **T√çNH TO√ÅN CON S·ªê**, th·ªëng k√™, so s√°nh d·ªØ li·ªáu ƒë·ªãnh l∆∞·ª£ng. | "T√≠nh t·ªïng", "Doanh thu", "Trung b√¨nh", "ƒê·∫øm s·ªë l∆∞·ª£ng", "% tƒÉng tr∆∞·ªüng". |
| **update_data** | User y√™u c·∫ßu **thay ƒë·ªïi/ghi d·ªØ li·ªáu** h·ªá th·ªëng. G·ªìm hai nh√≥m: (1) **Ghi nh·ªõ quy t·∫Øc**: "H√£y nh·ªõ r·∫±ng...", "C·∫≠p nh·∫≠t quy t·∫Øc...", "Th√™m nh√¢n v·∫≠t..." -> data_operation_type: "remember_rule", data_operation_target: "rule", update_summary: m√¥ t·∫£. (2) **Thao t√°c theo ch∆∞∆°ng**: tr√≠ch xu·∫•t/x√≥a/c·∫≠p nh·∫≠t Bible, Relation, Timeline, Chunking theo ch∆∞∆°ng -> data_operation_type: "extract"|"update"|"delete", data_operation_target: "bible"|"relation"|"timeline"|"chunking", chapter_range. | "H√£y nh·ªõ r·∫±ng...", "Tr√≠ch xu·∫•t Bible ch∆∞∆°ng 1", "X√≥a relation ch∆∞∆°ng 2", "C·∫≠p nh·∫≠t timeline ch∆∞∆°ng 3". |
| **read_full_content** | 1. Nh·∫Øc **T√äN FILE** ho·∫∑c **S·ªê CH∆Ø∆†NG** c·ª• th·ªÉ. 2. Y√™u c·∫ßu: T√≥m t·∫Øt, Review, Vi·∫øt ti·∫øp, Ki·ªÉm tra logic to√†n b√†i. | "Ch∆∞∆°ng 1", "Chapter 5", "File luong.xlsx", "T√≥m t·∫Øt ch∆∞∆°ng n√†y". |
| **manage_timeline** | H·ªèi v·ªÅ **TH·ª® T·ª∞ TH·ªúI GIAN**, s·ª± ki·ªán tr∆∞·ªõc/sau, timeline, flashback. | "S·ª± ki·ªán n√†o tr∆∞·ªõc", "Sau khi A ch·∫øt th√¨...", "M·ªëc th·ªùi gian", "NƒÉm bao nhi√™u". |
| **query_Sql** | H·ªèi chi ti·∫øt v·ªÅ **THU·ªòC T√çNH ƒê·ªêI T∆Ø·ª¢NG** (Structure Data) trong DB. | "Nh√¢n v·∫≠t A l√† ai", "ƒê·ªãa ƒëi·ªÉm B c√≥ ƒë·∫∑c ƒëi·ªÉm g√¨". |
| **mixed_context** | C·∫ßn **C·∫¢** n·ªôi dung file/ch∆∞∆°ng **V√Ä** th√¥ng tin Bible (v·ª´a ƒëo·∫°n vƒÉn v·ª´a nh√¢n v·∫≠t/lore). | "Trong ch∆∞∆°ng 3 nh√¢n v·∫≠t A l√†m g√¨ v√† quan h·ªá v·ªõi B", "N·ªôi dung ch∆∞∆°ng 5 k·∫øt h·ª£p m√¥ t·∫£ nh√¢n v·∫≠t". |
| **search_chunks** | H·ªèi **CHI TI·∫æT V·ª§N V·∫∂T** trong vƒÉn b·∫£n nh∆∞ng **KH√îNG** nh·∫Øc s·ªë ch∆∞∆°ng c·ª• th·ªÉ. | "Ai n√≥i c√¢u...", "H√πng c·∫ßm v≈© kh√≠ g√¨", "Chi ti·∫øt c√°i √°o m√†u ƒë·ªè". |
| **search_bible** | H·ªèi v·ªÅ Lore, c·ªët truy·ªán chung, kh√°i ni·ªám, quan h·ªá nh√¢n v·∫≠t; **ho·∫∑c** user tham chi·∫øu n·ªôi dung ƒë√£ n√≥i trong chat (crystallize). | (T√™n nh√¢n v·∫≠t trong Bible), "Th·∫ø gi·ªõi n√†y v·∫≠n h√†nh sao", "Quy t·∫Øc ph√©p thu·∫≠t"; "nh∆∞ t√¥i ƒë√£ n√≥i v·ªÅ...", "ch·ªß ƒë·ªÅ tr∆∞·ªõc ƒë√≥", "ƒëo·∫°n chat tr∆∞·ªõc v·ªÅ X". |
| **suggest_v7** | C√¢u h·ªèi **r√µ r√†ng c·∫ßn 2+ intent** ho·∫∑c **2+ thao t√°c update_data** (vd: tr√≠ch xu·∫•t Bible + Relation + Timeline + Chunking; ho·∫∑c "t√≥m t·∫Øt ch∆∞∆°ng 1 r·ªìi so s√°nh timeline"). D√πng REFERENCE (b·ªô l·ªçc nhanh) l√†m g·ª£i √Ω; n·∫øu ƒë·ªìng √Ω th√¨ tr·∫£ v·ªÅ suggest_v7. | "Ch·∫°y t·∫•t c·∫£ data analyze ch∆∞∆°ng 1", "t√≥m t·∫Øt ch∆∞∆°ng 1 r·ªìi so s√°nh v·ªõi timeline", "tr√≠ch xu·∫•t bible v√† relation ch∆∞∆°ng 2". |
| **chat_casual** | Ch√†o h·ªèi x√£ giao, kh√¥ng y√™u c·∫ßu d·ªØ li·ªáu hay tra c·ª©u. | "Hello", "C·∫£m ∆°n", "B·∫°n kh·ªèe kh√¥ng". |

### 3. H∆Ø·ªöNG D·∫™N X·ª¨ L√ù ƒê·∫∂C BI·ªÜT (CRITICAL RULES)
1. **Quy t·∫Øc "Ch∆∞∆°ng C·ª• Th·ªÉ":** Khi user nh·∫Øc "Ch∆∞∆°ng X", "Chapter Y" v√† y√™u c·∫ßu **ƒë·ªçc/t√≥m t·∫Øt/xem** n·ªôi dung -> ch·ªçn `read_full_content`, tuy·ªát ƒë·ªëi KH√îNG ch·ªçn `search_chunks`. N·∫øu user **ra l·ªánh thao t√°c d·ªØ li·ªáu** (extract/update/delete Bible, Relation, Timeline, Chunking) theo ch∆∞∆°ng th√¨ ∆∞u ti√™n quy t·∫Øc 7 -> `update_data`, kh√¥ng √°p d·ª•ng "Ch∆∞∆°ng C·ª• Th·ªÉ" cho read_full_content.
2. **Quy t·∫Øc "Th·ª±c T·∫ø":** N·∫øu h·ªèi t·ª∑ gi√°, tin t·ª©c, th·ªùi ti·∫øt, gi√° v√†ng, th√¥ng s·ªë th·ª±c t·∫ø -> B·∫ÆT BU·ªòC ch·ªçn `web_search`. Tuy·ªát ƒë·ªëi KH√îNG ch·ªçn `chat_casual` hay `search_bible`.
3. **Quy t·∫Øc "L√†m R√µ":** N·∫øu kh√¥ng hi·ªÉu user mu·ªën g√¨ (c√¢u qu√° ng·∫Øn/m∆° h·ªì) -> Ch·ªçn `ask_user_clarification` v√† ƒëi·ªÅn `clarification_question`.
4. **Quy t·∫Øc "Tham chi·∫øu chat c≈©":** N·∫øu tin nh·∫Øn m·ªõi CH·ªà l√† tham chi·∫øu ƒë·∫øn l·ªánh/c√¢u h·ªèi tr∆∞·ªõc (vd: "l√†m c√°i ƒë√≥", "ok l√†m ƒëi", "nh∆∞ v·ª´a n√≥i", "th·ª±c hi·ªán ƒëi", "ƒë√∫ng r·ªìi l√†m ƒëi") th√¨ d·ª±a v√†o L·ªäCH S·ª¨ CHAT: l·∫•y l·∫°i intent v√† rewritten_query c·ªßa tin nh·∫Øn user g·∫ßn nh·∫•t c√≥ n·ªôi dung c·ª• th·ªÉ, ƒëi·ªÅn v√†o output. V√≠ d·ª•: history c√≥ "user: T√≥m t·∫Øt ch∆∞∆°ng 1" r·ªìi "model: ..." r·ªìi "user: l√†m ƒëi" -> intent v·∫´n read_full_content, rewritten_query "T√≥m t·∫Øt ch∆∞∆°ng 1".
5. **Quy t·∫Øc "Tham chi·∫øu n·ªôi dung chat (crystallize)":** N·∫øu user n√≥i ƒë√£ b√†n / ƒë√£ n√≥i v·ªÅ ch·ªß ƒë·ªÅ X trong chat (vd: "nh∆∞ t√¥i ƒë√£ n√≥i v·ªÅ nh√¢n v·∫≠t A", "ch·ªß ƒë·ªÅ tr∆∞·ªõc ƒë√≥ v·ªÅ timeline", "theo ƒëo·∫°n chat tr∆∞·ªõc v·ªÅ quy t·∫Øc") -> ch·ªçn `search_bible`. ƒêi·ªÅn `rewritten_query` l√† ch·ªß ƒë·ªÅ ho·∫∑c t·ª´ kh√≥a c·∫ßn t√¨m (vd: "nh√¢n v·∫≠t A", "timeline", "quy t·∫Øc ƒë√£ th·∫£o lu·∫≠n"). H·ªá th·ªëng s·∫Ω t√¨m trong Bible k·ªÉ c·∫£ entry [CHAT] (crystallize t·ª´ chat).
6. **Quy t·∫Øc "Nhi·ªÅu b∆∞·ªõc (suggest_v7)":** N·∫øu c√¢u h·ªèi **r√µ r√†ng** c·∫ßn th·ª±c thi 2+ intent ho·∫∑c 2+ thao t√°c update_data (vd: "ch·∫°y t·∫•t c·∫£ data analyze", "t√≥m t·∫Øt ch∆∞∆°ng 1 r·ªìi so s√°nh timeline") -> ch·ªçn `suggest_v7`, ƒëi·ªÅn `reason` gi·∫£i th√≠ch ng·∫Øn. D√πng REFERENCE (b·ªô l·ªçc nhanh) l√†m tham kh·∫£o; b·∫°n c√≥ quy·ªÅn quy·∫øt ƒë·ªãnh cu·ªëi. N·∫øu ch·ªâ m·ªôt √Ω ƒë∆°n gi·∫£n th√¨ kh√¥ng ch·ªçn suggest_v7.
7. **Quy t·∫Øc "update_data ‚Äî tr√°nh nh·∫ßm":** Ch·ªâ ch·ªçn `update_data` khi user **ra l·ªánh thay ƒë·ªïi/ghi d·ªØ li·ªáu** (th·ª±c thi thao t√°c extract/x√≥a/c·∫≠p nh·∫≠t/ghi nh·ªõ). N·∫øu user **ch·ªâ mu·ªën xem, t√≥m t·∫Øt, h·ªèi** (kh√¥ng ra l·ªánh th·ª±c thi) th√¨ KH√îNG ch·ªçn update_data: d√πng `read_full_content` n·∫øu nh·∫Øc ch∆∞∆°ng/file v√† y√™u c·∫ßu t√≥m t·∫Øt/ƒë·ªçc/tr√≠ch n·ªôi dung ƒë·ªÉ xem; d√πng `search_bible` ho·∫∑c `manage_timeline` n·∫øu h·ªèi v·ªÅ Bible/timeline. VD: "Tr√≠ch xu·∫•t n·ªôi dung ch∆∞∆°ng 1 cho t√¥i" / "C·∫≠p nh·∫≠t gi√∫p t√¥i t√¨nh ti·∫øt ch∆∞∆°ng 3" (√Ω l√† xem/t√≥m t·∫Øt) -> `read_full_content`; "Timeline ch∆∞∆°ng 1 c√≥ nh·ªØng s·ª± ki·ªán g√¨" -> `manage_timeline` ho·∫∑c `read_full_content`; "Tr√≠ch xu·∫•t Bible ch∆∞∆°ng 1" (√Ω l√† ch·∫°y pipeline extract) -> `update_data`.
8. **Quy t·∫Øc "Tra c·ª©u":** "Tra c·ª©u" ƒëi v·ªõi t·ª∑ gi√°, tin t·ª©c, th·ªùi ti·∫øt, gi√° v√†ng, th√¥ng s·ªë th·ª±c t·∫ø -> `web_search`. "Tra c·ª©u" ƒëi v·ªõi n·ªôi dung d·ª± √°n (nh√¢n v·∫≠t, ch∆∞∆°ng, truy·ªán, lore) -> `search_bible` ho·∫∑c `read_full_content`, KH√îNG ch·ªçn web_search.
9. **Quy t·∫Øc "query_Sql vs search_bible":** `query_Sql` khi h·ªèi **thu·ªôc t√≠nh c·∫•u tr√∫c** trong DB (tr∆∞·ªùng, ƒë·ªëi t∆∞·ª£ng d·ªØ li·ªáu). `search_bible` khi h·ªèi **m√¥ t·∫£, lore, quan h·ªá nh√¢n v·∫≠t** (k·ªÉ c·∫£ c√≥ t√™n nh√¢n v·∫≠t). VD: "Nh√¢n v·∫≠t A c√≥ tr∆∞·ªùng parent_id kh√¥ng" -> query_Sql; "Nh√¢n v·∫≠t A l√† ai" (√Ω h·ªèi m√¥ t·∫£/lai l·ªãch) -> search_bible.
10. **Quy t·∫Øc "mixed_context vs read_full_content":** C·∫ßn **c·∫£** n·ªôi dung ch∆∞∆°ng **v√†** th√¥ng tin Bible/quan h·ªá (v·ª´a ƒë·ªçc ch∆∞∆°ng v·ª´a h·ªèi nh√¢n v·∫≠t/quan h·ªá trong ch∆∞∆°ng ƒë√≥) -> `mixed_context`. Ch·ªâ ƒë·ªçc/t√≥m t·∫Øt ch∆∞∆°ng, kh√¥ng ƒë√≤i h·ªèi k·∫øt h·ª£p tra Bible -> `read_full_content`. VD: "Trong ch∆∞∆°ng 3 nh√¢n v·∫≠t A l√†m g√¨ v√† quan h·ªá v·ªõi B" -> mixed_context; "T√≥m t·∫Øt ch∆∞∆°ng 3" -> read_full_content.

### 4. LOGIC TR√çCH XU·∫§T CHAPTER RANGE
- "Ch∆∞∆°ng 1", "Chap 5" -> chapter_range_mode: "range", chapter_range: [1, 1] ho·∫∑c [5, 5]
- User nh·∫Øc **T√äN CH∆Ø∆†NG** (kh·ªõp v·ªõi DANH S√ÅCH CH∆Ø∆†NG ph√≠a tr√™n): tr·∫£ v·ªÅ chapter_range [n, n] v·ªõi n = chapter_number t∆∞∆°ng ·ª©ng. VD: "ch∆∞∆°ng Kh·ªüi ƒë·∫ßu" m√† danh s√°ch c√≥ "1 - Kh·ªüi ƒë·∫ßu" -> chapter_range: [1, 1].
- "Ch∆∞∆°ng 1 ƒë·∫øn 5" -> chapter_range_mode: "range", chapter_range: [1, 5]
- "3 ch∆∞∆°ng ƒë·∫ßu", "m·∫•y ch∆∞∆°ng ƒë·∫ßu" -> chapter_range_mode: "first", chapter_range_count: 3 (ho·∫∑c s·ªë user n√≥i)
- "Ch∆∞∆°ng m·ªõi nh·∫•t", "m·∫•y ch∆∞∆°ng cu·ªëi" -> chapter_range_mode: "latest", chapter_range_count: 1 (ho·∫∑c s·ªë user n√≥i)
- Kh√¥ng li√™n quan ch∆∞∆°ng -> chapter_range: null, chapter_range_mode: null

### 5. V√ç D·ª§ MINH H·ªåA (FEW-SHOT)

**Input:** "T√≥m t·∫Øt n·ªôi dung ch∆∞∆°ng 1 cho anh."
**Output:** {{ "intent": "read_full_content", "reason": "User y√™u c·∫ßu t√≥m t·∫Øt v√† ch·ªâ ƒë·ªãnh ch∆∞∆°ng 1.", "chapter_range": [1, 1], "chapter_range_mode": "range", "rewritten_query": "T√≥m t·∫Øt ch∆∞∆°ng 1", "target_files": [], "target_bible_entities": [], "inferred_prefixes": [], "chapter_range_count": 5, "clarification_question": "", "update_summary": "" }}

**Input:** "Th·∫±ng H√πng s·ª≠ d·ª•ng lo·∫°i s√∫ng n√†o trong truy·ªán?" (Kh√¥ng nh·∫Øc ch∆∞∆°ng)
**Output:** {{ "intent": "search_chunks", "reason": "H·ªèi chi ti·∫øt c·ª• th·ªÉ v·ªÅ nh√¢n v·∫≠t H√πng, kh√¥ng r√µ v·ªã tr√≠ ch∆∞∆°ng.", "target_bible_entities": ["H√πng"], "rewritten_query": "H√πng s·ª≠ d·ª•ng s√∫ng g√¨", "target_files": [], "inferred_prefixes": [], "chapter_range": null, "chapter_range_mode": null, "chapter_range_count": 5, "clarification_question": "", "update_summary": "" }}

**Input:** "T·ª∑ gi√° USD/VND h√¥m nay bao nhi√™u?"
**Output:** {{ "intent": "web_search", "reason": "H·ªèi th√¥ng tin th·ªùi gian th·ª±c ngo√†i h·ªá th·ªëng.", "rewritten_query": "T·ª∑ gi√° USD VND h√¥m nay", "target_files": [], "target_bible_entities": [], "inferred_prefixes": [], "chapter_range": null, "chapter_range_mode": null, "chapter_range_count": 5, "clarification_question": "", "update_summary": "" }}

**Input:** "S·ª± ki·ªán H√πng g·∫∑p Th·∫£o x·∫£y ra tr∆∞·ªõc hay sau v·ª• n·ªï?"
**Output:** {{ "intent": "manage_timeline", "reason": "H·ªèi v·ªÅ th·ª© t·ª± tr∆∞·ªõc sau c·ªßa 2 s·ª± ki·ªán.", "rewritten_query": "So s√°nh th·ªùi gian s·ª± ki·ªán H√πng g·∫∑p Th·∫£o v√† v·ª• n·ªï", "target_files": [], "target_bible_entities": [], "inferred_prefixes": [], "chapter_range": null, "chapter_range_mode": null, "chapter_range_count": 5, "clarification_question": "", "update_summary": "" }}

**Input:** "T√≠nh t·ªïng doanh thu c·ªßa 3 th√°ng ƒë·∫ßu nƒÉm."
**Output:** {{ "intent": "numerical_calculation", "reason": "Y√™u c·∫ßu t√≠nh to√°n t·ªïng s·ªë li·ªáu.", "rewritten_query": "T·ªïng doanh thu 3 th√°ng ƒë·∫ßu nƒÉm", "target_files": [], "target_bible_entities": [], "inferred_prefixes": [], "chapter_range": null, "chapter_range_mode": null, "chapter_range_count": 5, "clarification_question": "", "update_summary": "" }}

**Input:** "L∆∞u √Ω quy t·∫Øc n√†y: Kh√¥ng ƒë∆∞·ª£c vi·∫øt t·∫Øt t√™n nh√¢n v·∫≠t."
**Output:** {{ "intent": "update_data", "reason": "User ra l·ªánh ghi nh·ªõ quy t·∫Øc.", "data_operation_type": "remember_rule", "data_operation_target": "rule", "update_summary": "Th√™m quy t·∫Øc c·∫•m vi·∫øt t·∫Øt t√™n nh√¢n v·∫≠t v√†o h·ªá th·ªëng.", "rewritten_query": "Ghi nh·ªõ quy t·∫Øc", "target_files": [], "target_bible_entities": [], "inferred_prefixes": [], "chapter_range": null, "chapter_range_mode": null, "chapter_range_count": 5, "clarification_question": "" }}

**Input:** "Tr√≠ch xu·∫•t Bible cho ch∆∞∆°ng Kh·ªüi ƒë·∫ßu." (gi·∫£ s·ª≠ danh s√°ch ch∆∞∆°ng c√≥ "1 - Kh·ªüi ƒë·∫ßu")
**Output:** {{ "intent": "update_data", "reason": "User y√™u c·∫ßu tr√≠ch xu·∫•t Bible theo ch∆∞∆°ng (t√™n ch∆∞∆°ng Kh·ªüi ƒë·∫ßu = ch∆∞∆°ng 1).", "data_operation_type": "extract", "data_operation_target": "bible", "chapter_range": [1, 1], "chapter_range_mode": "range", "rewritten_query": "Tr√≠ch xu·∫•t Bible ch∆∞∆°ng 1", "target_files": [], "target_bible_entities": [], "inferred_prefixes": [], "chapter_range_count": 5, "clarification_question": "", "update_summary": "" }}

### 6. INPUT C·ª¶A USER
"{user_prompt}"

### 7. OUTPUT (JSON ONLY) ‚Äî Tr·∫£ v·ªÅ ƒë√∫ng format sau, ƒë·ªß c√°c key:
{{
    "intent": "ask_user_clarification" | "web_search" | "numerical_calculation" | "update_data" | "read_full_content" | "manage_timeline" | "query_Sql" | "mixed_context" | "search_chunks" | "search_bible" | "suggest_v7" | "chat_casual",
    "target_files": [],
    "target_bible_entities": [],
    "inferred_prefixes": [],
    "reason": "L√Ω do ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát",
    "rewritten_query": "Vi·∫øt l·∫°i c√¢u h·ªèi cho search",
    "chapter_range": null ho·∫∑c [start, end],
    "chapter_range_mode": null ho·∫∑c "first" | "latest" | "range",
    "chapter_range_count": 5,
    "clarification_question": "" ho·∫∑c "C√¢u h·ªèi g·ª£i √Ω (khi intent ask_user_clarification)",
    "update_summary": "" ho·∫∑c "M√¥ t·∫£ n·ªôi dung s·∫Ω ghi (khi update_data + remember_rule)",
    "data_operation_type": "" ho·∫∑c "remember_rule" | "extract" | "update" | "delete" (khi intent update_data),
    "data_operation_target": "" ho·∫∑c "rule" | "bible" | "relation" | "timeline" | "chunking" (rule = ghi nh·ªõ quy t·∫Øc; bible/relation/timeline/chunking = thao t√°c theo ch∆∞∆°ng)
}}
"""

        messages = [
            {"role": "system", "content": "B·∫°n l√† AI Router th√¥ng minh. Ch·ªâ tr·∫£ v·ªÅ JSON."},
            {"role": "user", "content": router_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=_get_default_tool_model(),
                temperature=0.1,
                max_tokens=500,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            content = AIService.clean_json_text(content)

            result = json.loads(content)

            result.setdefault("target_files", [])
            result.setdefault("target_bible_entities", [])
            result.setdefault("inferred_prefixes", [])
            result.setdefault("rewritten_query", user_prompt)
            result.setdefault("chapter_range", None)
            result.setdefault("chapter_range_mode", None)
            result.setdefault("chapter_range_count", 5)
            result.setdefault("clarification_question", "")
            result.setdefault("update_summary", "")
            result.setdefault("data_operation_type", "")
            result.setdefault("data_operation_target", "")
            if not isinstance(result.get("inferred_prefixes"), list):
                result["inferred_prefixes"] = []
            # Ch·ªâ gi·ªØ inferred_prefixes c√≥ trong DB (get_valid_prefix_keys)
            valid_keys = Config.get_valid_prefix_keys()
            if valid_keys:
                result["inferred_prefixes"] = [
                    p for p in result["inferred_prefixes"]
                    if p and str(p).strip().upper().replace(" ", "_") in valid_keys
                ]

            return result

        except Exception as e:
            print(f"Router error: {e}")
            return {
                "intent": "chat_casual",
                "target_files": [],
                "target_bible_entities": [],
                "inferred_prefixes": [],
                "reason": f"Router error: {e}",
                "rewritten_query": user_prompt,
                "chapter_range": None,
                "chapter_range_mode": None,
                "chapter_range_count": 5,
                "clarification_question": "",
                "update_summary": "",
                "data_operation_type": "",
                "data_operation_target": "",
            }

    @staticmethod
    def get_plan_v7(user_prompt: str, chat_history_text: str, project_id: str = None) -> Dict:
        """
        V7 Agentic Planner: Tr·∫£ v·ªÅ plan (m·∫£ng b∆∞·ªõc) thay v√¨ single intent.
        Return: { "analysis": str, "plan": [ { step_id, intent, args: { query_refined, target_files, target_bible_entities, chapter_range, ... } } ], "verification_required": bool }
        N·∫øu c√¢u h·ªèi ƒë∆°n gi·∫£n -> plan 1 b∆∞·ªõc. C√¢u ph·ª©c t·∫°p (vd so s√°nh timeline + Bible) -> nhi·ªÅu b∆∞·ªõc.
        Fallback: n·∫øu parse l·ªói ho·∫∑c API tr·∫£ format c≈© (single intent) -> chuy·ªÉn th√†nh plan 1 b∆∞·ªõc.
        """
        rules_context = ""
        bible_index = ""
        prefix_setup_str = ""
        if project_id:
            rules_context = ContextManager.get_mandatory_rules(project_id)
            bible_index = get_bible_index(project_id, max_tokens=2000)
        try:
            prefix_setup = Config.get_prefix_setup()
            prefix_setup_str = "\n".join(
                f"- [{p.get('prefix_key', '')}]: {p.get('description', '')}" for p in (prefix_setup or [])
            ) if prefix_setup else "(Ch∆∞a c·∫•u h√¨nh Bible Prefix.)"
        except Exception:
            prefix_setup_str = "(Ch∆∞a c·∫•u h√¨nh Bible Prefix.)"

        # Gi·ªõi h·∫°n l·ªãch s·ª≠ chat theo token ƒë·ªÉ kh√¥ng v∆∞·ª£t context (gi·ªØ tin g·∫ßn nh·∫•t).
        chat_history_capped = cap_chat_history_to_tokens(chat_history_text or "")
        chapter_list_str = get_chapter_list_for_router(project_id) if project_id else "(Tr·ªëng)"
        planner_prompt = f"""B·∫°n l√† V7 Planner. Nhi·ªám v·ª•: ph√¢n t√≠ch c√¢u user v√† ƒë∆∞a ra K·∫æ HO·∫†CH (m·∫£ng b∆∞·ªõc) th·ª±c thi.

D·ªÆ LI·ªÜU: QUY T·∫ÆC={rules_context[:1500]} | PREFIX={prefix_setup_str[:800]} | BIBLE INDEX={bible_index[:2000] if bible_index else "(Tr·ªëng)"} | DANH S√ÅCH CH∆Ø∆†NG (s·ªë - t√™n)={chapter_list_str} | L·ªäCH S·ª¨={chat_history_capped}

INPUT USER: "{user_prompt}"

QUY T·∫ÆC (∆∞u ti√™n √°p d·ª•ng theo th·ª© t·ª± khi c√≥ xung ƒë·ªôt):
- **Tham chi·∫øu chat c≈©:** N·∫øu user ch·ªâ n√≥i ki·ªÉu x√°c nh·∫≠n/tham chi·∫øu (vd: "l√†m c√°i ƒë√≥", "ok l√†m ƒëi", "nh∆∞ v·ª´a n√≥i", "th·ª±c hi·ªán ƒëi") th√¨ d·ª±a v√†o L·ªäCH S·ª¨: l·∫•y l·∫°i √Ω ƒë·ªãnh/c√¢u h·ªèi c·ªßa tin nh·∫Øn user g·∫ßn nh·∫•t c√≥ n·ªôi dung c·ª• th·ªÉ, d√πng l√†m query_refined v√† intent t∆∞∆°ng ·ª©ng cho plan 1 b∆∞·ªõc.
- **Tham chi·∫øu n·ªôi dung chat (crystallize):** N·∫øu user n√≥i ƒë√£ b√†n/ƒë√£ n√≥i v·ªÅ ch·ªß ƒë·ªÅ X (vd: "nh∆∞ t√¥i ƒë√£ n√≥i v·ªÅ nh√¢n v·∫≠t A", "ch·ªß ƒë·ªÅ tr∆∞·ªõc ƒë√≥ v·ªÅ timeline") -> d√πng intent `search_bible`, query_refined = ch·ªß ƒë·ªÅ/t·ª´ kh√≥a c·∫ßn t√¨m (Bible g·ªìm c·∫£ entry [CHAT] crystallize).
- **Ch∆∞∆°ng c·ª• th·ªÉ ‚Äî ƒë·ªçc/t√≥m t·∫Øt:** Khi user nh·∫Øc **s·ªë ho·∫∑c t√™n ch∆∞∆°ng** v√† y√™u c·∫ßu **ƒë·ªçc, t√≥m t·∫Øt, xem** n·ªôi dung -> d√πng b∆∞·ªõc `read_full_content`, KH√îNG d√πng `search_chunks`. N·∫øu user **ra l·ªánh thao t√°c d·ªØ li·ªáu** (extract/update/delete Bible, Relation, Timeline, Chunking) theo ch∆∞∆°ng -> ∆∞u ti√™n `update_data` (xem hai rule update_data b√™n d∆∞·ªõi).
- C√¢u ƒê∆†N GI·∫¢N (m·ªôt √Ω): tr·∫£ v·ªÅ plan c√≥ 1 b∆∞·ªõc v·ªõi intent ph√π h·ª£p.
- C√¢u PH·ª®C T·∫†P (nhi·ªÅu √Ω): t√°ch th√†nh nhi·ªÅu b∆∞·ªõc. VD: "Ki·ªÉm tra th·ª© t·ª± s·ª± ki·ªán A r·ªìi so v·ªõi quy t·∫Øc Bible" -> step1: manage_timeline (l·∫•y s·ª± ki·ªán A), step2: search_bible (l·∫•y quy t·∫Øc).
- **update_data ‚Äî tr√°nh nh·∫ßm:** Ch·ªâ t·∫°o b∆∞·ªõc intent `update_data` khi user **ra l·ªánh** th·ª±c thi thao t√°c (extract/x√≥a/c·∫≠p nh·∫≠t/ghi nh·ªõ). N·∫øu user ch·ªâ mu·ªën **xem, t√≥m t·∫Øt, h·ªèi** th√¨ d√πng `read_full_content` (nh·∫Øc ch∆∞∆°ng + t√≥m t·∫Øt/ƒë·ªçc), `search_bible` ho·∫∑c `manage_timeline`, kh√¥ng d√πng `update_data`. VD: "Tr√≠ch xu·∫•t n·ªôi dung ch∆∞∆°ng 1 cho t√¥i" (√Ω xem) -> read_full_content; "Tr√≠ch xu·∫•t Bible ch∆∞∆°ng 1" (√Ω ch·∫°y pipeline) -> update_data.
- **update_data theo KHO·∫¢NG ch∆∞∆°ng (quan tr·ªçng):** Khi user y√™u c·∫ßu thao t√°c d·ªØ li·ªáu (extract/update/delete) cho **nhi·ªÅu ch∆∞∆°ng ho·∫∑c kho·∫£ng** (vd: "data analyze ch∆∞∆°ng 1-10", "tr√≠ch xu·∫•t bible v√† relation ch∆∞∆°ng 1 ƒë·∫øn 5", "ch·∫°y full pipeline ch∆∞∆°ng 2-4"), ch·ªâ t·∫°o **m·ªôt b∆∞·ªõc cho m·ªói c·∫∑p (data_operation_type, data_operation_target)** v·ªõi **chapter_range [start, end]** (m·∫£ng 2 s·ªë). KH√îNG t·∫°o m·ªôt b∆∞·ªõc ri√™ng cho t·ª´ng ch∆∞∆°ng. VD: "data analyze ch∆∞∆°ng 1-10" -> ƒë√∫ng 4 b∆∞·ªõc: (extract, bible, chapter_range [1,10]), (extract, relation, [1,10]), (extract, timeline, [1,10]), (extract, chunking, [1,10]). M·ªôt ch∆∞∆°ng l·∫ª -> chapter_range [n,n].
- **mixed_context vs read_full_content:** C·∫ßn **c·∫£** n·ªôi dung ch∆∞∆°ng **v√†** Bible/quan h·ªá (v·ª´a ƒë·ªçc ch∆∞∆°ng v·ª´a h·ªèi nh√¢n v·∫≠t/quan h·ªá) -> b∆∞·ªõc `mixed_context`. Ch·ªâ ƒë·ªçc/t√≥m t·∫Øt ch∆∞∆°ng -> `read_full_content`.
- M·ªói b∆∞·ªõc: step_id (s·ªë t·ª´ 1), intent (ƒë√∫ng t√™n: manage_timeline | numerical_calculation | read_full_content | search_chunks | search_bible | mixed_context | web_search | ask_user_clarification | update_data | query_Sql | chat_casual), args (query_refined, target_files[], target_bible_entities[], chapter_range [start,end] ho·∫∑c [n,n], chapter_range_mode, chapter_range_count, data_operation_type, data_operation_target khi intent=update_data). N·∫øu user nh·∫Øc T√äN CH∆Ø∆†NG th√¨ map theo DANH S√ÅCH CH∆Ø∆†NG v√† ƒëi·ªÅn chapter_range [n,n]. dependency: null ho·∫∑c step_id b∆∞·ªõc tr∆∞·ªõc (th∆∞·ªùng null v√¨ ch·∫°y tu·∫ßn t·ª±).
- verification_required: true n·∫øu plan c√≥ numerical_calculation, manage_timeline, ho·∫∑c b·∫•t k·ª≥ intent c·∫ßn grounding (read_full_content, search_chunks, search_bible, mixed_context, query_Sql); ng∆∞·ª£c l·∫°i false.

Tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON:
{{
  "analysis": "Gi·∫£i th√≠ch ng·∫Øn t·∫°i sao ch·ªçn c√°c b∆∞·ªõc n√†y",
  "plan": [
    {{ "step_id": 1, "intent": "t√™n_intent", "args": {{ "query_refined": "...", "target_files": [], "target_bible_entities": [], "chapter_range": null, "chapter_range_mode": null, "chapter_range_count": 5, "data_operation_type": "", "data_operation_target": "" }}, "dependency": null }}
  ],
  "verification_required": true
}}
Ch·ªâ tr·∫£ v·ªÅ JSON."""

        try:
            response = AIService.call_openrouter(
                messages=[
                    {"role": "system", "content": "B·∫°n l√† V7 Planner. Ch·ªâ tr·∫£ v·ªÅ JSON v·ªõi analysis, plan, verification_required."},
                    {"role": "user", "content": planner_prompt}
                ],
                model=_get_default_tool_model(),
                temperature=0.1,
                max_tokens=800,
                response_format={"type": "json_object"},
            )
            content = response.choices[0].message.content
            content = AIService.clean_json_text(content)
            data = json.loads(content)
        except Exception as e:
            print(f"Planner V7 error: {e}")
            single = SmartAIRouter.ai_router_pro_v2(user_prompt, chat_history_text, project_id)
            return SmartAIRouter._single_intent_to_plan(single, user_prompt)

        plan = data.get("plan")
        if not plan or not isinstance(plan, list):
            single = SmartAIRouter.ai_router_pro_v2(user_prompt, chat_history_text, project_id)
            return SmartAIRouter._single_intent_to_plan(single, user_prompt)

        analysis = data.get("analysis", "")
        verification_required = bool(data.get("verification_required", False))
        valid_intents = {"manage_timeline", "numerical_calculation", "read_full_content", "search_chunks", "search_bible", "mixed_context", "web_search", "ask_user_clarification", "update_data", "query_Sql", "chat_casual"}
        normalized_plan = []
        for i, s in enumerate(plan):
            if not isinstance(s, dict):
                continue
            intent = (s.get("intent") or "chat_casual").strip().lower()
            args = s.get("args") or {}
            if not isinstance(args, dict):
                args = {}
            if intent not in valid_intents:
                if intent in ("extract_bible", "extract_relation", "extract_timeline", "extract_chunking"):
                    target = intent.replace("extract_", "")
                    intent = "update_data"
                    args = dict(args)
                    if not args.get("data_operation_target"):
                        args["data_operation_target"] = target
                    if not args.get("data_operation_type"):
                        args["data_operation_type"] = "extract"
                else:
                    intent = "chat_casual"
            step_id = int(s.get("step_id", i + 1))
            dependency = s.get("dependency")
            normalized_plan.append({
                "step_id": step_id,
                "intent": intent,
                "args": {
                    "query_refined": args.get("query_refined") or args.get("rewritten_query") or user_prompt,
                    "target_files": args.get("target_files") if isinstance(args.get("target_files"), list) else [],
                    "target_bible_entities": args.get("target_bible_entities") if isinstance(args.get("target_bible_entities"), list) else [],
                    "chapter_range": args.get("chapter_range"),
                    "chapter_range_mode": args.get("chapter_range_mode"),
                    "chapter_range_count": args.get("chapter_range_count", 5),
                    "inferred_prefixes": args.get("inferred_prefixes") if isinstance(args.get("inferred_prefixes"), list) else [],
                    "clarification_question": args.get("clarification_question") or "",
                    "update_summary": args.get("update_summary") or "",
                    "data_operation_type": args.get("data_operation_type") or "",
                    "data_operation_target": args.get("data_operation_target") or "",
                },
                "dependency": dependency,
            })
        if not normalized_plan:
            single = SmartAIRouter.ai_router_pro_v2(user_prompt, chat_history_text, project_id)
            return SmartAIRouter._single_intent_to_plan(single, user_prompt)

        # B·∫≠t verify n·∫øu plan ch·ª©a b·∫•t k·ª≥ intent c·∫ßn numerical/timeline/grounding
        intents_need_verify = {"numerical_calculation", "manage_timeline", "read_full_content", "search_chunks", "search_bible", "mixed_context", "query_Sql"}
        if any(s.get("intent") in intents_need_verify for s in normalized_plan):
            verification_required = True

        return {
            "analysis": analysis,
            "plan": normalized_plan,
            "verification_required": verification_required,
        }

    @staticmethod
    def _single_intent_to_plan(single_router_result: Dict, user_prompt: str) -> Dict:
        """Chuy·ªÉn k·∫øt qu·∫£ router single-intent th√†nh plan 1 b∆∞·ªõc (t∆∞∆°ng th√≠ch V7)."""
        intent = single_router_result.get("intent", "chat_casual")
        return {
            "analysis": single_router_result.get("reason", ""),
            "plan": [{
                "step_id": 1,
                "intent": intent,
                "args": {
                    "query_refined": single_router_result.get("rewritten_query") or user_prompt,
                    "target_files": single_router_result.get("target_files") or [],
                    "target_bible_entities": single_router_result.get("target_bible_entities") or [],
                    "chapter_range": single_router_result.get("chapter_range"),
                    "chapter_range_mode": single_router_result.get("chapter_range_mode"),
                    "chapter_range_count": single_router_result.get("chapter_range_count", 5),
                    "inferred_prefixes": single_router_result.get("inferred_prefixes") or [],
                    "clarification_question": single_router_result.get("clarification_question") or "",
                    "update_summary": single_router_result.get("update_summary") or "",
                    "data_operation_type": single_router_result.get("data_operation_type") or "",
                    "data_operation_target": single_router_result.get("data_operation_target") or "",
                },
                "dependency": None,
            }],
            "verification_required": intent in (
                "numerical_calculation", "manage_timeline",
                "read_full_content", "search_chunks", "search_bible", "mixed_context", "query_Sql",
            ),
        }


# ==========================================
# üîÑ V7 DYNAMIC RE-PLANNING
# ==========================================
def evaluate_step_outcome(intent: str, ctx_text: str, sources: List[str]) -> Tuple[bool, str]:
    """
    ƒê√°nh gi√° b∆∞·ªõc v·ª´a ch·∫°y: c√≥ "th·∫•t b·∫°i" (kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu) c·∫ßn c√¢n nh·∫Øc re-plan kh√¥ng.
    Returns: (should_consider_replan, reason).
    """
    if not intent or intent in ("chat_casual", "ask_user_clarification", "update_data", "web_search"):
        return False, ""
    ctx_upper = (ctx_text or "").upper()
    ctx_lower = (ctx_text or "").lower()
    src_list = sources or []

    if intent == "read_full_content":
        if "--- TARGET CONTENT ---" not in ctx_text and "N·ªòI DUNG CH∆Ø∆†NG" not in ctx_text:
            return True, "read_full_content: kh√¥ng t√¨m th·∫•y file/ch∆∞∆°ng (target content tr·ªëng)"
        return False, ""

    if intent == "search_chunks":
        has_chunk = any("chunk" in s.lower() or "reverse" in s.lower() for s in src_list)
        has_fallback = "Chapter fallback" in str(src_list) or "N·ªòI DUNG CH∆Ø∆†NG" in ctx_text
        if not has_chunk and not has_fallback:
            return True, "search_chunks: kh√¥ng t√¨m th·∫•y chunk ho·∫∑c fallback ch∆∞∆°ng"
        return False, ""

    if intent == "search_bible":
        has_bible = "üìö" in str(src_list) or "KNOWLEDGE BASE" in ctx_upper or "--- " in ctx_text and "---" in ctx_text
        if not has_bible or (len(ctx_text or "") < 500 and "Bible" not in ctx_text):
            return True, "search_bible: kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu Bible"
        return False, ""

    if intent == "mixed_context":
        has_any = "üìö" in str(src_list) or "RELATED FILES" in ctx_text or "Timeline" in ctx_upper or "Chunk" in str(src_list)
        if not has_any:
            return True, "mixed_context: kh√¥ng c√≥ Bible, file, timeline hay chunk"
        return False, ""

    if intent == "manage_timeline":
        if "[TIMELINE] Ch∆∞a c√≥ d·ªØ li·ªáu" in ctx_text or "Timeline (empty)" in str(src_list):
            return True, "manage_timeline: ch∆∞a c√≥ d·ªØ li·ªáu timeline_events"
        return False, ""

    if intent == "query_Sql":
        if "KNOWLEDGE BASE (query_Sql" not in ctx_text and "üîç Query SQL" not in str(src_list):
            return True, "query_Sql: kh√¥ng c√≥ d·ªØ li·ªáu Bible/ƒë·ªëi t∆∞·ª£ng"
        return False, ""

    return False, ""


def replan_after_step(
    user_prompt: str,
    cumulative_context: str,
    step_results: List[Dict],
    step_just_done: Dict,
    outcome_reason: str,
    remaining_plan: List[Dict],
    project_id: Optional[str] = None,
) -> Tuple[str, str, List[Dict]]:
    """
    G·ªçi LLM quy·∫øt ƒë·ªãnh: continue / replace / abort sau khi m·ªôt b∆∞·ªõc th·∫•t b·∫°i (kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu).
    Returns: (action, reason, new_plan). new_plan ch·ªâ c√≥ khi action == "replace".
    """
    intent_done = step_just_done.get("intent", "chat_casual")
    args_done = step_just_done.get("args") or {}
    remaining_summary = json.dumps([{"step_id": s.get("step_id"), "intent": s.get("intent")} for s in remaining_plan], ensure_ascii=False)

    prompt_text = f"""User h·ªèi: "{user_prompt[:500]}"

V·ª´a th·ª±c thi xong b∆∞·ªõc: intent={intent_done}, args={json.dumps(args_done, ensure_ascii=False)[:300]}.
K·∫øt qu·∫£ b∆∞·ªõc n√†y: {outcome_reason} (kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu / th·∫•t b·∫°i).

Context ƒë√£ t√≠ch l≈©y (r√∫t g·ªçn): {cumulative_context[:2500]}...

K·∫ø ho·∫°ch c√≤n l·∫°i (ch∆∞a ch·∫°y): {remaining_summary}

Nhi·ªám v·ª•: Quy·∫øt ƒë·ªãnh m·ªôt trong ba:
1. **continue** ‚Äì Gi·ªØ nguy√™n plan c√≤n l·∫°i, ch·∫°y ti·∫øp (th·ª≠ b∆∞·ªõc ti·∫øp theo).
2. **replace** ‚Äì Thay th·∫ø plan c√≤n l·∫°i b·∫±ng plan m·ªõi (vd: thay "t√¨m file A" b·∫±ng "t√¨m file B", ho·∫∑c ƒë·ªïi intent kh√°c ph√π h·ª£p). Tr·∫£ v·ªÅ new_plan l√† m·∫£ng b∆∞·ªõc thay th·∫ø (format gi·ªëng plan: step_id, intent, args v·ªõi query_refined, target_files, target_bible_entities, chapter_range, ...).
3. **abort** ‚Äì D·ª´ng th·ª±c thi, kh√¥ng ch·∫°y th√™m b∆∞·ªõc; tr·∫£ l·ªùi d·ª±a tr√™n context hi·ªán c√≥.

Tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON (ch·ªâ JSON, kh√¥ng gi·∫£i th√≠ch):
{{ "action": "continue" | "replace" | "abort", "reason": "L√Ω do ng·∫Øn", "new_plan": [] }}

V·ªõi action=replace th√¨ new_plan ph·∫£i c√≥ √≠t nh·∫•t 1 b∆∞·ªõc. V·ªõi continue/abort th√¨ new_plan ƒë·ªÉ []."""

    try:
        r = AIService.call_openrouter(
            messages=[
                {"role": "system", "content": "B·∫°n l√† V7 Re-planner. Ch·ªâ tr·∫£ v·ªÅ JSON v·ªõi action, reason, new_plan."},
                {"role": "user", "content": prompt_text},
            ],
            model=_get_default_tool_model(),
            temperature=0.2,
            max_tokens=600,
            response_format={"type": "json_object"},
        )
        content = AIService.clean_json_text(r.choices[0].message.content or "{}")
        data = json.loads(content)
        action = (data.get("action") or "continue").strip().lower()
        if action not in ("continue", "replace", "abort"):
            action = "continue"
        reason = str(data.get("reason") or "").strip() or outcome_reason
        new_plan = data.get("new_plan") if isinstance(data.get("new_plan"), list) else []
        if action == "replace" and not new_plan:
            action = "continue"
            new_plan = []
        return action, reason, new_plan
    except Exception as e:
        print(f"replan_after_step error: {e}")
        return "continue", "", []


# ==========================================
# üìö CONTEXT MANAGER (V5 + V6 Arc & Reverse Lookup)
# ==========================================
class ContextManager:
    """Qu·∫£n l√Ω context cho AI v·ªõi kh·∫£ nƒÉng k·∫øt h·ª£p nhi·ªÅu ngu·ªìn. V6: Arc scoping + Triangle assembler."""

    @staticmethod
    def _build_arc_scope_context(project_id: str, current_arc_id: Optional[str], session_state: Optional[Dict] = None) -> Tuple[str, int]:
        """
        V6 MODULE 1 & 3: Build [Past Arc Summaries] + [Current Arc] for Sequential/Standalone.
        Global Bible is still injected via get_mandatory_rules and search_bible below.
        Returns (text, estimated_tokens).
        """
        if not ArcService or not current_arc_id:
            return "", 0
        arc = ArcService.get_arc(current_arc_id)
        if not arc:
            return "", 0
        parts = []
        scope = ArcService.get_scope_for_search(project_id, current_arc_id)
        if scope.get("scope_type") == ArcService.ARC_TYPE_SEQUENTIAL and scope.get("arc_summaries"):
            parts.append("[PAST ARC SUMMARIES - Timeline Inheritance]")
            for a in scope["arc_summaries"]:
                parts.append("- ARC: %s\n  Summary: %s" % (a.get("name", ""), (a.get("summary") or "").strip() or "(none)"))
            parts.append("")
        parts.append("[MACRO CONTEXT - ARC: %s]" % (arc.get("name") or "Current"))
        parts.append("Summary: %s" % ((arc.get("summary") or "").strip() or "(none)"))
        text = "\n".join(parts)
        return text, AIService.estimate_tokens(text)

    @staticmethod
    def build_context_with_chunk_reverse_lookup(
        project_id: str,
        chunk_ids: List[str],
        current_arc_id: Optional[str],
        token_limit: int = 12000,
    ) -> Tuple[str, List[str], int]:
        """
        V6 MODULE 3: Assemble context from chunk IDs using Triangle (Macro/Meso/Micro).
        Optionally prepend arc scope. Returns (full_context, sources, total_tokens).
        """
        context_parts = []
        sources = []
        total_tokens = 0
        if ArcService and current_arc_id:
            arc_scope, t = ContextManager._build_arc_scope_context(project_id, current_arc_id, None)
            if arc_scope:
                context_parts.append(arc_scope)
                total_tokens += t
        if ReverseLookupAssembler and chunk_ids:
            assembled, chunk_sources = ReverseLookupAssembler.assemble_from_chunks(chunk_ids, token_limit=token_limit)
            if assembled:
                context_parts.append("[REVERSE LOOKUP - Micro to Macro Evidence]\n" + assembled)
                total_tokens += AIService.estimate_tokens(assembled)
                sources.extend(chunk_sources)
        return "\n\n".join(context_parts), sources, total_tokens

    @staticmethod
    def get_entity_relations(entity_id: Any, project_id: str) -> str:
        """L·∫•y quan h·ªá c·ªßa entity: t·ª´ b·∫£ng entity_relations (n·∫øu c√≥) v√† c√°c bi·∫øn th·ªÉ (parent_id) t·ª´ story_bible. Tr·∫£ v·ªÅ chu·ªói d·∫°ng '> [RELATION]: ...'. Defensive: kh√¥ng crash n·∫øu b·∫£ng/ c·ªôt ch∆∞a c√≥."""
        lines = []
        try:
            services = init_services()
            if not services:
                return ""
            supabase = services["supabase"]

            try:
                rel_res = supabase.table("entity_relations").select("*").or_(
                    f"source_entity_id.eq.{entity_id},target_entity_id.eq.{entity_id}"
                ).execute()
            except Exception:
                try:
                    rel_res = supabase.table("entity_relations").select("*").or_(
                        f"entity_id.eq.{entity_id},target_entity_id.eq.{entity_id}"
                    ).execute()
                except Exception:
                    rel_res = None
            if rel_res:
                if rel_res.data:
                    id_to_name = {}
                    for r in rel_res.data:
                        eid = r.get("entity_id") or r.get("source_entity_id") or r.get("from_entity_id")
                        tid = r.get("target_entity_id") or r.get("to_entity_id")
                        if eid and eid not in id_to_name:
                            id_to_name[eid] = None
                        if tid and tid not in id_to_name:
                            id_to_name[tid] = None
                    if id_to_name:
                        sb = supabase.table("story_bible").select("id, entity_name").eq(
                            "story_id", project_id
                        ).in_("id", list(id_to_name.keys())).execute()
                        if sb.data:
                            for row in sb.data:
                                id_to_name[row.get("id")] = row.get("entity_name") or ""
                    for r in rel_res.data:
                        rel_type = r.get("relation_type") or r.get("relation") or "li√™n quan"
                        eid = r.get("entity_id") or r.get("source_entity_id") or r.get("from_entity_id")
                        tid = r.get("target_entity_id") or r.get("to_entity_id")
                        name_a = id_to_name.get(eid) if eid else ""
                        name_b = id_to_name.get(tid) if tid else ""
                        if name_a or name_b:
                            lines.append(f"> [RELATION]: {name_a or 'Entity'} l√† {rel_type} c·ªßa {name_b or 'Entity'}.")

            try:
                variants = supabase.table("story_bible").select("entity_name, description").eq(
                    "story_id", project_id
                ).eq("parent_id", entity_id).execute()
                if variants.data:
                    for v in variants.data:
                        name = v.get("entity_name") or ""
                        desc = (v.get("description") or "")[:200]
                        if name:
                            lines.append(f"> [RELATION]: Bi·∫øn th·ªÉ: {name} ‚Äî {desc}...")
            except Exception:
                pass
        except Exception as e:
            print(f"get_entity_relations error: {e}")
        return "\n".join(lines) if lines else ""

    # Gi·ªõi h·∫°n token khi load nhi·ªÅu ch∆∞∆°ng (∆∞u ti√™n summary n·∫øu v∆∞·ª£t)
    DEFAULT_CHAPTER_TOKEN_LIMIT = 60000

    @staticmethod
    def _resolve_chapter_range(
        project_id: str,
        chapter_range_mode: Optional[str],
        chapter_range_count: int,
        chapter_range: Optional[List[int]],
    ) -> Optional[Tuple[int, int]]:
        """Tr·∫£ v·ªÅ (start, end) chapter_number t·ª´ router. first/latest query DB; range d√πng tr·ª±c ti·∫øp."""
        try:
            services = init_services()
            if not services:
                return None
            supabase = services["supabase"]
            count = max(1, min(50, int(chapter_range_count) if chapter_range_count else 5))

            if chapter_range_mode == "range" and chapter_range and len(chapter_range) >= 2:
                return (int(chapter_range[0]), int(chapter_range[1]))

            if chapter_range_mode == "first":
                r = supabase.table("chapters").select("chapter_number").eq(
                    "story_id", project_id
                ).order("chapter_number").limit(1).execute()
                if r.data and len(r.data) > 0:
                    start = int(r.data[0].get("chapter_number", 1))
                    return (start, start + count - 1)
                return (1, count)

            if chapter_range_mode == "latest":
                r = supabase.table("chapters").select("chapter_number").eq(
                    "story_id", project_id
                ).order("chapter_number", desc=True).limit(1).execute()
                if r.data and len(r.data) > 0:
                    end = int(r.data[0].get("chapter_number", 1))
                    start = max(1, end - count + 1)
                    return (start, end)
                return (1, count)

        except Exception as e:
            print(f"_resolve_chapter_range error: {e}")
        return None

    @staticmethod
    def load_chapters_by_range(
        project_id: str,
        start: int,
        end: int,
        token_limit: int = 60000,
    ) -> Tuple[str, List[str]]:
        """Load ch∆∞∆°ng theo kho·∫£ng chapter_number; c√≥ summary v√† art_style; n·∫øu v∆∞·ª£t token_limit th√¨ ∆∞u ti√™n summary cho ch∆∞∆°ng c≈©, full content cho ch∆∞∆°ng ƒëang b√†n (cu·ªëi)."""
        try:
            services = init_services()
            if not services:
                return "", []
            supabase = services["supabase"]
            r = supabase.table("chapters").select("*").eq(
                "story_id", project_id
            ).gte("chapter_number", start).lte("chapter_number", end).order(
                "chapter_number"
            ).execute()
            rows = r.data if r.data else []
        except Exception as e:
            print(f"load_chapters_by_range error: {e}")
            return "", []

        full_text = ""
        loaded_sources = []
        total_tokens = 0
        focus_idx = len(rows) - 1 if rows else -1

        for i, item in enumerate(rows):
            title = item.get("title") or f"Ch∆∞∆°ng {item.get('chapter_number', i+1)}"
            content = item.get("content") or ""
            summary = item.get("summary") or ""
            art_style = item.get("art_style") or ""
            use_full = (token_limit <= 0 or total_tokens < token_limit) or (i == focus_idx)
            block = f"\n\n=== üìÑ {title} ===\n"
            if summary:
                block += f"[Summary]: {summary}\n"
            if art_style:
                block += f"[Art style]: {art_style}\n"
            if use_full and content:
                block += f"[Content]:\n{content}\n"
            elif summary and not use_full:
                block += f"(Ch·ªâ t√≥m t·∫Øt do gi·ªõi h·∫°n token.)\n"
            full_text += block
            loaded_sources.append(f"üìÑ {title}")
            total_tokens += AIService.estimate_tokens(block)

        return full_text, loaded_sources

    @staticmethod
    def load_full_content(
        file_names: List[str],
        project_id: str,
        token_limit: int = 60000,
        focus_chapter_name: Optional[str] = None,
    ) -> Tuple[str, List[str]]:
        """Load n·ªôi dung file/ch∆∞∆°ng; th√™m summary v√† art_style; n·∫øu v∆∞·ª£t token_limit th√¨ ∆∞u ti√™n summary, full content cho ch∆∞∆°ng focus."""
        if not file_names:
            return "", []

        try:
            services = init_services()
            supabase = services["supabase"]
        except Exception:
            return "", []

        full_text = ""
        loaded_sources = []
        total_tokens = 0
        rows_with_meta = []

        for name in file_names:
            try:
                res = supabase.table("chapters").select("*").eq(
                    "story_id", project_id
                ).ilike("title", f"%{name}%").execute()
            except Exception:
                res = type("Res", (), {"data": None})()

            if res.data and len(res.data) > 0:
                item = res.data[0]
                item["_name"] = name
                item["_is_focus"] = (focus_chapter_name and focus_chapter_name in (item.get("title") or ""))
                rows_with_meta.append(item)
            else:
                try:
                    res_bible = supabase.table("story_bible").select(
                        "entity_name, description"
                    ).eq("story_id", project_id).ilike("entity_name", f"%{name}%").execute()
                    if res_bible.data and len(res_bible.data) > 0:
                        item = res_bible.data[0]
                        full_text += f"\n\n=== ‚ö†Ô∏è BIBLE SUMMARY: {item.get('entity_name', name)} ===\n{item.get('description', '')}\n"
                        loaded_sources.append(f"üóÇÔ∏è {item.get('entity_name', name)} (Summary)")
                except Exception:
                    pass

        for item in rows_with_meta:
            title = item.get("title") or f"Ch∆∞∆°ng {item.get('chapter_number')}"
            content = item.get("content") or ""
            summary = item.get("summary") or ""
            art_style = item.get("art_style") or ""
            is_focus = item.get("_is_focus", False)
            use_full = token_limit <= 0 or total_tokens + AIService.estimate_tokens(content) <= token_limit or is_focus
            block = f"\n\n=== üìÑ SOURCE FILE/CHAP: {title} ===\n"
            if summary:
                block += f"[Summary]: {summary}\n"
            if art_style:
                block += f"[Art style]: {art_style}\n"
            if use_full and content:
                block += f"[Content]:\n{content}\n"
            elif summary:
                block += "(Ch·ªâ t√≥m t·∫Øt do gi·ªõi h·∫°n token.)\n"
            full_text += block
            loaded_sources.append(f"üìÑ {title}")
            total_tokens += AIService.estimate_tokens(block)

        return full_text, loaded_sources

    @staticmethod
    def get_mandatory_rules(project_id: str) -> str:
        """L·∫•y t·∫•t c·∫£ c√°c lu·∫≠t (RULE) b·∫Øt bu·ªôc"""
        try:
            services = init_services()
            supabase = services['supabase']

            res = supabase.table("story_bible") \
                .select("description") \
                .eq("story_id", project_id) \
                .ilike("entity_name", "%[RULE]%") \
                .execute()

            if res.data:
                rules_text = "\n".join([f"- {r['description']}" for r in res.data])
                return f"\nüî• --- MANDATORY RULES ---\n{rules_text}\n"
            return ""
        except Exception as e:
            print(f"Error getting rules: {e}")
            return ""

    @staticmethod
    def build_context(
        router_result: Dict,
        project_id: str,
        persona: Dict,
        strict_mode: bool = False,
        current_arc_id: Optional[str] = None,
        session_state: Optional[Dict] = None,
        free_chat_mode: bool = False,
        max_context_tokens: Optional[int] = None,
    ) -> Tuple[str, List[str], int]:
        """X√¢y d·ª±ng context t·ª´ router result. max_context_tokens: gi·ªõi h·∫°n ƒë·ªô d√†i (t·ª´ Settings Context Size); None = kh√¥ng gi·ªõi h·∫°n."""
        context_parts = []
        sources = []
        total_tokens = 0

        persona_text = f"üé≠ PERSONA: {persona['role']}\n{persona['core_instruction']}\n"
        context_parts.append(persona_text)
        total_tokens += AIService.estimate_tokens(persona_text)

        if free_chat_mode:
            rules_text = ContextManager.get_mandatory_rules(project_id)
            if rules_text:
                context_parts.append(rules_text)
                total_tokens += AIService.estimate_tokens(rules_text)
            free_instruction = "[CH·∫æ ƒê·ªò CHAT T·ª∞ DO / CHAT PHI·∫æM]\nTr·∫£ l·ªùi nh∆∞ chatbot th√¥ng th∆∞·ªùng, d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t. Kh√¥ng b·∫Øt bu·ªôc d·ª±a v√†o d·ªØ li·ªáu d·ª± √°n (Bible/chunk/file); c√≥ th·ªÉ tr·∫£ l·ªùi m·ªçi ch·ªß ƒë·ªÅ."
            context_parts.append(free_instruction)
            total_tokens += AIService.estimate_tokens(free_instruction)
            sources.append("üåê Chat t·ª± do")
            return "\n".join(context_parts), sources, total_tokens

        # V6 MODULE 1: Arc scope (Past Arc Summaries + Current Arc)
        if current_arc_id and ArcService:
            arc_scope, arc_tokens = ContextManager._build_arc_scope_context(project_id, current_arc_id, session_state)
            if arc_scope:
                context_parts.append(arc_scope)
                total_tokens += arc_tokens
                sources.append("üìê Arc Scope")

        if strict_mode:
            strict_text = """
            \n\n‚ÄºÔ∏è CH·∫æ ƒê·ªò NGHI√äM NG·∫∂T (STRICT MODE) ƒêANG B·∫¨T:
            1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong [CONTEXT].
            2. TUY·ªÜT ƒê·ªêI KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c d√πng ki·∫øn th·ª©c b√™n ngo√†i ƒë·ªÉ ƒëi·ªÅn v√†o ch·ªó tr·ªëng.
            3. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong Context, h√£y tr·∫£ l·ªùi: "D·ªØ li·ªáu d·ª± √°n ch∆∞a c√≥ th√¥ng tin n√†y."
            4. N·∫øu User h·ªèi v·ªÅ "l·ªãch s·ª≠", "c·ªët truy·ªán", h√£y ∆∞u ti√™n tr√≠ch xu·∫•t t·ª´ [KNOWLEDGE BASE].
            5. Kh√¥ng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c d·ªØ li·ªáu th·ª±c t·∫ø (fact) ch·ªâ v√¨ t√≠nh c√°ch Persona.
            """
            context_parts.append(strict_text)
            total_tokens += AIService.estimate_tokens(strict_text)

        rules_text = ContextManager.get_mandatory_rules(project_id)
        if rules_text:
            context_parts.append(rules_text)
            total_tokens += AIService.estimate_tokens(rules_text)

        intent = router_result.get("intent", "chat_casual")
        target_files = router_result.get("target_files", [])
        target_bible_entities = router_result.get("target_bible_entities", [])
        chapter_range_mode = router_result.get("chapter_range_mode")
        chapter_range_count = router_result.get("chapter_range_count", 5)
        chapter_range = router_result.get("chapter_range")

        if intent == "read_full_content":
            full_text, source_names = "", []
            range_bounds = ContextManager._resolve_chapter_range(
                project_id, chapter_range_mode, chapter_range_count, chapter_range
            )
            if range_bounds is not None:
                full_text, source_names = ContextManager.load_chapters_by_range(
                    project_id, range_bounds[0], range_bounds[1],
                    token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
                )
            if not full_text and target_files:
                full_text, source_names = ContextManager.load_full_content(
                    target_files, project_id,
                    token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
                )
            if full_text:
                context_parts.append(f"\n--- TARGET CONTENT ---\n{full_text}")
                sources.extend(source_names)
                total_tokens += AIService.estimate_tokens(full_text)

        elif intent == "search_chunks":
            # Chunk vector search + reverse lookup (chunk -> chapter -> arc)
            chunk_ids = []
            query_for_chunk = (router_result.get("rewritten_query") or (router_result.get("target_files") or [""])[0] or "").strip()
            chunk_rows = search_chunks_vector(
                query_for_chunk or "n·ªôi dung",
                project_id,
                arc_id=current_arc_id,
                top_k=8,
            )
            if chunk_rows:
                chunk_ids = [str(c.get("id")) for c in chunk_rows if c.get("id")]
            if not chunk_ids and current_arc_id and query_for_chunk:
                chunk_rows = search_chunks_vector(query_for_chunk, project_id, arc_id=None, top_k=8)
                if chunk_rows:
                    chunk_ids = [str(c.get("id")) for c in chunk_rows if c.get("id")]
            if chunk_ids and ReverseLookupAssembler:
                chunk_ctx, chunk_sources, chunk_tokens = ContextManager.build_context_with_chunk_reverse_lookup(
                    project_id, chunk_ids, current_arc_id, token_limit=8000
                )
                if chunk_ctx:
                    context_parts.append(chunk_ctx)
                    total_tokens += chunk_tokens
                    sources.extend(chunk_sources)
                    sources.append("üì¶ Chunk + Reverse Lookup")
            # Fallback: khi kh√¥ng c√≥ chunk ho·∫∑c c√¢u h·ªèi nh·∫Øc s·ªë ch∆∞∆°ng c·ª• th·ªÉ -> load n·ªôi dung ch∆∞∆°ng theo s·ªë (chunk th∆∞·ªùng kh√¥ng ch·ª©a "ch∆∞∆°ng 1" trong text)
            chapter_range_from_query = parse_chapter_range_from_query(query_for_chunk or router_result.get("rewritten_query") or "")
            if chapter_range_from_query and (not chunk_ids or not context_parts):
                full_text, source_names = ContextManager.load_chapters_by_range(
                    project_id, chapter_range_from_query[0], chapter_range_from_query[1],
                    token_limit=8000,
                )
                if full_text:
                    context_parts.append(f"\n--- üìÑ N·ªòI DUNG CH∆Ø∆†NG (fallback theo s·ªë ch∆∞∆°ng) ---\n{full_text}")
                    total_tokens += AIService.estimate_tokens(full_text)
                    sources.extend(source_names)
                    sources.append("üìÑ Chapter fallback")
            if not chunk_ids and not chapter_range_from_query:
                # Fallback: search bible
                intent = "search_bible"

        elif intent == "manage_timeline":
            events = get_timeline_events(project_id)
            if events:
                lines = ["[TIMELINE EVENTS - Th·ª© t·ª± s·ª± ki·ªán / m·ªëc th·ªùi gian]"]
                for e in events:
                    order = e.get("event_order", 0)
                    title = e.get("title", "")
                    desc = (e.get("description") or "")[:800]
                    raw_date = e.get("raw_date", "")
                    etype = e.get("event_type", "event")
                    lines.append(f"- #{order} [{etype}] {title}" + (f" (Th·ªùi ƒëi·ªÉm: {raw_date})" if raw_date else "") + f"\n  {desc}")
                block = "\n".join(lines)
                context_parts.append(block)
                total_tokens += AIService.estimate_tokens(block)
                sources.append("üìÖ Timeline Events")
            else:
                context_parts.append("[TIMELINE] Ch∆∞a c√≥ d·ªØ li·ªáu timeline_events cho d·ª± √°n n√†y. Tr·∫£ l·ªùi th√¥ng tin c√≥ trong Bible/ch∆∞∆°ng n·∫øu li√™n quan.")
                sources.append("üìÖ Timeline (empty)")

        elif intent == "web_search":
            try:
                from utils.web_search import web_search as do_web_search
                search_text = do_web_search(router_result.get("rewritten_query") or "", max_results=5)
            except Exception as ex:
                search_text = f"[WEB SEARCH] L·ªói: {ex}. Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c√≥ s·∫µn."
            context_parts.append(search_text)
            total_tokens += AIService.estimate_tokens(search_text)
            sources.append("üåê Web Search")

        elif intent == "ask_user_clarification":
            clarification_question = router_result.get("clarification_question", "") or "B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n c√¢u h·ªèi ho·∫∑c ch·ªß ƒë·ªÅ b·∫°n mu·ªën h·ªèi?"
            context_parts.append(f"[C·∫¶N L√ÄM R√ï]\nH·ªá th·ªëng c·∫ßn th√™m th√¥ng tin: {clarification_question}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn, l·ªãch s·ª± y√™u c·∫ßu user l√†m r√µ theo g·ª£i √Ω tr√™n (kh√¥ng ƒëo√°n b·ª´a).")
            sources.append("‚ùì Clarification")

        elif intent == "update_data":
            op_type = router_result.get("data_operation_type") or ""
            op_target = router_result.get("data_operation_target") or ""
            if op_target in ("bible", "relation", "timeline", "chunking"):
                ch_range = router_result.get("chapter_range")
                ch_desc = f"ch∆∞∆°ng {ch_range[0]}" if (ch_range and len(ch_range) >= 1) else "ch∆∞∆°ng"
                context_parts.append(
                    f"[C·∫¨P NH·∫¨T D·ªÆ LI·ªÜU - C·∫¶N X√ÅC NH·∫¨N]\n"
                    f"User y√™u c·∫ßu: {op_type} {op_target} cho {ch_desc}. "
                    "Thao t√°c n√†y ch·ªâ th·ª±c hi·ªán sau khi user x√°c nh·∫≠n. Tr·∫£ l·ªùi ng·∫Øn g·ªçn: n√™u r√µ thao t√°c v√† ƒë·ªëi t∆∞·ª£ng c√πng ch∆∞∆°ng, nh·∫Øc user x√°c nh·∫≠n (s·∫Ω ch·∫°y ng·∫ßm v√† xem nh∆∞ ƒë√£ ch·∫•p nh·∫≠n)."
                )
                sources.append("üì¶ Update data (thao t√°c theo ch∆∞∆°ng, pending confirm)")
            else:
                update_summary = router_result.get("update_summary", "") or "Ghi nh·ªõ / c·∫≠p nh·∫≠t d·ªØ li·ªáu theo y√™u c·∫ßu user."
                context_parts.append(f"[C·∫¨P NH·∫¨T D·ªÆ LI·ªÜU - C·∫¶N X√ÅC NH·∫¨N]\n{update_summary}\n\nThao t√°c n√†y ch·ªâ th·ª±c hi·ªán sau khi user x√°c nh·∫≠n. Tr·∫£ l·ªùi t√≥m t·∫Øt n·ªôi dung s·∫Ω ƒë∆∞·ª£c ghi v√† nh·∫Øc user x√°c nh·∫≠n tr∆∞·ªõc khi th·ª±c hi·ªán.")
                sources.append("‚úèÔ∏è Update data (ghi nh·ªõ quy t·∫Øc, pending confirm)")

        elif intent == "query_Sql":
            # D·ªØ li·ªáu ƒë·ªëi t∆∞·ª£ng (entity, thu·ªôc t√≠nh): Bible + chapters. Kh√¥ng d√πng timeline_events (ƒë√≥ l√† manage_timeline).
            rewritten = (router_result.get("rewritten_query") or "").strip() or (router_result.get("target_bible_entities") or [""])[0]
            sql_context_parts = []
            raw_list = HybridSearch.smart_search_hybrid_raw(rewritten, project_id, top_k=5) if rewritten else []
            if raw_list:
                part = format_bible_context_by_sections(raw_list)
                sql_context_parts.append(f"\n--- KNOWLEDGE BASE (query_Sql - ƒë·ªëi t∆∞·ª£ng) ---\n{part}")
            if sql_context_parts:
                block = "\n".join(sql_context_parts)
                context_parts.append(block)
                total_tokens += AIService.estimate_tokens(block)
                sources.append("üîç Query SQL")
            else:
                intent = "search_bible"

        if intent == "search_bible" or intent == "mixed_context":
            raw_inferred = router_result.get("inferred_prefixes") or []
            valid_keys = Config.get_valid_prefix_keys()
            inferred_prefixes = [
                p for p in raw_inferred
                if p and str(p).strip().upper().replace(" ", "_") in valid_keys
            ] if valid_keys else raw_inferred
            bible_context = ""
            for entity in target_bible_entities:
                raw_list = HybridSearch.smart_search_hybrid_raw(
                    entity, project_id, top_k=2, inferred_prefixes=inferred_prefixes
                )
                if raw_list:
                    for item in raw_list:
                        try:
                            eid = item.get("id")
                            if eid is not None:
                                HybridSearch.update_lookup_stats(eid)
                        except Exception:
                            pass
                    main_id = raw_list[0].get("id") if raw_list else None
                    rel_block = ""
                    if main_id:
                        rel_text = ContextManager.get_entity_relations(main_id, project_id)
                        if rel_text:
                            rel_block = f"> [RELATION]:\n{rel_text}\n\n"
                    part = format_bible_context_by_sections(raw_list)
                    bible_context += f"\n--- {entity.upper()} ---\n{rel_block}{part}\n"

            if not bible_context and router_result.get("rewritten_query"):
                raw_list = HybridSearch.smart_search_hybrid_raw(
                    router_result["rewritten_query"],
                    project_id,
                    top_k=5,
                    inferred_prefixes=inferred_prefixes,
                )
                if raw_list:
                    for item in raw_list:
                        try:
                            eid = item.get("id")
                            if eid is not None:
                                HybridSearch.update_lookup_stats(eid)
                        except Exception:
                            pass
                    main_id = raw_list[0].get("id") if raw_list else None
                    rel_block = ""
                    if main_id:
                        rel_text = ContextManager.get_entity_relations(main_id, project_id)
                        if rel_text:
                            rel_block = f"> [RELATION]:\n{rel_text}\n\n"
                    part = format_bible_context_by_sections(raw_list)
                    bible_context = f"\n--- KNOWLEDGE BASE ---\n{rel_block}{part}\n"

            if bible_context:
                context_parts.append(bible_context)
                total_tokens += AIService.estimate_tokens(bible_context)
                sources.append("üìö Bible Search")

            try:
                services = init_services()
                supabase = services['supabase']
                related_chapter_nums = set()

                if target_bible_entities:
                    for entity in target_bible_entities:
                        res = supabase.table("story_bible") \
                            .select("source_chapter") \
                            .eq("story_id", project_id) \
                            .ilike("entity_name", f"%{entity}%") \
                            .execute()

                        if res.data:
                            for row in res.data:
                                if row.get('source_chapter') and row['source_chapter'] > 0:
                                    related_chapter_nums.add(row['source_chapter'])

                if related_chapter_nums:
                    chap_res = supabase.table("chapters") \
                        .select("title") \
                        .eq("story_id", project_id) \
                        .in_("chapter_number", list(related_chapter_nums)) \
                        .execute()

                    if chap_res.data:
                        auto_files = [c['title'] for c in chap_res.data if c.get('title')]

                        if auto_files:
                            extra_text, extra_sources = ContextManager.load_full_content(auto_files, project_id)

                            if extra_text:
                                context_parts.append(f"\n--- üïµÔ∏è AUTO-DETECTED CONTEXT (REVERSE LOOKUP) ---\n{extra_text}")
                                sources.extend([f"{s} (Auto)" for s in extra_sources])
                                total_tokens += AIService.estimate_tokens(extra_text)

            except Exception as e:
                print(f"Reverse lookup error: {e}")
                pass

        if intent == "mixed_context" and target_files:
            full_text, source_names = ContextManager.load_full_content(
                target_files, project_id,
                token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
            )
            if full_text:
                context_parts.append(f"\n--- RELATED FILES ---\n{full_text}")
                sources.extend(source_names)
                total_tokens += AIService.estimate_tokens(full_text)

        # mixed_context: b·ªï sung timeline + chunks (Bible v√† file ƒë√£ c√≥ ·ªü tr√™n) ƒë·ªÉ ƒë·ªß ngu·ªìn tr·∫£ l·ªùi.
        if intent == "mixed_context":
            events = get_timeline_events(project_id, limit=30)
            if events:
                lines = ["[TIMELINE EVENTS - Th·ª© t·ª± s·ª± ki·ªán / m·ªëc th·ªùi gian]"]
                for e in events:
                    order = e.get("event_order", 0)
                    title = e.get("title", "")
                    desc = (e.get("description") or "")[:500]
                    raw_date = e.get("raw_date", "")
                    etype = e.get("event_type", "event")
                    lines.append(f"- #{order} [{etype}] {title}" + (f" (Th·ªùi ƒëi·ªÉm: {raw_date})" if raw_date else "") + f"\n  {desc}")
                block = "\n".join(lines)
                context_parts.append(block)
                total_tokens += AIService.estimate_tokens(block)
                sources.append("üìÖ Timeline Events (mixed)")
            query_for_chunk = (router_result.get("rewritten_query") or "").strip() or "n·ªôi dung"
            chunk_rows = search_chunks_vector(query_for_chunk, project_id, arc_id=current_arc_id, top_k=5)
            if not chunk_rows and current_arc_id:
                chunk_rows = search_chunks_vector(query_for_chunk, project_id, arc_id=None, top_k=5)
            if chunk_rows and ReverseLookupAssembler:
                chunk_ids = [str(c.get("id")) for c in chunk_rows if c.get("id")]
                if chunk_ids:
                    chunk_ctx, chunk_sources, chunk_tokens = ContextManager.build_context_with_chunk_reverse_lookup(
                        project_id, chunk_ids, current_arc_id, token_limit=5000
                    )
                    if chunk_ctx:
                        context_parts.append(chunk_ctx)
                        total_tokens += chunk_tokens
                        sources.extend(chunk_sources)
                        sources.append("üì¶ Chunks (mixed)")

        context_str = "\n".join(context_parts)
        if max_context_tokens is not None and total_tokens > max_context_tokens:
            context_str, total_tokens = cap_context_to_tokens(context_str, max_context_tokens)
        return context_str, sources, total_tokens


# ==========================================
# üìù AUTO-SUMMARY / CHAPTER METADATA (V5)
# ==========================================
def suggest_import_category(text: str) -> str:
    """G·ª£i √Ω prefix/category cho n·ªôi dung import (d√πng LLM nh·∫π). D√πng prefix t·ª´ DB (get_prefixes), tr·∫£ v·ªÅ [OTHER] n·∫øu kh√¥ng kh·ªõp."""
    if not text or len(text.strip()) < 20:
        return "[OTHER]"
    try:
        model = _get_default_tool_model()
        prefixes = Config.get_prefixes()
        if not prefixes:
            return "[OTHER]"
        if "[OTHER]" not in prefixes:
            prefixes = list(prefixes) + ["[OTHER]"]
        prompt = f"""Ph√¢n lo·∫°i n·ªôi dung sau v√†o ƒê√öNG M·ªòT trong c√°c lo·∫°i (ch·ªâ tr·∫£ v·ªÅ chu·ªói lo·∫°i, kh√¥ng gi·∫£i th√≠ch):
{', '.join(prefixes)}

N·ªòI DUNG (r√∫t g·ªçn):
{text[:1500]}

Tr·∫£ v·ªÅ ƒë√∫ng m·ªôt chu·ªói, v√≠ d·ª•: [CHARACTER] ho·∫∑c [RULE]."""
        resp = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.1,
            max_tokens=50,
        )
        raw = (resp.choices[0].message.content or "").strip()
        for p in prefixes:
            if p in raw or (p.strip("[]") and p.strip("[]").lower() in raw.lower()):
                return p
        return "[OTHER]"
    except Exception as e:
        print(f"suggest_import_category error: {e}")
        return "[OTHER]"


def generate_arc_summary_from_chapters(chapter_summaries: List[Dict[str, Any]], arc_name: str = "") -> Optional[str]:
    """T·ª´ danh s√°ch t√≥m t·∫Øt ch∆∞∆°ng, AI t·∫°o t√≥m t·∫Øt ng·∫Øn cho Arc. Tr·∫£ v·ªÅ str ho·∫∑c None n·∫øu l·ªói."""
    if not chapter_summaries or not isinstance(chapter_summaries, list):
        return None
    parts = []
    for i, ch in enumerate(chapter_summaries):
        num = ch.get("chapter_number") or ch.get("num") or (i + 1)
        summ = ch.get("summary") or ch.get("description") or ""
        if summ:
            parts.append(f"Ch∆∞∆°ng {num}: {summ}")
    if not parts:
        return None
    combined = "\n".join(parts)
    try:
        model = _get_default_tool_model()
        prompt = f"""C√°c t√≥m t·∫Øt ch∆∞∆°ng thu·ªôc Arc '{arc_name or 'Unnamed'}':

{combined}

Nhi·ªám v·ª•: Vi·∫øt 1 ƒëo·∫°n t√≥m t·∫Øt ng·∫Øn g·ªçn (2-5 c√¢u) cho to√†n b·ªô Arc, n·ªëi m·∫°ch c√°c s·ª± ki·ªán/t√¨nh ti·∫øt ch√≠nh. Ch·ªâ tr·∫£ v·ªÅ ƒëo·∫°n t√≥m t·∫Øt, kh√¥ng l·ªùi d·∫´n."""
        resp = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.3,
            max_tokens=500,
        )
        raw = (resp.choices[0].message.content or "").strip()
        return raw if raw else None
    except Exception as e:
        print(f"generate_arc_summary_from_chapters error: {e}")
        return None


def generate_chapter_metadata(content: str) -> Dict[str, str]:
    """D√πng model t·ª´ Settings ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung v√† ph√¢n t√≠ch art_style. Tr·∫£ v·ªÅ {"summary": "...", "art_style": "..."}. Defensive: tr·∫£ v·ªÅ dict r·ªóng n·∫øu l·ªói."""
    if not content or not str(content).strip():
        return {"summary": "", "art_style": ""}
    try:
        model = _get_default_tool_model()
        prompt = f"""Ph√¢n t√≠ch ƒëo·∫°n vƒÉn/ch∆∞∆°ng sau v√† tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON v·ªõi 2 key:
- "summary": T√≥m t·∫Øt n·ªôi dung (2-4 c√¢u, ti·∫øng Vi·ªát).
- "art_style": Phong c√°ch vi·∫øt (v√≠ d·ª•: k·ªÉ chuy·ªán, m√¥ t·∫£, ƒë·ªëi tho·∫°i, h√†nh ƒë·ªông; 1-2 c√¢u).

N·ªòI DUNG:
{content[:12000]}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch. V√≠ d·ª•: {{"summary": "...", "art_style": "..."}}"""
        messages = [{"role": "user", "content": prompt}]
        response = AIService.call_openrouter(
            messages=messages,
            model=model,
            temperature=0.2,
            max_tokens=500,
            response_format={"type": "json_object"},
        )
        raw = response.choices[0].message.content
        raw = AIService.clean_json_text(raw)
        data = json.loads(raw)
        return {
            "summary": str(data.get("summary", ""))[:2000],
            "art_style": str(data.get("art_style", ""))[:500],
        }
    except Exception as e:
        print(f"generate_chapter_metadata error: {e}")
        return {"summary": "", "art_style": ""}


def extract_timeline_events_from_content(content: str, chapter_label: str = "") -> List[Dict[str, Any]]:
    """
    AI tr√≠ch xu·∫•t c√°c s·ª± ki·ªán timeline t·ª´ n·ªôi dung ch∆∞∆°ng (th·ª© t·ª±, m·ªëc th·ªùi gian, flashback).
    Tr·∫£ v·ªÅ list [{"event_order": int, "title": str, "description": str, "raw_date": str, "event_type": "event"|"flashback"|"milestone"|"timeskip"|"other"}].
    """
    if not content or not str(content).strip():
        return []
    try:
        model = _get_default_tool_model()
        ctx = f"Ch∆∞∆°ng: {chapter_label}\n\n" if chapter_label else ""
        prompt = f"""Tr√≠ch xu·∫•t c√°c S·ª∞ KI·ªÜN theo th·ª© t·ª± th·ªùi gian t·ª´ n·ªôi dung d∆∞·ªõi ƒë√¢y. M·ªói s·ª± ki·ªán c√≥ th·ª© t·ª± (event_order b·∫Øt ƒë·∫ßu 1), ti√™u ƒë·ªÅ ng·∫Øn, m√¥ t·∫£, th·ªùi ƒëi·ªÉm (raw_date: c√≥ th·ªÉ l√† "ƒë·∫ßu ch∆∞∆°ng", "sau khi X", "tr∆∞·ªõc chi·∫øn tranh", nƒÉm, v.v.), v√† lo·∫°i (event_type: event, flashback, milestone, timeskip, other).

{ctx}N·ªòI DUNG:
{content[:25000]}

Tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON v·ªõi key "events" l√† m·∫£ng c√°c object:
{{ "event_order": 1, "title": "...", "description": "...", "raw_date": "...", "event_type": "event" }}
event_type ch·ªâ ƒë∆∞·ª£c l√† m·ªôt trong: event, flashback, milestone, timeskip, other.
N·∫øu kh√¥ng c√≥ s·ª± ki·ªán r√µ r√†ng, tr·∫£ v·ªÅ {{ "events": [] }}. Ch·ªâ tr·∫£ v·ªÅ JSON."""
        response = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.2,
            max_tokens=4000,
            response_format={"type": "json_object"},
        )
        raw = (response.choices[0].message.content or "").strip()
        raw = AIService.clean_json_text(raw)
        data = json.loads(raw)
        events = data.get("events") if isinstance(data, dict) else []
        if not isinstance(events, list):
            return []
        out = []
        for i, e in enumerate(events):
            if not isinstance(e, dict):
                continue
            order = int(e.get("event_order", i + 1))
            title = str(e.get("title", "")).strip() or f"S·ª± ki·ªán {order}"
            desc = str(e.get("description", ""))[:2000]
            raw_date = str(e.get("raw_date", ""))[:200]
            etype = str(e.get("event_type", "event")).lower()
            if etype not in ("event", "flashback", "milestone", "timeskip", "other"):
                etype = "event"
            out.append({
                "event_order": order,
                "title": title,
                "description": desc,
                "raw_date": raw_date,
                "event_type": etype,
            })
        return out
    except Exception as ex:
        print(f"extract_timeline_events_from_content error: {ex}")
        return []


def get_file_sample(file_content: str, sample_size: int = 80) -> str:
    """
    L·∫•y m·∫´u r·∫£i r√°c: 80 d√≤ng ƒë·∫ßu + 80 d√≤ng gi·ªØa + 80 d√≤ng cu·ªëi (n·∫øu file d√†i).
    Tr·∫£ v·ªÅ chu·ªói k·∫øt h·ª£p v·ªõi marker [ƒê·∫¶U], [GI·ªÆA], [CU·ªêI].
    """
    if not file_content or not str(file_content).strip():
        return ""
    lines = str(file_content).strip().splitlines()
    total_lines = len(lines)
    if total_lines <= sample_size * 3:
        return "\n".join(lines)
    parts = []
    parts.append(f"[ƒê·∫¶U FILE - {sample_size} d√≤ng ƒë·∫ßu]")
    parts.append("\n".join(lines[:sample_size]))
    mid_start = total_lines // 2 - sample_size // 2
    parts.append(f"\n\n[GI·ªÆA FILE - {sample_size} d√≤ng gi·ªØa (t·ª´ d√≤ng {mid_start})]")
    parts.append("\n".join(lines[mid_start:mid_start + sample_size]))
    parts.append(f"\n\n[CU·ªêI FILE - {sample_size} d√≤ng cu·ªëi]")
    parts.append("\n".join(lines[-sample_size:]))
    return "\n".join(parts)


def analyze_split_strategy(
    file_content: str,
    file_type: str = "story",
    context_hint: str = "",
) -> Dict[str, Any]:
    """
    AI Analyzer (Nh·∫π): Ph√¢n t√≠ch m·∫´u r·∫£i r√°c (80 ƒë·∫ßu + 80 gi·ªØa + 80 cu·ªëi) ƒë·ªÉ t√¨m quy lu·∫≠t ph√¢n c√°ch.
    Tr·∫£ v·ªÅ {"split_type": "by_keyword"|"by_length"|"by_sheet", "split_value": str (regex/keyword)}.
    """
    if not file_content or not str(file_content).strip():
        return {"split_type": "by_length", "split_value": "2000"}
    sample = get_file_sample(file_content, sample_size=80)
    try:
        model = _get_default_tool_model()
        type_hints = {
            "story": "Truy·ªán - t√¨m quy lu·∫≠t ph√¢n c√°ch ch∆∞∆°ng (VD: 'Ch∆∞∆°ng' vi·∫øt hoa, d·∫•u '***', xu·ªëng d√≤ng 2 l·∫ßn).",
            "character_data": "D·ªØ li·ªáu nh√¢n v·∫≠t - t√¨m quy lu·∫≠t ph√¢n c√°ch entity (VD: '##', '---', t√™n ri√™ng ·ªü ƒë·∫ßu d√≤ng).",
            "excel_export": "Excel/CSV - x√°c ƒë·ªãnh c·∫Øt theo 'Sheet' marker hay 'Row count' (s·ªë d√≤ng c·ªë ƒë·ªãnh).",
        }
        hint_text = type_hints.get(file_type.strip().lower(), type_hints["story"])
        if context_hint:
            hint_text += f"\nG·ª£i √Ω ng∆∞·ªùi d√πng: {context_hint}"
        prompt = f"""Ph√¢n t√≠ch m·∫´u file (80 d√≤ng ƒë·∫ßu + 80 d√≤ng gi·ªØa + 80 d√≤ng cu·ªëi) v√† T√åM QUY LU·∫¨T PH√ÇN C√ÅCH.

Lo·∫°i file: {hint_text}

M·∫™U FILE (240 d√≤ng t·ªïng h·ª£p):
---
{sample}
---

NHI·ªÜM V·ª§: T√¨m quy lu·∫≠t ph√¢n c√°ch ch∆∞∆°ng/th·ª±c th·ªÉ/sheet trong file n√†y.
- V√≠ d·ª•: "Ch∆∞∆°ng" vi·∫øt hoa ·ªü ƒë·∫ßu d√≤ng, d·∫•u "***", xu·ªëng d√≤ng 2 l·∫ßn, "[Sheet: X]", v.v.

Y√äU C·∫¶U: Tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON v·ªõi:
- "split_type": m·ªôt trong ["by_keyword", "by_length", "by_sheet"]
  * "by_keyword": T√¨m th·∫•y t·ª´ kh√≥a/pattern l·∫∑p l·∫°i ‚Üí tr·∫£ v·ªÅ regex pattern ho·∫∑c keyword ƒë∆°n gi·∫£n
  * "by_length": Kh√¥ng t√¨m th·∫•y pattern r√µ r√†ng ‚Üí c·∫Øt theo s·ªë k√Ω t·ª± c·ªë ƒë·ªãnh
  * "by_sheet": File Excel ‚Üí c·∫Øt theo Sheet marker
- "split_value": 
  * N·∫øu by_keyword: Regex pattern (VD: "^Ch∆∞∆°ng\\s+\\d+", "\\*{3,}", "^##\\s+") ho·∫∑c keyword ƒë∆°n gi·∫£n (VD: "Ch∆∞∆°ng", "---")
  * N·∫øu by_length: s·ªë k√Ω t·ª± (VD: "2000")
  * N·∫øu by_sheet: "Sheet" ho·∫∑c "Row count"

QUAN TR·ªåNG: Ch·ªâ tr·∫£ v·ªÅ Regex pattern ho·∫∑c Keyword ƒë·ªÉ Python d√πng `re` module c·∫Øt file. KH√îNG c·∫Øt th·ª±c t·∫ø.

V√≠ d·ª•: {{"split_type": "by_keyword", "split_value": "^Ch∆∞∆°ng\\s+\\d+"}}
Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch."""

        response = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.2,
            max_tokens=500,
            response_format={"type": "json_object"},
        )
        raw = (response.choices[0].message.content or "").strip()
        raw = AIService.clean_json_text(raw)
        data = json.loads(raw)
        split_type = data.get("split_type", "by_length")
        split_value = str(data.get("split_value", "2000")).strip()
        if split_type not in ["by_keyword", "by_length", "by_sheet"]:
            split_type = "by_length"
        return {"split_type": split_type, "split_value": split_value}
    except Exception as e:
        print(f"analyze_split_strategy error: {e}")
        return {"split_type": "by_length", "split_value": "2000"}


def _build_smart_regex_pattern(keyword: str) -> str:
    """
    X√¢y d·ª±ng regex pattern h·ªó tr·ª£ c√≥ d·∫•u/kh√¥ng d·∫•u v√† kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng.
    VD: "Ch∆∞∆°ng" -> r"(?i)(CH∆Ø∆†NG|CHUONG|CHAPTER)\s+\d+[:\s-]*"
    """
    import re
    keyword_upper = keyword.upper().strip()
    if keyword_upper in ["CH∆Ø∆†NG", "CHUONG", "CHAPTER"]:
        return r"(?i)(CH∆Ø∆†NG|CHUONG|CHAPTER)\s+\d+[:\s-]*"
    elif keyword_upper in ["PH·∫¶N", "PHAN", "PART"]:
        return r"(?i)(PH·∫¶N|PHAN|PART)\s+\d+[:\s-]*"
    elif keyword_upper in ["---", "***", "==="]:
        return rf"(?i)\s*{re.escape(keyword)}\s*"
    else:
        return rf"(?i)^\s*{re.escape(keyword)}\s*"


def execute_split_logic(
    file_content: str,
    split_type: str,
    split_value: str,
    debug: bool = False,
) -> List[Dict[str, Any]]:
    """
    Python Worker (M·∫°nh): C·∫Øt file b·∫±ng code Python thu·∫ßn, kh√¥ng g·ªçi AI.
    Tr·∫£ v·ªÅ list of {"title": str, "content": str, "order": int}.
    debug=True: In ra debug log (d√πng trong Streamlit v·ªõi st.write).
    """
    if not file_content or not str(file_content).strip():
        return []
    content = str(file_content).strip()
    out = []
    try:
        if split_type == "by_keyword":
            import re
            pattern_str = split_value.strip()
            if not pattern_str:
                pattern_str = "---"
            
            is_regex = any(c in pattern_str for c in ["^", "$", "\\d", "\\s", "\\w", "\\+", "\\*", "\\?", "\\[", "\\(", "\\{", "("])
            
            if not is_regex:
                pattern_str = _build_smart_regex_pattern(pattern_str)
                is_regex = True
            
            try:
                pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
            except Exception as e:
                if debug:
                    print(f"Regex compile error: {e}, fallback to simple pattern")
                pattern_str = rf"^\s*{re.escape(split_value.strip())}\s*"
                pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
            
            matches = list(pattern.finditer(content))
            if debug:
                import streamlit as st
                st.write(f"üîç **Debug Log:** T√¨m th·∫•y **{len(matches)}** v·ªã tr√≠ ph√¢n c√°ch:")
                for i, m in enumerate(matches[:10]):
                    line_num = content[:m.start()].count('\n') + 1
                    preview = content[max(0, m.start()-30):m.end()+30].replace('\n', ' ')
                    st.code(f"{i+1}. D√≤ng {line_num}: ...{preview}...", language=None)
                if len(matches) > 10:
                    st.caption(f"... v√† {len(matches) - 10} v·ªã tr√≠ kh√°c")
            
            if len(matches) == 0:
                if debug:
                    import streamlit as st
                    st.error("‚ùå **Kh√¥ng t√¨m th·∫•y d·∫•u hi·ªáu ph√¢n chia ch∆∞∆°ng.** Vui l√≤ng ki·ªÉm tra l·∫°i ƒë·ªãnh d·∫°ng ho·∫∑c th·ª≠ keyword/pattern kh√°c.")
                return []
            
            # Ph·∫ßn tr∆∞·ªõc t·ª´ kh√≥a ƒë·∫ßu (n·∫øu c√≥)
            if matches[0].start() > 0:
                part_content = content[0:matches[0].start()].strip()
                if part_content:
                    title = "Ph·∫ßn m·ªü ƒë·∫ßu" if not out else "Ph·∫ßn 0"
                    out.append({"title": title, "content": part_content, "order": 1})
            
            # N·ªôi dung N·∫∞M GI·ªÆA hai t·ª´ kh√≥a: t·ª´ sau keyword[i] ƒë·∫øn tr∆∞·ªõc keyword[i+1]
            for i, match in enumerate(matches):
                start = match.end()  # B·∫Øt ƒë·∫ßu SAU t·ª´ kh√≥a hi·ªán t·∫°i
                end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
                part_content = content[start:end].strip()
                if not part_content:
                    continue
                title = match.group(0).strip()[:50] if match.group(0) else f"Ph·∫ßn {len(out)+1}"
                if not title or len(title.strip()) < 2:
                    first_line = part_content.splitlines()[0] if part_content.splitlines() else ""
                    title = first_line[:50] if first_line else f"Ph·∫ßn {len(out)+1}"
                out.append({"title": title, "content": part_content, "order": len(out) + 1})
        elif split_type == "by_length":
            chunk_size = int(split_value) if split_value.isdigit() else 2000
            chunk_size = max(500, min(chunk_size, 50000))
            lines = content.splitlines()
            current_chunk = []
            current_len = 0
            chunk_num = 1
            for line in lines:
                line_len = len(line) + 1
                if current_len + line_len > chunk_size and current_chunk:
                    chunk_text = "\n".join(current_chunk).strip()
                    if chunk_text:
                        out.append({"title": f"Ph·∫ßn {chunk_num}", "content": chunk_text, "order": chunk_num})
                        chunk_num += 1
                    current_chunk = [line]
                    current_len = line_len
                else:
                    current_chunk.append(line)
                    current_len += line_len
            if current_chunk:
                chunk_text = "\n".join(current_chunk).strip()
                if chunk_text:
                    out.append({"title": f"Ph·∫ßn {chunk_num}", "content": chunk_text, "order": chunk_num})
        elif split_type == "by_sheet":
            import re
            if split_value.lower() == "row count" or split_value.isdigit():
                row_count = int(split_value) if split_value.isdigit() else 100
                lines = content.splitlines()
                for i in range(0, len(lines), row_count):
                    chunk_lines = lines[i:i + row_count]
                    if chunk_lines:
                        out.append({"title": f"Sheet {i // row_count + 1}", "content": "\n".join(chunk_lines), "order": i // row_count + 1})
            elif "[Sheet:" in content or "[Sheet " in content:
                pattern = re.compile(r"\[Sheet[:\s]+([^\]]+)\]", re.IGNORECASE)
                parts = pattern.split(content)
                current_sheet = "Sheet 1"
                current_content = []
                idx = 0
                for i, part in enumerate(parts):
                    if i % 2 == 0:
                        if part.strip():
                            current_content.append(part.strip())
                    else:
                        if current_content:
                            out.append({"title": current_sheet, "content": "\n".join(current_content), "order": idx + 1})
                            idx += 1
                        current_sheet = part.strip() or f"Sheet {idx + 2}"
                        current_content = []
                if current_content:
                    out.append({"title": current_sheet, "content": "\n".join(current_content), "order": idx + 1})
            else:
                out.append({"title": "Ph·∫ßn 1", "content": content, "order": 1})
        else:
            out.append({"title": "Ph·∫ßn 1", "content": content, "order": 1})
        return out
    except Exception as e:
        print(f"execute_split_logic error: {e}")
        return [{"title": "Ph·∫ßn 1", "content": content, "order": 1}]


# ==========================================
# üß¨ RULE MINING SYSTEM
# ==========================================
class RuleMiningSystem:
    """H·ªá th·ªëng khai th√°c v√† qu·∫£n l√Ω lu·∫≠t t·ª´ chat"""

    @staticmethod
    def extract_rule_raw(user_prompt: str, ai_response: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t lu·∫≠t th√¥ t·ª´ h·ªôi tho·∫°i"""
        prompt = f"""
        B·∫°n l√† "Trinh S√°t Lu·∫≠t" (Rule Scout). Nhi·ªám v·ª•: Ph√°t hi·ªán s·ªü th√≠ch/y√™u c·∫ßu c·ªßa User.

        H·ªòI THO·∫†I:
        - User: "{user_prompt}"
        - AI: (Ph·∫£n h·ªìi tr∆∞·ªõc ƒë√≥...)

        M·ª§C TI√äU:
        Ph√°t hi·ªán xem User c√≥ ƒëang ng·∫ßm ch·ªâ ƒë·ªãnh C√ÅCH L√ÄM VI·ªÜC, C√ÅCH VI·∫æT, ho·∫∑c ƒê·ªäNH D·∫†NG kh√¥ng.

        TI√äU CH√ç (ƒê·ªô nh·∫°y cao):
        1. Y√™u c·∫ßu ƒë·ªãnh d·∫°ng: "ch·ªâ json", "d√πng markdown", "ƒë·ª´ng vi·∫øt code", "vi·∫øt ng·∫Øn th√¥i".
        2. ƒêi·ªÅu ch·ªânh vƒÉn phong: "nghi√™m t√∫c h∆°n", "b·ªõt n√≥i nh·∫£m", "d√πng ti·∫øng Vi·ªát".
        3. S·ª≠a l·ªói: "sai r·ªìi", "kh√¥ng ph·∫£i th·∫ø", "l√†m th·∫ø n√†y m·ªõi ƒë√∫ng".

        H∆Ø·ªöNG D·∫™N:
        - N·∫øu User n√≥i: "Vi·∫øt c√°i n√†y b·∫±ng Python nh√©" -> T·∫°o lu·∫≠t: "Lu√¥n ∆∞u ti√™n d√πng Python".
        - Th√† b·∫Øt nh·∫ßm c√≤n h∆°n b·ªè s√≥t.

        OUTPUT:
        - N·∫øu ph√°t hi·ªán lu·∫≠t: Tr·∫£ v·ªÅ 1 c√¢u m·ªánh l·ªánh ng·∫Øn g·ªçn k√®m ng·ªØ c·∫£nh (Ti·∫øng Vi·ªát). V√≠ d·ª•: "Lu√¥n tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON khi ƒë∆∞·ª£c y√™u c·∫ßu...", "Kh√¥ng gi·∫£i th√≠ch d√†i d√≤ng khi user ƒëang kh√≥ ch·ªãu...".
        - N·∫øu ch·ªâ l√† ch√†o h·ªèi/c·∫£m ∆°n: Tr·∫£ v·ªÅ "NO_RULE".

        Ch·ªâ tr·∫£ v·ªÅ Text.
        """

        messages = [
            {"role": "system", "content": "You are Rule Extractor. Return text only."},
            {"role": "user", "content": prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=_get_default_tool_model(),
                temperature=0.3,
                max_tokens=300
            )

            text = response.choices[0].message.content.strip()

            if "NO_RULE" in text or len(text) < 5:
                return None
            return text
        except Exception as e:
            print(f"Rule extraction error: {e}")
            return None

    @staticmethod
    def analyze_rule_conflict(new_rule_content: str, project_id: str) -> Dict:
        """Check rule conflict with DB - Safe Version"""
        similar_rules_str = HybridSearch.smart_search_hybrid(new_rule_content, project_id, top_k=3)

        if not similar_rules_str:
            return {
                "status": "NEW",
                "reason": "No conflicts found",
                "existing_rule_summary": "None",
                "merged_content": None,
                "suggested_content": new_rule_content
            }

        judge_prompt = f"""
        Lu·∫≠t M·ªõi: "{new_rule_content}"
        Lu·∫≠t C≈© trong DB: "{similar_rules_str}"

        Nhi·ªám v·ª•: So s√°nh m·ªëi quan h·ªá.

        - CONFLICT (Xung ƒë·ªôt): M√¢u thu·∫´n tr·ª±c ti·∫øp (Vd: C≈© b·∫£o A, M·ªõi b·∫£o kh√¥ng A).
        - MERGE (G·ªôp): C√πng ch·ªß ƒë·ªÅ nh∆∞ng lu·∫≠t M·ªõi chi ti·∫øt h∆°n ho·∫∑c b·ªï sung cho lu·∫≠t C≈©.
        - NEW (M·ªõi): Ch·ªß ƒë·ªÅ kh√°c h·∫≥n.

        OUTPUT JSON ONLY:
        {{
            "status": "CONFLICT" | "MERGE" | "NEW",
            "existing_rule_summary": "T√≥m t·∫Øt lu·∫≠t c≈© (Ti·∫øng Vi·ªát)",
            "reason": "L√Ω do (Ti·∫øng Vi·ªát)",
            "merged_content": "N·ªôi dung lu·∫≠t ƒë√£ g·ªôp ho√†n ch·ªânh (n·∫øu MERGE). N·∫øu kh√°c th√¨ ƒë·ªÉ null."
        }}
        """

        messages = [
            {"role": "system", "content": "You are Rule Judge. Return only JSON."},
            {"role": "user", "content": judge_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=_get_default_tool_model(),
                temperature=0.2,
                max_tokens=4000,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            content = AIService.clean_json_text(content)

            result = json.loads(content)

            return {
                "status": result.get("status", "NEW"),
                "reason": result.get("reason", "No reason provided by AI"),
                "existing_rule_summary": result.get("existing_rule_summary", "N/A"),
                "merged_content": result.get("merged_content", None),
                "suggested_content": new_rule_content
            }

        except Exception as e:
            print(f"Rule analysis error: {e}")
            return {
                "status": "NEW",
                "reason": f"AI Judge Error: {str(e)}",
                "existing_rule_summary": "Error analyzing",
                "merged_content": None,
                "suggested_content": new_rule_content
            }

    @staticmethod
    def crystallize_session(chat_history: List[Dict], persona_role: str) -> str:
        """T√≥m t·∫Øt v√† l·ªçc th√¥ng tin gi√° tr·ªã t·ª´ chat history"""
        chat_text = "\n".join([f"{m['role']}: {m['content']}" for m in chat_history])

        crystallize_prompt = f"""
        B·∫°n l√† Th∆∞ K√Ω Cu·ªôc H·ªçp ({persona_role}).
        
        Nhi·ªám v·ª•: ƒê·ªçc ƒëo·∫°n chat d∆∞·ªõi ƒë√¢y v√† L·ªåC B·ªé NH·ªÆNG TH·ª® V√î NGHƒ®A.
        Ch·ªâ gi·ªØ l·∫°i v√† T√ìM T·∫ÆT nh·ªØng th√¥ng tin gi√° tr·ªã (S·ª± ki·ªán, √ù t∆∞·ªüng, Quy·∫øt ƒë·ªãnh, Lore m·ªõi).

        CHAT LOG: {chat_text}

        OUTPUT: Tr·∫£ v·ªÅ b·∫£n t√≥m t·∫Øt s√∫c t√≠ch (50-100 t·ª´) b·∫±ng Ti·∫øng Vi·ªát. 
        N·∫øu to√†n l√† ch√†o h·ªèi v√¥ nghƒ©a, tr·∫£ v·ªÅ "NO_INFO".
        """

        messages = [
            {"role": "system", "content": "You are Conversation Summarizer. Return text only."},
            {"role": "user", "content": crystallize_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=_get_default_tool_model(),
                temperature=0.3,
                max_tokens=8000
            )

            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Crystallize error: {e}")
            return f"AI Error: {e}"
