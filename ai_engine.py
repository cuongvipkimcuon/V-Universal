# ai_engine.py - AI Service, Router, Context, Rule Mining
import json
import re
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Tuple, Any

import streamlit as st
from openai import OpenAI

from config import Config, init_services


# ==========================================
# ü§ñ AI SERVICE
# ==========================================
class AIService:
    """D·ªãch v·ª• AI s·ª≠ d·ª•ng OpenAI client cho OpenRouter v·ªõi c√°c t√≠nh nƒÉng n√¢ng cao"""

    @staticmethod
    @st.cache_data(ttl=3600)
    def get_available_models():
        """L·∫•y danh s√°ch model c√≥ s·∫µn t·ª´ OpenRouter"""
        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY
            )
            return Config.AVAILABLE_MODELS
        except Exception:
            return Config.AVAILABLE_MODELS

    @staticmethod
    def call_openrouter(
        messages: List[Dict],
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 8000,
        stream: bool = False,
        response_format: Optional[Dict] = None
    ) -> Any:
        """G·ªçi OpenRouter API s·ª≠ d·ª•ng OpenAI client"""
        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY,
                default_headers={
                    "HTTP-Referer": "https://v-universe.streamlit.app",
                    "X-Title": "V-Universe AI Hub"
                }
            )

            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream,
                response_format=response_format
            )

            return response
        except Exception as e:
            raise Exception(f"OpenRouter API error: {str(e)}")

    @staticmethod
    def get_embedding(text: str) -> Optional[List[float]]:
        """L·∫•y embedding t·ª´ OpenRouter"""
        if not text or not isinstance(text, str) or not text.strip():
            return None

        try:
            client = OpenAI(
                base_url=Config.OPENROUTER_BASE_URL,
                api_key=Config.OPENROUTER_API_KEY
            )

            response = client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=text
            )

            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    @staticmethod
    def estimate_tokens(text: str) -> int:
        """∆Ø·ªõc t√≠nh s·ªë token"""
        if not text:
            return 0
        return len(text) // 4

    @staticmethod
    def calculate_cost(
        input_tokens: int,
        output_tokens: int,
        model: str
    ) -> float:
        """T√≠nh chi ph√≠ cho request"""
        model_costs = Config.MODEL_COSTS.get(model, {"input": 0.0, "output": 0.0})

        input_cost = (input_tokens / 1_000_000) * model_costs["input"]
        output_cost = (output_tokens / 1_000_000) * model_costs["output"]

        return round(input_cost + output_cost, 6)

    @staticmethod
    def clean_json_text(text):
        """L√†m s·∫°ch markdown (```json ... ```) tr∆∞·ªõc khi parse"""
        if not text:
            return "{}"
        text = text.replace("```json", "").replace("```", "").strip()
        start = text.find("{")
        end = text.rfind("}") + 1
        if start != -1 and end != 0:
            return text[start:end]
        return text


# ==========================================
# üîç HYBRID SEARCH SYSTEM (V5 - Re-ranking + lookup stats)
# ==========================================
# Tr·ªçng s·ªë re-rank: VectorSim * 0.7 + RecencyBonus * 0.1 + ImportanceBias * 0.2
VECTOR_WEIGHT = 0.7
RECENCY_WEIGHT = 0.1
IMPORTANCE_WEIGHT = 0.2
RECENCY_BONUS_HOURS = 24


def _safe_float(value: Any, default: float = 0.5) -> float:
    """L·∫•y s·ªë th·ª±c an to√†n t·ª´ record (defensive)."""
    if value is None:
        return default
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


def _recency_bonus(last_lookup_at: Any) -> float:
    """RecencyBonus: 1.0 n·∫øu last_lookup_at trong v√≤ng 24h, else 0.0."""
    if last_lookup_at is None:
        return 0.0
    try:
        if isinstance(last_lookup_at, str):
            dt = datetime.fromisoformat(last_lookup_at.replace("Z", "+00:00"))
        else:
            dt = last_lookup_at
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        delta = now - dt
        return 1.0 if delta <= timedelta(hours=RECENCY_BONUS_HOURS) else 0.0
    except Exception:
        return 0.0


def _rerank_by_score(rows: List[Dict], top_k: int) -> List[Dict]:
    """T√≠nh Final Score v√† s·∫Øp x·∫øp l·∫°i: (VectorSim*0.7) + (RecencyBonus*0.1) + (ImportanceBias*0.2)."""
    for item in rows:
        vector_sim = _safe_float(item.get("similarity") or item.get("score"), 0.5)
        vector_sim = max(0.0, min(1.0, vector_sim))
        recency = _recency_bonus(item.get("last_lookup_at"))
        importance = _safe_float(item.get("importance_bias"), 0.5)
        importance = max(0.0, min(1.0, importance))
        item["_final_score"] = (vector_sim * VECTOR_WEIGHT) + (recency * RECENCY_WEIGHT) + (importance * IMPORTANCE_WEIGHT)
    sorted_rows = sorted(rows, key=lambda x: x.get("_final_score", 0.0), reverse=True)
    for item in sorted_rows:
        item.pop("_final_score", None)
    return sorted_rows[:top_k]


def _rerank_by_score_with_breakdown(rows: List[Dict], top_k: int) -> List[Dict]:
    """Gi·ªëng _rerank_by_score nh∆∞ng gi·ªØ l·∫°i score_vector, score_recency, score_bias, score_final ƒë·ªÉ hi·ªÉn th·ªã."""
    for item in rows:
        vector_sim = _safe_float(item.get("similarity") or item.get("score"), 0.5)
        vector_sim = max(0.0, min(1.0, vector_sim))
        recency = _recency_bonus(item.get("last_lookup_at"))
        importance = _safe_float(item.get("importance_bias"), 0.5)
        importance = max(0.0, min(1.0, importance))
        item["score_vector"] = round(vector_sim * VECTOR_WEIGHT, 4)
        item["score_recency"] = round(recency * RECENCY_WEIGHT, 4)
        item["score_bias"] = round(importance * IMPORTANCE_WEIGHT, 4)
        item["score_final"] = round(
            item["score_vector"] + item["score_recency"] + item["score_bias"], 4
        )
    sorted_rows = sorted(rows, key=lambda x: x.get("score_final", 0.0), reverse=True)
    return sorted_rows[:top_k]


class HybridSearch:
    """H·ªá th·ªëng t√¨m ki·∫øm k·∫øt h·ª£p vector v√† t·ª´ kh√≥a (V5: re-ranking, lookup_count, last_lookup_at)"""

    @staticmethod
    def smart_search_hybrid_raw(query_text: str, project_id: str, top_k: int = 10) -> List[Dict]:
        """T√¨m ki·∫øm hybrid tr·∫£ v·ªÅ raw data; select th√™m lookup_count, last_lookup_at, importance_bias; re-rank trong Python."""
        try:
            services = init_services()
            supabase = services["supabase"]
            query_vec = AIService.get_embedding(query_text)
            candidate_limit = max(top_k * 3, 30)

            if query_vec:
                try:
                    response = supabase.rpc("hybrid_search", {
                        "query_text": query_text,
                        "query_embedding": query_vec,
                        "match_threshold": 0.3,
                        "match_count": candidate_limit,
                        "story_id_input": project_id,
                    }).execute()
                    raw_list = response.data if response.data else []
                except Exception:
                    raw_list = []
                if not raw_list:
                    try:
                        response = supabase.table("story_bible").select("*").eq(
                            "story_id", project_id
                        ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                            candidate_limit
                        ).execute()
                        raw_list = response.data if response.data else []
                        for item in raw_list:
                            item["similarity"] = 0.5
                    except Exception:
                        raw_list = []
            else:
                try:
                    response = supabase.table("story_bible").select("*").eq(
                        "story_id", project_id
                    ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                        candidate_limit
                    ).execute()
                    raw_list = response.data if response.data else []
                    for item in raw_list:
                        item["similarity"] = 0.5
                except Exception:
                    raw_list = []

            if not raw_list:
                return []

            reranked = _rerank_by_score(raw_list, top_k)
            return reranked

        except Exception as e:
            print(f"Search error: {e}")
            return []

    @staticmethod
    def smart_search_hybrid_raw_with_scores(query_text: str, project_id: str, top_k: int = 10) -> List[Dict]:
        """Gi·ªëng smart_search_hybrid_raw nh∆∞ng m·ªói item c√≥ th√™m score_vector, score_recency, score_bias, score_final."""
        try:
            services = init_services()
            supabase = services["supabase"]
            query_vec = AIService.get_embedding(query_text)
            candidate_limit = max(top_k * 3, 30)
            if query_vec:
                try:
                    response = supabase.rpc("hybrid_search", {
                        "query_text": query_text,
                        "query_embedding": query_vec,
                        "match_threshold": 0.3,
                        "match_count": candidate_limit,
                        "story_id_input": project_id,
                    }).execute()
                    raw_list = response.data if response.data else []
                except Exception:
                    raw_list = []
                if not raw_list:
                    try:
                        response = supabase.table("story_bible").select("*").eq(
                            "story_id", project_id
                        ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                            candidate_limit
                        ).execute()
                        raw_list = response.data if response.data else []
                        for item in raw_list:
                            item["similarity"] = 0.5
                    except Exception:
                        raw_list = []
            else:
                try:
                    response = supabase.table("story_bible").select("*").eq(
                        "story_id", project_id
                    ).or_(f"entity_name.ilike.%{query_text}%,description.ilike.%{query_text}%").limit(
                        candidate_limit
                    ).execute()
                    raw_list = response.data if response.data else []
                    for item in raw_list:
                        item["similarity"] = 0.5
                except Exception:
                    raw_list = []
            if not raw_list:
                return []
            return _rerank_by_score_with_breakdown(raw_list, top_k)
        except Exception as e:
            print(f"Search error: {e}")
            return []

    @staticmethod
    def update_lookup_stats(entity_id: Any) -> None:
        """TƒÉng lookup_count += 1 v√† c·∫≠p nh·∫≠t last_lookup_at = now() cho record v·ª´a ƒë∆∞·ª£c t√¨m th·∫•y. Defensive: kh√¥ng crash n·∫øu c·ªôt ch∆∞a c√≥."""
        if entity_id is None:
            return
        try:
            services = init_services()
            if not services:
                return
            supabase = services["supabase"]
            now_iso = datetime.now(timezone.utc).isoformat()
            try:
                row = supabase.table("story_bible").select("lookup_count").eq("id", entity_id).execute()
                current = 0
                if row.data and len(row.data) > 0:
                    current = _safe_float(row.data[0].get("lookup_count"), 0.0)
                new_count = int(current) + 1
                supabase.table("story_bible").update({
                    "lookup_count": new_count,
                    "last_lookup_at": now_iso,
                }).eq("id", entity_id).execute()
            except Exception:
                pass
        except Exception as e:
            print(f"update_lookup_stats error: {e}")

    @staticmethod
    def smart_search_hybrid(query_text: str, project_id: str, top_k: int = 10) -> str:
        """Wrapper tr·∫£ v·ªÅ string context (gi·ªØ t∆∞∆°ng th√≠ch)."""
        raw_data = HybridSearch.smart_search_hybrid_raw(query_text, project_id, top_k)
        results = []
        if raw_data:
            for item in raw_data:
                name = item.get("entity_name") or ""
                desc = item.get("description") or ""
                results.append(f"- [{name}]: {desc}")
        return "\n".join(results) if results else ""


# ==========================================
# üß≠ SMART AI ROUTER SYSTEM
# ==========================================


def extract_prefix(name: str) -> Tuple[str, str]:
    """
    B√≥c t√°ch ti·ªÅn t·ªë: t√¨m n·ªôi dung trong [...] ·ªü ƒë·∫ßu chu·ªói.
    VD: "[V≈® KH√ç] Ki·∫øm Thi√™n" -> ("V≈® KH√ç", "Ki·∫øm Thi√™n"). Defensive: l·ªói -> ("", name g·ªëc).
    """
    if not name or not isinstance(name, str):
        return "", (name or "")
    s = name.strip()
    if not s:
        return "", name
    try:
        if s.startswith("["):
            idx = s.find("]")
            if idx > 0:
                prefix = s[1:idx].strip()
                rest = s[idx + 1:].strip()
                return prefix, rest if rest else s
        return "", s
    except Exception:
        return "", s


def _estimate_tokens(text: str) -> int:
    """∆Ø·ªõc l∆∞·ª£ng s·ªë token (~4 k√Ω t·ª±/token)."""
    if not text:
        return 0
    return max(1, len(text) // 4)


# Legacy map (unused here, kept if other code references)
_PREFIX_TO_LABEL = {
    "[CHARACTER]": "CHARACTERS",
    "[RULE]": "RULES",
    "[LOCATION]": "LOCATIONS",
    "[ITEM]": "ITEMS",
    "[CONCEPT]": "CONCEPTS",
    "[EVENT]": "EVENTS",
    "[SYSTEM]": "SYSTEMS",
    "[LORE]": "LORE",
    "[TECH]": "SKILLS",
    "[META]": "META",
    "[CHAT]": "CHAT",
    "[MERGED]": "MERGED",
}


def get_bible_index(story_id: str, max_tokens: int = 2000) -> str:
    """
    Danh s√°ch th√¥ cho Router: m·ªói d√≤ng "Entity: [LO·∫†I] T√™n" (gi·ªØ nguy√™n format [PREFIX] Name).
    Top 100 theo (lookup_count + importance_bias). C√≥ parent_id th√¨ g·ª£i √Ω th·ª±c th·ªÉ g·ªëc.
    """
    if not story_id:
        return ""
    try:
        services = init_services()
        if not services:
            return ""
        supabase = services["supabase"]
        try:
            rows = (
                supabase.table("story_bible")
                .select("entity_name, lookup_count, importance_bias, parent_id")
                .eq("story_id", story_id)
                .execute()
            )
        except Exception:
            try:
                rows = (
                    supabase.table("story_bible")
                    .select("entity_name, lookup_count, importance_bias")
                    .eq("story_id", story_id)
                    .execute()
                )
            except Exception:
                return ""
        data = list(rows.data) if rows.data else []
        for r in data:
            r.setdefault("parent_id", None)
        def _score(r):
            try:
                lk = int(r.get("lookup_count") or 0)
                bi = r.get("importance_bias")
                b = float(bi) if bi is not None else 0.0
                return lk + b
            except (TypeError, ValueError):
                return 0
        data.sort(key=_score, reverse=True)
        top100 = data[:100]
        parent_ids = [r["parent_id"] for r in top100 if r.get("parent_id")]
        parent_names: Dict[Any, str] = {}
        if parent_ids:
            try:
                ids = list(set(str(pid) for pid in parent_ids if pid is not None))
                if ids:
                    pr = supabase.table("story_bible").select("id, entity_name").in_("id", ids).execute()
                    if pr.data:
                        for row in pr.data:
                            try:
                                _, disp = extract_prefix(row.get("entity_name") or "")
                                parent_names[row.get("id")] = disp.strip() or "(g·ªëc)"
                            except Exception:
                                parent_names[row.get("id")] = (row.get("entity_name") or "").strip() or "(g·ªëc)"
            except Exception:
                pass
        lines = []
        for r in top100:
            name = r.get("entity_name")
            if not name:
                continue
            line = f"Entity: {name}"
            pid = r.get("parent_id")
            if pid is not None and parent_names.get(pid):
                line += f" (g·ªëc: {parent_names[pid]})"
            lines.append(line)
        out = "\n".join(lines) if lines else ""
        if _estimate_tokens(out) > max_tokens:
            out = out[: max(100, max_tokens * 4)]
        return out
    except Exception as e:
        print(f"get_bible_index error: {e}")
        return ""


def get_bible_entries(story_id: str) -> List[Dict[str, Any]]:
    """Tr·∫£ v·ªÅ danh s√°ch entity trong Bible c·ªßa story: [{id, entity_name}, ...]. ƒê·ªÉ resolve t√™n -> id khi ƒë·ªÅ xu·∫•t quan h·ªá."""
    if not story_id:
        return []
    try:
        services = init_services()
        if not services:
            return []
        services = init_services()
        supabase = services["supabase"] if services else None
        if not supabase:
            return []
        r = (
            supabase
            .table("story_bible")
            .select("id, entity_name")
            .eq("story_id", story_id)
            .execute()
        )
        return list(r.data) if r.data else []
    except Exception:
        return []


def suggest_relations(content: str, story_id: str) -> List[Dict[str, Any]]:
    """
    AI qu√©t n·ªôi dung (ch∆∞∆°ng/ƒëo·∫°n) v√† so kh·ªõp v·ªõi bible_index ƒë·ªÉ ƒë·ªÅ xu·∫•t:
    - Quan h·ªá gi·ªØa hai th·ª±c th·ªÉ: Source, Target, Relation_Type, Reason -> tr·∫£ v·ªÅ kind="relation".
    - Nh√¢n v·∫≠t ti·∫øn h√≥a (1-n): th·ª±c th·ªÉ m·ªõi c√πng g·ªëc -> g·ª£i √Ω parent_id, kind="parent".
    Output: list of {
      "kind": "relation" | "parent",
      "source_entity_id", "target_entity_id", "relation_type", "description" (reason), "story_id"  (cho relation),
      ho·∫∑c "entity_id", "parent_entity_id", "reason" (cho parent).
    }
    """
    if not content or not content.strip() or not story_id:
        return []
    entries = get_bible_entries(story_id)
    if not entries:
        return []
    name_to_id = {}
    for e in entries:
        name = (e.get("entity_name") or "").strip()
        if name:
            name_to_id[name] = e.get("id")
    index_text = "\n".join([f"- {e.get('entity_name', '')}" for e in entries[:150]])
    prompt = f"""B·∫°n l√† tr·ª£ l√Ω ph√¢n t√≠ch vƒÉn b·∫£n. Cho N·ªòI DUNG v√† DANH S√ÅCH TH·ª∞C TH·ªÇ (Bible) c·ªßa m·ªôt truy·ªán.

DANH S√ÅCH TH·ª∞C TH·ªÇ (ch√≠nh x√°c t·ª´ Bible):
{index_text}

N·ªòI DUNG (ƒëo·∫°n/ch∆∞∆°ng c·∫ßn ph√¢n t√≠ch):
---
{content[:15000]}
---

Nhi·ªám v·ª•:
1) QUAN H·ªÜ: T√¨m c√°c c·∫∑p th·ª±c th·ªÉ c√≥ t∆∞∆°ng t√°c/li√™n quan trong n·ªôi dung (v√≠ d·ª•: A l√† b·∫°n c·ªßa B, X ph·∫£n b·ªôi Y). V·ªõi m·ªói c·∫∑p, tr·∫£ v·ªÅ source (t√™n ƒë√∫ng nh∆∞ trong danh s√°ch), target, relation_type (ng·∫Øn g·ªçn: b·∫°n, k·∫ª th√π, ƒë·ªìng ƒë·ªôi, y√™u, cha-con...), reason (l√Ω do ng·∫Øn).
2) NH√ÇN V·∫¨T TI·∫æN H√ìA (1-n): N·∫øu trong n·ªôi dung c√≥ th·ª±c th·ªÉ m·ªõi m√† th·ª±c ch·∫•t l√† "phi√™n b·∫£n kh√°c" c·ªßa m·ªôt th·ª±c th·ªÉ ƒë√£ c√≥ (VD: "C∆∞·ªùng l√∫c nh·ªè" / "C∆∞·ªùng l√∫c l·ªõn", c√πng m·ªôt nh√¢n v·∫≠t ·ªü hai giai ƒëo·∫°n), KH√îNG t·∫°o quan h·ªá r·ªùi r·∫°c m√† g·ª£i √Ω ƒë·∫∑t parent: entity (t√™n th·ª±c th·ªÉ con/bi·∫øn th·ªÉ) v√† parent (t√™n th·ª±c th·ªÉ g·ªëc trong danh s√°ch), k√®m reason.

Tr·∫£ v·ªÅ ƒê√öNG m·ªôt JSON object v·ªõi hai key:
- "relations": [ {{ "source": "<t√™n trong danh s√°ch>", "target": "<t√™n trong danh s√°ch>", "relation_type": "...", "reason": "..." }} ]
- "parent_suggestions": [ {{ "entity": "<t√™n con/bi·∫øn th·ªÉ trong danh s√°ch>", "parent": "<t√™n g·ªëc trong danh s√°ch>", "reason": "..." }} ]

Ch·ªâ d√πng t√™n c√≥ trong DANH S√ÅCH TH·ª∞C TH·ªÇ. N·∫øu kh√¥ng c√≥ g√¨ ph√π h·ª£p, tr·∫£ v·ªÅ "relations": [] v√† "parent_suggestions": [].
Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m."""

    try:
        response = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=Config.ROUTER_MODEL,
            temperature=0.2,
            max_tokens=2000,
        )
        text = (response.choices[0].message.content or "").strip()
        text = re.sub(r"^```\w*\n?", "", text).strip()
        text = re.sub(r"\n?```\s*$", "", text).strip()
        data = json.loads(text)
        relations_in = data.get("relations") or []
        parent_in = data.get("parent_suggestions") or []

        def resolve_name(name: str) -> Optional[Any]:
            n = (name or "").strip()
            if n in name_to_id:
                return name_to_id[n]
            for k, vid in name_to_id.items():
                if n in k or k in n:
                    return vid
            return None

        out = []
        for r in relations_in:
            src_id = resolve_name(r.get("source") or "")
            tgt_id = resolve_name(r.get("target") or "")
            if src_id and tgt_id and src_id != tgt_id:
                out.append({
                    "kind": "relation",
                    "source_entity_id": src_id,
                    "target_entity_id": tgt_id,
                    "relation_type": (r.get("relation_type") or "li√™n quan").strip(),
                    "description": (r.get("reason") or "").strip(),
                    "story_id": story_id,
                })
        for p in parent_in:
            child_id = resolve_name(p.get("entity") or "")
            parent_id = resolve_name(p.get("parent") or "")
            if child_id and parent_id and child_id != parent_id:
                out.append({
                    "kind": "parent",
                    "entity_id": child_id,
                    "parent_entity_id": parent_id,
                    "reason": (p.get("reason") or "").strip(),
                })
        return out
    except Exception as e:
        print(f"suggest_relations error: {e}")
        return []


class SmartAIRouter:
    """B·ªô ƒë·ªãnh tuy·∫øn AI th√¥ng minh v·ªõi hybrid search v√† bible index"""

    @staticmethod
    def ai_router_pro_v2(user_prompt: str, chat_history_text: str, project_id: str = None) -> Dict:
        """Router V2: Ph√¢n t√≠ch Intent v√† Target Files, c√≥ inject bible_index ƒë·ªÉ nh·∫≠n di·ªán √Ω ƒë·ªãnh."""
        rules_context = ""
        bible_index = ""
        prefix_setup_str = ""
        if project_id:
            rules_context = ContextManager.get_mandatory_rules(project_id)
            bible_index = get_bible_index(project_id, max_tokens=2000)
        try:
            prefix_setup = Config.get_prefix_setup()
            if prefix_setup:
                prefix_setup_str = "\n".join(
                    f"- [{p.get('prefix_key', '')}]: {p.get('description', '')}" for p in prefix_setup
                )
        except Exception:
            prefix_setup_str = "- [RULE]: Quy t·∫Øc, lu·∫≠t l·ªá.\n- [CHAT]: ƒêi·ªÉm nh·ªõ t·ª´ h·ªôi tho·∫°i."

        router_prompt = f"""
        ƒê√≥ng vai ƒêi·ªÅu Ph·ªëi Vi√™n D·ª± √Ån (Project Coordinator).
        
        ‚ö†Ô∏è QUY T·∫ÆC B·∫ÆT BU·ªòC:
        {rules_context}

        B·∫¢NG M√î T·∫¢ C√ÅC LO·∫†I TH·ª∞C TH·ªÇ (do ng∆∞·ªùi d√πng cung c·∫•p):
        {prefix_setup_str}

        DANH S√ÅCH TH·ª∞C TH·ªÇ TRONG STORY BIBLE (m·ªói d√≤ng: Entity: [LO·∫†I] T√™n):
        {bible_index if bible_index else "(Ch∆∞a c√≥ d·ªØ li·ªáu)"}

        Y√äU C·∫¶U ƒêI·ªÄU H∆Ø·ªöNG: D·ª±a v√†o b·∫£ng m√¥ t·∫£ c√°c lo·∫°i th·ª±c th·ªÉ tr√™n. N·∫øu user h·ªèi v·ªÅ m·ªôt th·ª±c th·ªÉ c√≥ lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi m√¥ t·∫£, h√£y d√πng tool search_bible (ho·∫∑c get_entity_relations). N·∫øu kh√¥ng ph√¢n lo·∫°i ƒë∆∞·ª£c, h√£y coi l√† Other nh∆∞ng v·∫´n ∆∞u ti√™n search_bible n·∫øu ƒë√≥ l√† t√™n ri√™ng. Ch·ªâ ∆∞u ti√™n search_chapters khi user h·ªèi r√µ v·ªÅ di·ªÖn bi·∫øn, s·ª± ki·ªán theo th·ªùi gian ho·∫∑c n·ªôi dung ch∆∞∆°ng c·ª• th·ªÉ.

        L·ªäCH S·ª¨ CHAT:
        {chat_history_text}
        
        INPUT C·ª¶A USER: "{user_prompt}"
        
        NHI·ªÜM V·ª§: Ph√¢n t√≠ch intent, target files V√Ä nh·∫≠n di·ªán PH·∫†M VI CH∆Ø∆†NG (Chapter Range) n·∫øu user ƒë·ªÅ c·∫≠p.

        PH√ÇN LO·∫†I INTENT:
        1. "read_full_content": User mu·ªën S·ª≠a, Review, Vi·∫øt ti·∫øp, Ki·ªÉm tra code/vƒÉn, ho·∫∑c nh·∫Øc ƒë·∫øn t√™n file c·ª• th·ªÉ -> C·∫ßn ƒë·ªçc NGUY√äN VƒÇN FILE.
        2. "search_bible": User h·ªèi th√¥ng tin chung, Lore, c·ªët truy·ªán, quy ƒë·ªãnh, kh√°i ni·ªám, ho·∫∑c nh·∫Øc t√™n nh√¢n v·∫≠t/th·ª±c th·ªÉ c√≥ trong danh s√°ch Bible tr√™n -> Tra c·ª©u Bible (search_bible / get_entity_relations).
        3. "chat_casual": Ch√†o h·ªèi, khen ch√™, n√≥i chuy·ªán phi·∫øm kh√¥ng c·∫ßn d·ªØ li·ªáu d·ª± √°n.
        4. "mixed_context": C·∫ßn c·∫£ n·ªôi dung file V√Ä ki·∫øn th·ª©c Bible.

        NH·∫¨N DI·ªÜN PH·∫†M VI CH∆Ø∆†NG (chapter_range):
        - N·∫øu user n√≥i "ch∆∞∆°ng ƒë·∫ßu", "m·∫•y ch∆∞∆°ng ƒë·∫ßu", "ƒë·∫ßu truy·ªán" -> ƒë·∫∑t "chapter_range_mode": "first", "chapter_range_count": 5 (ho·∫∑c s·ªë user n√≥i n·∫øu r√µ).
        - N·∫øu user n√≥i "m·ªõi nh·∫•t", "ch∆∞∆°ng m·ªõi", "m·∫•y ch∆∞∆°ng cu·ªëi" -> ƒë·∫∑t "chapter_range_mode": "latest", "chapter_range_count": 5 (ho·∫∑c s·ªë user n√≥i n·∫øu r√µ).
        - N·∫øu user n√≥i c·ª• th·ªÉ "t·ª´ ch∆∞∆°ng 5 ƒë·∫øn 10", "ch∆∞∆°ng 5 ƒë·∫øn 10" -> ƒë·∫∑t "chapter_range": [5, 10], "chapter_range_mode": "range".
        - N·∫øu kh√¥ng li√™n quan ph·∫°m vi ch∆∞∆°ng -> ƒë·ªÉ "chapter_range": null, "chapter_range_mode": null.

        OUTPUT (JSON ONLY):
        {{
            "intent": "...",
            "target_files": ["t√™n file 1", "t√™n file 2"],
            "target_bible_entities": ["t√™n th·ª±c th·ªÉ 1", "t√™n th·ª±c th·ªÉ 2"],
            "reason": "L√Ω do ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát",
            "rewritten_query": "Vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa user cho r√µ nghƒ©a h∆°n ƒë·ªÉ search database",
            "chapter_range": [start, end] ho·∫∑c null,
            "chapter_range_mode": "first" | "latest" | "range" | null,
            "chapter_range_count": 5
        }}
        """

        messages = [
            {"role": "system", "content": "B·∫°n l√† AI Router th√¥ng minh. Ch·ªâ tr·∫£ v·ªÅ JSON."},
            {"role": "user", "content": router_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=Config.ROUTER_MODEL,
                temperature=0.1,
                max_tokens=500,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            content = AIService.clean_json_text(content)

            result = json.loads(content)

            result.setdefault("target_files", [])
            result.setdefault("target_bible_entities", [])
            result.setdefault("rewritten_query", user_prompt)
            result.setdefault("chapter_range", None)
            result.setdefault("chapter_range_mode", None)
            result.setdefault("chapter_range_count", 5)

            return result

        except Exception as e:
            print(f"Router error: {e}")
            return {
                "intent": "chat_casual",
                "target_files": [],
                "target_bible_entities": [],
                "reason": f"Router error: {e}",
                "rewritten_query": user_prompt,
                "chapter_range": None,
                "chapter_range_mode": None,
                "chapter_range_count": 5,
            }


# ==========================================
# üìö CONTEXT MANAGER (V5 - Entity relations, parent_id)
# ==========================================
class ContextManager:
    """Qu·∫£n l√Ω context cho AI v·ªõi kh·∫£ nƒÉng k·∫øt h·ª£p nhi·ªÅu ngu·ªìn"""

    @staticmethod
    def get_entity_relations(entity_id: Any, project_id: str) -> str:
        """L·∫•y quan h·ªá c·ªßa entity: t·ª´ b·∫£ng entity_relations (n·∫øu c√≥) v√† c√°c bi·∫øn th·ªÉ (parent_id) t·ª´ story_bible. Tr·∫£ v·ªÅ chu·ªói d·∫°ng '> [RELATION]: ...'. Defensive: kh√¥ng crash n·∫øu b·∫£ng/ c·ªôt ch∆∞a c√≥."""
        lines = []
        try:
            services = init_services()
            if not services:
                return ""
            supabase = services["supabase"]

            try:
                rel_res = supabase.table("entity_relations").select("*").or_(
                    f"source_entity_id.eq.{entity_id},target_entity_id.eq.{entity_id}"
                ).execute()
            except Exception:
                try:
                    rel_res = supabase.table("entity_relations").select("*").or_(
                        f"entity_id.eq.{entity_id},target_entity_id.eq.{entity_id}"
                    ).execute()
                except Exception:
                    rel_res = None
            if rel_res:
                if rel_res.data:
                    id_to_name = {}
                    for r in rel_res.data:
                        eid = r.get("entity_id") or r.get("source_entity_id") or r.get("from_entity_id")
                        tid = r.get("target_entity_id") or r.get("to_entity_id")
                        if eid and eid not in id_to_name:
                            id_to_name[eid] = None
                        if tid and tid not in id_to_name:
                            id_to_name[tid] = None
                    if id_to_name:
                        sb = supabase.table("story_bible").select("id, entity_name").eq(
                            "story_id", project_id
                        ).in_("id", list(id_to_name.keys())).execute()
                        if sb.data:
                            for row in sb.data:
                                id_to_name[row.get("id")] = row.get("entity_name") or ""
                    for r in rel_res.data:
                        rel_type = r.get("relation_type") or r.get("relation") or "li√™n quan"
                        eid = r.get("entity_id") or r.get("source_entity_id") or r.get("from_entity_id")
                        tid = r.get("target_entity_id") or r.get("to_entity_id")
                        name_a = id_to_name.get(eid) if eid else ""
                        name_b = id_to_name.get(tid) if tid else ""
                        if name_a or name_b:
                            lines.append(f"> [RELATION]: {name_a or 'Entity'} l√† {rel_type} c·ªßa {name_b or 'Entity'}.")

            try:
                variants = supabase.table("story_bible").select("entity_name, description").eq(
                    "story_id", project_id
                ).eq("parent_id", entity_id).execute()
                if variants.data:
                    for v in variants.data:
                        name = v.get("entity_name") or ""
                        desc = (v.get("description") or "")[:200]
                        if name:
                            lines.append(f"> [RELATION]: Bi·∫øn th·ªÉ: {name} ‚Äî {desc}...")
            except Exception:
                pass
        except Exception as e:
            print(f"get_entity_relations error: {e}")
        return "\n".join(lines) if lines else ""

    # Gi·ªõi h·∫°n token khi load nhi·ªÅu ch∆∞∆°ng (∆∞u ti√™n summary n·∫øu v∆∞·ª£t)
    DEFAULT_CHAPTER_TOKEN_LIMIT = 60000

    @staticmethod
    def _resolve_chapter_range(
        project_id: str,
        chapter_range_mode: Optional[str],
        chapter_range_count: int,
        chapter_range: Optional[List[int]],
    ) -> Optional[Tuple[int, int]]:
        """Tr·∫£ v·ªÅ (start, end) chapter_number t·ª´ router. first/latest query DB; range d√πng tr·ª±c ti·∫øp."""
        try:
            services = init_services()
            if not services:
                return None
            supabase = services["supabase"]
            count = max(1, min(50, int(chapter_range_count) if chapter_range_count else 5))

            if chapter_range_mode == "range" and chapter_range and len(chapter_range) >= 2:
                return (int(chapter_range[0]), int(chapter_range[1]))

            if chapter_range_mode == "first":
                r = supabase.table("chapters").select("chapter_number").eq(
                    "story_id", project_id
                ).order("chapter_number").limit(1).execute()
                if r.data and len(r.data) > 0:
                    start = int(r.data[0].get("chapter_number", 1))
                    return (start, start + count - 1)
                return (1, count)

            if chapter_range_mode == "latest":
                r = supabase.table("chapters").select("chapter_number").eq(
                    "story_id", project_id
                ).order("chapter_number", desc=True).limit(1).execute()
                if r.data and len(r.data) > 0:
                    end = int(r.data[0].get("chapter_number", 1))
                    start = max(1, end - count + 1)
                    return (start, end)
                return (1, count)

        except Exception as e:
            print(f"_resolve_chapter_range error: {e}")
        return None

    @staticmethod
    def load_chapters_by_range(
        project_id: str,
        start: int,
        end: int,
        token_limit: int = 60000,
    ) -> Tuple[str, List[str]]:
        """Load ch∆∞∆°ng theo kho·∫£ng chapter_number; c√≥ summary v√† art_style; n·∫øu v∆∞·ª£t token_limit th√¨ ∆∞u ti√™n summary cho ch∆∞∆°ng c≈©, full content cho ch∆∞∆°ng ƒëang b√†n (cu·ªëi)."""
        try:
            services = init_services()
            if not services:
                return "", []
            supabase = services["supabase"]
            r = supabase.table("chapters").select("*").eq(
                "story_id", project_id
            ).gte("chapter_number", start).lte("chapter_number", end).order(
                "chapter_number"
            ).execute()
            rows = r.data if r.data else []
        except Exception as e:
            print(f"load_chapters_by_range error: {e}")
            return "", []

        full_text = ""
        loaded_sources = []
        total_tokens = 0
        focus_idx = len(rows) - 1 if rows else -1

        for i, item in enumerate(rows):
            title = item.get("title") or f"Ch∆∞∆°ng {item.get('chapter_number', i+1)}"
            content = item.get("content") or ""
            summary = item.get("summary") or ""
            art_style = item.get("art_style") or ""
            use_full = (token_limit <= 0 or total_tokens < token_limit) or (i == focus_idx)
            block = f"\n\n=== üìÑ {title} ===\n"
            if summary:
                block += f"[Summary]: {summary}\n"
            if art_style:
                block += f"[Art style]: {art_style}\n"
            if use_full and content:
                block += f"[Content]:\n{content}\n"
            elif summary and not use_full:
                block += f"(Ch·ªâ t√≥m t·∫Øt do gi·ªõi h·∫°n token.)\n"
            full_text += block
            loaded_sources.append(f"üìÑ {title}")
            total_tokens += AIService.estimate_tokens(block)

        return full_text, loaded_sources

    @staticmethod
    def load_full_content(
        file_names: List[str],
        project_id: str,
        token_limit: int = 60000,
        focus_chapter_name: Optional[str] = None,
    ) -> Tuple[str, List[str]]:
        """Load n·ªôi dung file/ch∆∞∆°ng; th√™m summary v√† art_style; n·∫øu v∆∞·ª£t token_limit th√¨ ∆∞u ti√™n summary, full content cho ch∆∞∆°ng focus."""
        if not file_names:
            return "", []

        try:
            services = init_services()
            supabase = services["supabase"]
        except Exception:
            return "", []

        full_text = ""
        loaded_sources = []
        total_tokens = 0
        rows_with_meta = []

        for name in file_names:
            try:
                res = supabase.table("chapters").select("*").eq(
                    "story_id", project_id
                ).ilike("title", f"%{name}%").execute()
            except Exception:
                res = type("Res", (), {"data": None})()

            if res.data and len(res.data) > 0:
                item = res.data[0]
                item["_name"] = name
                item["_is_focus"] = (focus_chapter_name and focus_chapter_name in (item.get("title") or ""))
                rows_with_meta.append(item)
            else:
                try:
                    res_bible = supabase.table("story_bible").select(
                        "entity_name, description"
                    ).eq("story_id", project_id).ilike("entity_name", f"%{name}%").execute()
                    if res_bible.data and len(res_bible.data) > 0:
                        item = res_bible.data[0]
                        full_text += f"\n\n=== ‚ö†Ô∏è BIBLE SUMMARY: {item.get('entity_name', name)} ===\n{item.get('description', '')}\n"
                        loaded_sources.append(f"üóÇÔ∏è {item.get('entity_name', name)} (Summary)")
                except Exception:
                    pass

        for item in rows_with_meta:
            title = item.get("title") or f"Ch∆∞∆°ng {item.get('chapter_number')}"
            content = item.get("content") or ""
            summary = item.get("summary") or ""
            art_style = item.get("art_style") or ""
            is_focus = item.get("_is_focus", False)
            use_full = token_limit <= 0 or total_tokens + AIService.estimate_tokens(content) <= token_limit or is_focus
            block = f"\n\n=== üìÑ SOURCE FILE/CHAP: {title} ===\n"
            if summary:
                block += f"[Summary]: {summary}\n"
            if art_style:
                block += f"[Art style]: {art_style}\n"
            if use_full and content:
                block += f"[Content]:\n{content}\n"
            elif summary:
                block += "(Ch·ªâ t√≥m t·∫Øt do gi·ªõi h·∫°n token.)\n"
            full_text += block
            loaded_sources.append(f"üìÑ {title}")
            total_tokens += AIService.estimate_tokens(block)

        return full_text, loaded_sources

    @staticmethod
    def get_mandatory_rules(project_id: str) -> str:
        """L·∫•y t·∫•t c·∫£ c√°c lu·∫≠t (RULE) b·∫Øt bu·ªôc"""
        try:
            services = init_services()
            supabase = services['supabase']

            res = supabase.table("story_bible") \
                .select("description") \
                .eq("story_id", project_id) \
                .ilike("entity_name", "%[RULE]%") \
                .execute()

            if res.data:
                rules_text = "\n".join([f"- {r['description']}" for r in res.data])
                return f"\nüî• --- MANDATORY RULES ---\n{rules_text}\n"
            return ""
        except Exception as e:
            print(f"Error getting rules: {e}")
            return ""

    @staticmethod
    def build_context(
        router_result: Dict,
        project_id: str,
        persona: Dict,
        strict_mode: bool = False
    ) -> Tuple[str, List[str], int]:
        """X√¢y d·ª±ng context t·ª´ router result v·ªõi kh·∫£ nƒÉng k·∫øt h·ª£p"""
        context_parts = []
        sources = []
        total_tokens = 0

        persona_text = f"üé≠ PERSONA: {persona['role']}\n{persona['core_instruction']}\n"
        context_parts.append(persona_text)
        total_tokens += AIService.estimate_tokens(persona_text)

        if strict_mode:
            strict_text = """
            \n\n‚ÄºÔ∏è CH·∫æ ƒê·ªò NGHI√äM NG·∫∂T (STRICT MODE) ƒêANG B·∫¨T:
            1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong [CONTEXT].
            2. TUY·ªÜT ƒê·ªêI KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c d√πng ki·∫øn th·ª©c b√™n ngo√†i ƒë·ªÉ ƒëi·ªÅn v√†o ch·ªó tr·ªëng.
            3. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong Context, h√£y tr·∫£ l·ªùi: "D·ªØ li·ªáu d·ª± √°n ch∆∞a c√≥ th√¥ng tin n√†y."
            4. N·∫øu User h·ªèi v·ªÅ "l·ªãch s·ª≠", "c·ªët truy·ªán", h√£y ∆∞u ti√™n tr√≠ch xu·∫•t t·ª´ [KNOWLEDGE BASE].
            5. Kh√¥ng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c d·ªØ li·ªáu th·ª±c t·∫ø (fact) ch·ªâ v√¨ t√≠nh c√°ch Persona.
            """
            context_parts.append(strict_text)
            total_tokens += AIService.estimate_tokens(strict_text)

        rules_text = ContextManager.get_mandatory_rules(project_id)
        if rules_text:
            context_parts.append(rules_text)
            total_tokens += AIService.estimate_tokens(rules_text)

        intent = router_result.get("intent", "chat_casual")
        target_files = router_result.get("target_files", [])
        target_bible_entities = router_result.get("target_bible_entities", [])
        chapter_range_mode = router_result.get("chapter_range_mode")
        chapter_range_count = router_result.get("chapter_range_count", 5)
        chapter_range = router_result.get("chapter_range")

        if intent == "read_full_content":
            full_text, source_names = "", []
            range_bounds = ContextManager._resolve_chapter_range(
                project_id, chapter_range_mode, chapter_range_count, chapter_range
            )
            if range_bounds is not None:
                full_text, source_names = ContextManager.load_chapters_by_range(
                    project_id, range_bounds[0], range_bounds[1],
                    token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
                )
            if not full_text and target_files:
                full_text, source_names = ContextManager.load_full_content(
                    target_files, project_id,
                    token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
                )
            if full_text:
                context_parts.append(f"\n--- TARGET CONTENT ---\n{full_text}")
                sources.extend(source_names)
                total_tokens += AIService.estimate_tokens(full_text)

        elif intent == "search_bible" or intent == "mixed_context":
            bible_context = ""
            for entity in target_bible_entities:
                raw_list = HybridSearch.smart_search_hybrid_raw(entity, project_id, top_k=2)
                if raw_list:
                    for item in raw_list:
                        try:
                            eid = item.get("id")
                            if eid is not None:
                                HybridSearch.update_lookup_stats(eid)
                        except Exception:
                            pass
                    main_id = raw_list[0].get("id") if raw_list else None
                    rel_block = ""
                    if main_id:
                        rel_text = ContextManager.get_entity_relations(main_id, project_id)
                        if rel_text:
                            rel_block = f"> [RELATION]:\n{rel_text}\n\n"
                    part = "\n".join(
                        f"- [{item.get('entity_name', '')}]: {item.get('description', '')}"
                        for item in raw_list
                    )
                    bible_context += f"\n--- {entity.upper()} ---\n{rel_block}{part}\n"

            if not bible_context and router_result.get("rewritten_query"):
                raw_list = HybridSearch.smart_search_hybrid_raw(
                    router_result["rewritten_query"], project_id, top_k=5
                )
                if raw_list:
                    for item in raw_list:
                        try:
                            eid = item.get("id")
                            if eid is not None:
                                HybridSearch.update_lookup_stats(eid)
                        except Exception:
                            pass
                    main_id = raw_list[0].get("id") if raw_list else None
                    rel_block = ""
                    if main_id:
                        rel_text = ContextManager.get_entity_relations(main_id, project_id)
                        if rel_text:
                            rel_block = f"> [RELATION]:\n{rel_text}\n\n"
                    part = "\n".join(
                        f"- [{item.get('entity_name', '')}]: {item.get('description', '')}"
                        for item in raw_list
                    )
                    bible_context = f"\n--- KNOWLEDGE BASE ---\n{rel_block}{part}\n"

            if bible_context:
                context_parts.append(bible_context)
                total_tokens += AIService.estimate_tokens(bible_context)
                sources.append("üìö Bible Search")

            try:
                services = init_services()
                supabase = services['supabase']
                related_chapter_nums = set()

                if target_bible_entities:
                    for entity in target_bible_entities:
                        res = supabase.table("story_bible") \
                            .select("source_chapter") \
                            .eq("story_id", project_id) \
                            .ilike("entity_name", f"%{entity}%") \
                            .execute()

                        if res.data:
                            for row in res.data:
                                if row.get('source_chapter') and row['source_chapter'] > 0:
                                    related_chapter_nums.add(row['source_chapter'])

                if related_chapter_nums:
                    chap_res = supabase.table("chapters") \
                        .select("title") \
                        .eq("story_id", project_id) \
                        .in_("chapter_number", list(related_chapter_nums)) \
                        .execute()

                    if chap_res.data:
                        auto_files = [c['title'] for c in chap_res.data if c.get('title')]

                        if auto_files:
                            extra_text, extra_sources = ContextManager.load_full_content(auto_files, project_id)

                            if extra_text:
                                context_parts.append(f"\n--- üïµÔ∏è AUTO-DETECTED CONTEXT (REVERSE LOOKUP) ---\n{extra_text}")
                                sources.extend([f"{s} (Auto)" for s in extra_sources])
                                total_tokens += AIService.estimate_tokens(extra_text)

            except Exception as e:
                print(f"Reverse lookup error: {e}")
                pass

        if intent == "mixed_context" and target_files:
            full_text, source_names = ContextManager.load_full_content(
                target_files, project_id,
                token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
            )
            if full_text:
                context_parts.append(f"\n--- RELATED FILES ---\n{full_text}")
                sources.extend(source_names)
                total_tokens += AIService.estimate_tokens(full_text)

        return "\n".join(context_parts), sources, total_tokens


# ==========================================
# üìù AUTO-SUMMARY / CHAPTER METADATA (V5)
# ==========================================
def suggest_import_category(text: str) -> str:
    """G·ª£i √Ω prefix/category cho n·ªôi dung import (d√πng LLM nh·∫π). Tr·∫£ v·ªÅ prefix v√≠ d·ª• [CHARACTER], [RULE]... ho·∫∑c [OTHER]. Defensive: l·ªói th√¨ tr·∫£ v·ªÅ [OTHER]."""
    if not text or len(text.strip()) < 20:
        return "[OTHER]"
    try:
        model = getattr(Config, "METADATA_MODEL", None) or "google/gemini-2.5-flash"
        prefixes = getattr(Config, "BIBLE_PREFIXES", ["[RULE]", "[CHARACTER]", "[LOCATION]", "[ITEM]", "[OTHER]"])
        prompt = f"""Ph√¢n lo·∫°i n·ªôi dung sau v√†o ƒê√öNG M·ªòT trong c√°c lo·∫°i (ch·ªâ tr·∫£ v·ªÅ chu·ªói lo·∫°i, kh√¥ng gi·∫£i th√≠ch):
{', '.join(prefixes)}

N·ªòI DUNG (r√∫t g·ªçn):
{text[:1500]}

Tr·∫£ v·ªÅ ƒë√∫ng m·ªôt chu·ªói, v√≠ d·ª•: [CHARACTER] ho·∫∑c [RULE]."""
        resp = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.1,
            max_tokens=50,
        )
        raw = (resp.choices[0].message.content or "").strip()
        for p in prefixes:
            if p in raw or p.strip("[]").lower() in raw.lower():
                return p
        return "[OTHER]"
    except Exception as e:
        print(f"suggest_import_category error: {e}")
        return "[OTHER]"


def generate_chapter_metadata(content: str) -> Dict[str, str]:
    """D√πng model r·∫ª (gemini/haiku/deepseek) ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung v√† ph√¢n t√≠ch art_style. Tr·∫£ v·ªÅ {"summary": "...", "art_style": "..."}. Defensive: tr·∫£ v·ªÅ dict r·ªóng n·∫øu l·ªói."""
    if not content or not str(content).strip():
        return {"summary": "", "art_style": ""}
    try:
        model = getattr(Config, "METADATA_MODEL", None) or "google/gemini-2.5-flash"
        prompt = f"""Ph√¢n t√≠ch ƒëo·∫°n vƒÉn/ch∆∞∆°ng sau v√† tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON v·ªõi 2 key:
- "summary": T√≥m t·∫Øt n·ªôi dung (2-4 c√¢u, ti·∫øng Vi·ªát).
- "art_style": Phong c√°ch vi·∫øt (v√≠ d·ª•: k·ªÉ chuy·ªán, m√¥ t·∫£, ƒë·ªëi tho·∫°i, h√†nh ƒë·ªông; 1-2 c√¢u).

N·ªòI DUNG:
{content[:12000]}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch. V√≠ d·ª•: {{"summary": "...", "art_style": "..."}}"""
        messages = [{"role": "user", "content": prompt}]
        response = AIService.call_openrouter(
            messages=messages,
            model=model,
            temperature=0.2,
            max_tokens=500,
            response_format={"type": "json_object"},
        )
        raw = response.choices[0].message.content
        raw = AIService.clean_json_text(raw)
        data = json.loads(raw)
        return {
            "summary": str(data.get("summary", ""))[:2000],
            "art_style": str(data.get("art_style", ""))[:500],
        }
    except Exception as e:
        print(f"generate_chapter_metadata error: {e}")
        return {"summary": "", "art_style": ""}


def get_file_sample(file_content: str, sample_size: int = 80) -> str:
    """
    L·∫•y m·∫´u r·∫£i r√°c: 80 d√≤ng ƒë·∫ßu + 80 d√≤ng gi·ªØa + 80 d√≤ng cu·ªëi (n·∫øu file d√†i).
    Tr·∫£ v·ªÅ chu·ªói k·∫øt h·ª£p v·ªõi marker [ƒê·∫¶U], [GI·ªÆA], [CU·ªêI].
    """
    if not file_content or not str(file_content).strip():
        return ""
    lines = str(file_content).strip().splitlines()
    total_lines = len(lines)
    if total_lines <= sample_size * 3:
        return "\n".join(lines)
    parts = []
    parts.append(f"[ƒê·∫¶U FILE - {sample_size} d√≤ng ƒë·∫ßu]")
    parts.append("\n".join(lines[:sample_size]))
    mid_start = total_lines // 2 - sample_size // 2
    parts.append(f"\n\n[GI·ªÆA FILE - {sample_size} d√≤ng gi·ªØa (t·ª´ d√≤ng {mid_start})]")
    parts.append("\n".join(lines[mid_start:mid_start + sample_size]))
    parts.append(f"\n\n[CU·ªêI FILE - {sample_size} d√≤ng cu·ªëi]")
    parts.append("\n".join(lines[-sample_size:]))
    return "\n".join(parts)


def analyze_split_strategy(
    file_content: str,
    file_type: str = "story",
    context_hint: str = "",
) -> Dict[str, Any]:
    """
    AI Analyzer (Nh·∫π): Ph√¢n t√≠ch m·∫´u r·∫£i r√°c (80 ƒë·∫ßu + 80 gi·ªØa + 80 cu·ªëi) ƒë·ªÉ t√¨m quy lu·∫≠t ph√¢n c√°ch.
    Tr·∫£ v·ªÅ {"split_type": "by_keyword"|"by_length"|"by_sheet", "split_value": str (regex/keyword)}.
    """
    if not file_content or not str(file_content).strip():
        return {"split_type": "by_length", "split_value": "2000"}
    sample = get_file_sample(file_content, sample_size=80)
    try:
        model = getattr(Config, "METADATA_MODEL", None) or "google/gemini-2.5-flash"
        type_hints = {
            "story": "Truy·ªán - t√¨m quy lu·∫≠t ph√¢n c√°ch ch∆∞∆°ng (VD: 'Ch∆∞∆°ng' vi·∫øt hoa, d·∫•u '***', xu·ªëng d√≤ng 2 l·∫ßn).",
            "character_data": "D·ªØ li·ªáu nh√¢n v·∫≠t - t√¨m quy lu·∫≠t ph√¢n c√°ch entity (VD: '##', '---', t√™n ri√™ng ·ªü ƒë·∫ßu d√≤ng).",
            "excel_export": "Excel/CSV - x√°c ƒë·ªãnh c·∫Øt theo 'Sheet' marker hay 'Row count' (s·ªë d√≤ng c·ªë ƒë·ªãnh).",
        }
        hint_text = type_hints.get(file_type.strip().lower(), type_hints["story"])
        if context_hint:
            hint_text += f"\nG·ª£i √Ω ng∆∞·ªùi d√πng: {context_hint}"
        prompt = f"""Ph√¢n t√≠ch m·∫´u file (80 d√≤ng ƒë·∫ßu + 80 d√≤ng gi·ªØa + 80 d√≤ng cu·ªëi) v√† T√åM QUY LU·∫¨T PH√ÇN C√ÅCH.

Lo·∫°i file: {hint_text}

M·∫™U FILE (240 d√≤ng t·ªïng h·ª£p):
---
{sample}
---

NHI·ªÜM V·ª§: T√¨m quy lu·∫≠t ph√¢n c√°ch ch∆∞∆°ng/th·ª±c th·ªÉ/sheet trong file n√†y.
- V√≠ d·ª•: "Ch∆∞∆°ng" vi·∫øt hoa ·ªü ƒë·∫ßu d√≤ng, d·∫•u "***", xu·ªëng d√≤ng 2 l·∫ßn, "[Sheet: X]", v.v.

Y√äU C·∫¶U: Tr·∫£ v·ªÅ ƒê√öNG M·ªòT JSON v·ªõi:
- "split_type": m·ªôt trong ["by_keyword", "by_length", "by_sheet"]
  * "by_keyword": T√¨m th·∫•y t·ª´ kh√≥a/pattern l·∫∑p l·∫°i ‚Üí tr·∫£ v·ªÅ regex pattern ho·∫∑c keyword ƒë∆°n gi·∫£n
  * "by_length": Kh√¥ng t√¨m th·∫•y pattern r√µ r√†ng ‚Üí c·∫Øt theo s·ªë k√Ω t·ª± c·ªë ƒë·ªãnh
  * "by_sheet": File Excel ‚Üí c·∫Øt theo Sheet marker
- "split_value": 
  * N·∫øu by_keyword: Regex pattern (VD: "^Ch∆∞∆°ng\\s+\\d+", "\\*{3,}", "^##\\s+") ho·∫∑c keyword ƒë∆°n gi·∫£n (VD: "Ch∆∞∆°ng", "---")
  * N·∫øu by_length: s·ªë k√Ω t·ª± (VD: "2000")
  * N·∫øu by_sheet: "Sheet" ho·∫∑c "Row count"

QUAN TR·ªåNG: Ch·ªâ tr·∫£ v·ªÅ Regex pattern ho·∫∑c Keyword ƒë·ªÉ Python d√πng `re` module c·∫Øt file. KH√îNG c·∫Øt th·ª±c t·∫ø.

V√≠ d·ª•: {{"split_type": "by_keyword", "split_value": "^Ch∆∞∆°ng\\s+\\d+"}}
Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch."""

        response = AIService.call_openrouter(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.2,
            max_tokens=500,
            response_format={"type": "json_object"},
        )
        raw = (response.choices[0].message.content or "").strip()
        raw = AIService.clean_json_text(raw)
        data = json.loads(raw)
        split_type = data.get("split_type", "by_length")
        split_value = str(data.get("split_value", "2000")).strip()
        if split_type not in ["by_keyword", "by_length", "by_sheet"]:
            split_type = "by_length"
        return {"split_type": split_type, "split_value": split_value}
    except Exception as e:
        print(f"analyze_split_strategy error: {e}")
        return {"split_type": "by_length", "split_value": "2000"}


def _build_smart_regex_pattern(keyword: str) -> str:
    """
    X√¢y d·ª±ng regex pattern h·ªó tr·ª£ c√≥ d·∫•u/kh√¥ng d·∫•u v√† kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng.
    VD: "Ch∆∞∆°ng" -> r"(?i)(CH∆Ø∆†NG|CHUONG|CHAPTER)\s+\d+[:\s-]*"
    """
    import re
    keyword_upper = keyword.upper().strip()
    if keyword_upper in ["CH∆Ø∆†NG", "CHUONG", "CHAPTER"]:
        return r"(?i)(CH∆Ø∆†NG|CHUONG|CHAPTER)\s+\d+[:\s-]*"
    elif keyword_upper in ["PH·∫¶N", "PHAN", "PART"]:
        return r"(?i)(PH·∫¶N|PHAN|PART)\s+\d+[:\s-]*"
    elif keyword_upper in ["---", "***", "==="]:
        return rf"(?i)\s*{re.escape(keyword)}\s*"
    else:
        return rf"(?i)^\s*{re.escape(keyword)}\s*"


def execute_split_logic(
    file_content: str,
    split_type: str,
    split_value: str,
    debug: bool = False,
) -> List[Dict[str, Any]]:
    """
    Python Worker (M·∫°nh): C·∫Øt file b·∫±ng code Python thu·∫ßn, kh√¥ng g·ªçi AI.
    Tr·∫£ v·ªÅ list of {"title": str, "content": str, "order": int}.
    debug=True: In ra debug log (d√πng trong Streamlit v·ªõi st.write).
    """
    if not file_content or not str(file_content).strip():
        return []
    content = str(file_content).strip()
    out = []
    try:
        if split_type == "by_keyword":
            import re
            pattern_str = split_value.strip()
            if not pattern_str:
                pattern_str = "---"
            
            is_regex = any(c in pattern_str for c in ["^", "$", "\\d", "\\s", "\\w", "\\+", "\\*", "\\?", "\\[", "\\(", "\\{", "("])
            
            if not is_regex:
                pattern_str = _build_smart_regex_pattern(pattern_str)
                is_regex = True
            
            try:
                pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
            except Exception as e:
                if debug:
                    print(f"Regex compile error: {e}, fallback to simple pattern")
                pattern_str = rf"^\s*{re.escape(split_value.strip())}\s*"
                pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
            
            matches = list(pattern.finditer(content))
            if debug:
                import streamlit as st
                st.write(f"üîç **Debug Log:** T√¨m th·∫•y **{len(matches)}** v·ªã tr√≠ ph√¢n c√°ch:")
                for i, m in enumerate(matches[:10]):
                    line_num = content[:m.start()].count('\n') + 1
                    preview = content[max(0, m.start()-30):m.end()+30].replace('\n', ' ')
                    st.code(f"{i+1}. D√≤ng {line_num}: ...{preview}...", language=None)
                if len(matches) > 10:
                    st.caption(f"... v√† {len(matches) - 10} v·ªã tr√≠ kh√°c")
            
            if len(matches) == 0:
                if debug:
                    import streamlit as st
                    st.error("‚ùå **Kh√¥ng t√¨m th·∫•y d·∫•u hi·ªáu ph√¢n chia ch∆∞∆°ng.** Vui l√≤ng ki·ªÉm tra l·∫°i ƒë·ªãnh d·∫°ng ho·∫∑c th·ª≠ keyword/pattern kh√°c.")
                return []
            
            if matches[0].start() > 0:
                part_content = content[0:matches[0].start()].strip()
                if part_content:
                    title = matches[0].group(0).strip()[:50] if matches[0].group(0) else "Ph·∫ßn 0"
                    if not title or len(title.strip()) < 2:
                        first_line = part_content.splitlines()[0] if part_content.splitlines() else ""
                        title = first_line[:50] if first_line else "Ph·∫ßn 0"
                    out.append({"title": title, "content": part_content, "order": 1})
            
            for i, match in enumerate(matches):
                start = match.start()
                end = matches[i+1].start() if i+1 < len(matches) else len(content)
                part_content = content[start:end].strip()
                if not part_content:
                    continue
                title = match.group(0).strip()[:50] if match.group(0) else f"Ph·∫ßn {len(out)+1}"
                if not title or len(title.strip()) < 2:
                    first_line = part_content.splitlines()[0] if part_content.splitlines() else ""
                    title = first_line[:50] if first_line else f"Ph·∫ßn {len(out)+1}"
                out.append({"title": title, "content": part_content, "order": len(out) + 1})
        elif split_type == "by_length":
            chunk_size = int(split_value) if split_value.isdigit() else 2000
            chunk_size = max(500, min(chunk_size, 50000))
            lines = content.splitlines()
            current_chunk = []
            current_len = 0
            chunk_num = 1
            for line in lines:
                line_len = len(line) + 1
                if current_len + line_len > chunk_size and current_chunk:
                    chunk_text = "\n".join(current_chunk).strip()
                    if chunk_text:
                        out.append({"title": f"Ph·∫ßn {chunk_num}", "content": chunk_text, "order": chunk_num})
                        chunk_num += 1
                    current_chunk = [line]
                    current_len = line_len
                else:
                    current_chunk.append(line)
                    current_len += line_len
            if current_chunk:
                chunk_text = "\n".join(current_chunk).strip()
                if chunk_text:
                    out.append({"title": f"Ph·∫ßn {chunk_num}", "content": chunk_text, "order": chunk_num})
        elif split_type == "by_sheet":
            import re
            if split_value.lower() == "row count" or split_value.isdigit():
                row_count = int(split_value) if split_value.isdigit() else 100
                lines = content.splitlines()
                for i in range(0, len(lines), row_count):
                    chunk_lines = lines[i:i + row_count]
                    if chunk_lines:
                        out.append({"title": f"Sheet {i // row_count + 1}", "content": "\n".join(chunk_lines), "order": i // row_count + 1})
            elif "[Sheet:" in content or "[Sheet " in content:
                pattern = re.compile(r"\[Sheet[:\s]+([^\]]+)\]", re.IGNORECASE)
                parts = pattern.split(content)
                current_sheet = "Sheet 1"
                current_content = []
                idx = 0
                for i, part in enumerate(parts):
                    if i % 2 == 0:
                        if part.strip():
                            current_content.append(part.strip())
                    else:
                        if current_content:
                            out.append({"title": current_sheet, "content": "\n".join(current_content), "order": idx + 1})
                            idx += 1
                        current_sheet = part.strip() or f"Sheet {idx + 2}"
                        current_content = []
                if current_content:
                    out.append({"title": current_sheet, "content": "\n".join(current_content), "order": idx + 1})
            else:
                out.append({"title": "Ph·∫ßn 1", "content": content, "order": 1})
        else:
            out.append({"title": "Ph·∫ßn 1", "content": content, "order": 1})
        return out
    except Exception as e:
        print(f"execute_split_logic error: {e}")
        return [{"title": "Ph·∫ßn 1", "content": content, "order": 1}]


# ==========================================
# üß¨ RULE MINING SYSTEM
# ==========================================
class RuleMiningSystem:
    """H·ªá th·ªëng khai th√°c v√† qu·∫£n l√Ω lu·∫≠t t·ª´ chat"""

    @staticmethod
    def extract_rule_raw(user_prompt: str, ai_response: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t lu·∫≠t th√¥ t·ª´ h·ªôi tho·∫°i"""
        prompt = f"""
        B·∫°n l√† "Trinh S√°t Lu·∫≠t" (Rule Scout). Nhi·ªám v·ª•: Ph√°t hi·ªán s·ªü th√≠ch/y√™u c·∫ßu c·ªßa User.

        H·ªòI THO·∫†I:
        - User: "{user_prompt}"
        - AI: (Ph·∫£n h·ªìi tr∆∞·ªõc ƒë√≥...)

        M·ª§C TI√äU:
        Ph√°t hi·ªán xem User c√≥ ƒëang ng·∫ßm ch·ªâ ƒë·ªãnh C√ÅCH L√ÄM VI·ªÜC, C√ÅCH VI·∫æT, ho·∫∑c ƒê·ªäNH D·∫†NG kh√¥ng.

        TI√äU CH√ç (ƒê·ªô nh·∫°y cao):
        1. Y√™u c·∫ßu ƒë·ªãnh d·∫°ng: "ch·ªâ json", "d√πng markdown", "ƒë·ª´ng vi·∫øt code", "vi·∫øt ng·∫Øn th√¥i".
        2. ƒêi·ªÅu ch·ªânh vƒÉn phong: "nghi√™m t√∫c h∆°n", "b·ªõt n√≥i nh·∫£m", "d√πng ti·∫øng Vi·ªát".
        3. S·ª≠a l·ªói: "sai r·ªìi", "kh√¥ng ph·∫£i th·∫ø", "l√†m th·∫ø n√†y m·ªõi ƒë√∫ng".

        H∆Ø·ªöNG D·∫™N:
        - N·∫øu User n√≥i: "Vi·∫øt c√°i n√†y b·∫±ng Python nh√©" -> T·∫°o lu·∫≠t: "Lu√¥n ∆∞u ti√™n d√πng Python".
        - Th√† b·∫Øt nh·∫ßm c√≤n h∆°n b·ªè s√≥t.

        OUTPUT:
        - N·∫øu ph√°t hi·ªán lu·∫≠t: Tr·∫£ v·ªÅ 1 c√¢u m·ªánh l·ªánh ng·∫Øn g·ªçn k√®m ng·ªØ c·∫£nh (Ti·∫øng Vi·ªát). V√≠ d·ª•: "Lu√¥n tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON khi ƒë∆∞·ª£c y√™u c·∫ßu...", "Kh√¥ng gi·∫£i th√≠ch d√†i d√≤ng khi user ƒëang kh√≥ ch·ªãu...".
        - N·∫øu ch·ªâ l√† ch√†o h·ªèi/c·∫£m ∆°n: Tr·∫£ v·ªÅ "NO_RULE".

        Ch·ªâ tr·∫£ v·ªÅ Text.
        """

        messages = [
            {"role": "system", "content": "You are Rule Extractor. Return text only."},
            {"role": "user", "content": prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=Config.ROUTER_MODEL,
                temperature=0.3,
                max_tokens=300
            )

            text = response.choices[0].message.content.strip()

            if "NO_RULE" in text or len(text) < 5:
                return None
            return text
        except Exception as e:
            print(f"Rule extraction error: {e}")
            return None

    @staticmethod
    def analyze_rule_conflict(new_rule_content: str, project_id: str) -> Dict:
        """Check rule conflict with DB - Safe Version"""
        similar_rules_str = HybridSearch.smart_search_hybrid(new_rule_content, project_id, top_k=3)

        if not similar_rules_str:
            return {
                "status": "NEW",
                "reason": "No conflicts found",
                "existing_rule_summary": "None",
                "merged_content": None,
                "suggested_content": new_rule_content
            }

        judge_prompt = f"""
        Lu·∫≠t M·ªõi: "{new_rule_content}"
        Lu·∫≠t C≈© trong DB: "{similar_rules_str}"

        Nhi·ªám v·ª•: So s√°nh m·ªëi quan h·ªá.

        - CONFLICT (Xung ƒë·ªôt): M√¢u thu·∫´n tr·ª±c ti·∫øp (Vd: C≈© b·∫£o A, M·ªõi b·∫£o kh√¥ng A).
        - MERGE (G·ªôp): C√πng ch·ªß ƒë·ªÅ nh∆∞ng lu·∫≠t M·ªõi chi ti·∫øt h∆°n ho·∫∑c b·ªï sung cho lu·∫≠t C≈©.
        - NEW (M·ªõi): Ch·ªß ƒë·ªÅ kh√°c h·∫≥n.

        OUTPUT JSON ONLY:
        {{
            "status": "CONFLICT" | "MERGE" | "NEW",
            "existing_rule_summary": "T√≥m t·∫Øt lu·∫≠t c≈© (Ti·∫øng Vi·ªát)",
            "reason": "L√Ω do (Ti·∫øng Vi·ªát)",
            "merged_content": "N·ªôi dung lu·∫≠t ƒë√£ g·ªôp ho√†n ch·ªânh (n·∫øu MERGE). N·∫øu kh√°c th√¨ ƒë·ªÉ null."
        }}
        """

        messages = [
            {"role": "system", "content": "You are Rule Judge. Return only JSON."},
            {"role": "user", "content": judge_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=Config.ROUTER_MODEL,
                temperature=0.2,
                max_tokens=4000,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            content = AIService.clean_json_text(content)

            result = json.loads(content)

            return {
                "status": result.get("status", "NEW"),
                "reason": result.get("reason", "No reason provided by AI"),
                "existing_rule_summary": result.get("existing_rule_summary", "N/A"),
                "merged_content": result.get("merged_content", None),
                "suggested_content": new_rule_content
            }

        except Exception as e:
            print(f"Rule analysis error: {e}")
            return {
                "status": "NEW",
                "reason": f"AI Judge Error: {str(e)}",
                "existing_rule_summary": "Error analyzing",
                "merged_content": None,
                "suggested_content": new_rule_content
            }

    @staticmethod
    def crystallize_session(chat_history: List[Dict], persona_role: str) -> str:
        """T√≥m t·∫Øt v√† l·ªçc th√¥ng tin gi√° tr·ªã t·ª´ chat history"""
        chat_text = "\n".join([f"{m['role']}: {m['content']}" for m in chat_history])

        crystallize_prompt = f"""
        B·∫°n l√† Th∆∞ K√Ω Cu·ªôc H·ªçp ({persona_role}).
        
        Nhi·ªám v·ª•: ƒê·ªçc ƒëo·∫°n chat d∆∞·ªõi ƒë√¢y v√† L·ªåC B·ªé NH·ªÆNG TH·ª® V√î NGHƒ®A.
        Ch·ªâ gi·ªØ l·∫°i v√† T√ìM T·∫ÆT nh·ªØng th√¥ng tin gi√° tr·ªã (S·ª± ki·ªán, √ù t∆∞·ªüng, Quy·∫øt ƒë·ªãnh, Lore m·ªõi).

        CHAT LOG: {chat_text}

        OUTPUT: Tr·∫£ v·ªÅ b·∫£n t√≥m t·∫Øt s√∫c t√≠ch (50-100 t·ª´) b·∫±ng Ti·∫øng Vi·ªát. 
        N·∫øu to√†n l√† ch√†o h·ªèi v√¥ nghƒ©a, tr·∫£ v·ªÅ "NO_INFO".
        """

        messages = [
            {"role": "system", "content": "You are Conversation Summarizer. Return text only."},
            {"role": "user", "content": crystallize_prompt}
        ]

        try:
            response = AIService.call_openrouter(
                messages=messages,
                model=Config.ROUTER_MODEL,
                temperature=0.3,
                max_tokens=8000
            )

            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Crystallize error: {e}")
            return f"AI Error: {e}"
