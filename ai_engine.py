# ai_engine.py - Router, Context, Rule Mining (AIService + context_helpers ƒë√£ t√°ch ra ai/)
import json
import re
from collections import defaultdict
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Tuple, Any

import streamlit as st

from config import Config, init_services

from ai.service import AIService, _get_default_tool_model
from ai.context_helpers import get_mandatory_rules as _get_mandatory_rules, resolve_chapter_range as _resolve_chapter_range, get_entity_relations as _get_entity_relations
from ai.hybrid_search import HybridSearch, check_semantic_intent, search_chunks_vector
from ai.query_sql import VALID_QUERY_TARGETS, build_query_sql_context, infer_query_target
from ai.router import get_v7_reminder_message, is_multi_intent_request, is_multi_step_update_data_request, SmartAIRouter
from ai.evaluate import evaluate_step_outcome, replan_after_step
from ai.content import (
    suggest_relations,
    suggest_import_category,
    generate_arc_summary_from_chapters,
    generate_chapter_metadata,
    extract_timeline_events_from_content,
    get_file_sample,
    analyze_split_strategy,
    execute_split_logic,
)
from ai.rule_mining import RuleMiningSystem
from ai.context_schema import (
    normalize_context_needs,
    infer_default_context_needs,
    normalize_context_priority,
)
from ai.utils import (
    cap_context_to_tokens,
    cap_chat_history_to_tokens,
    ROUTER_PLANNER_CHAT_HISTORY_MAX_TOKENS,
    get_chapter_list_for_router,
    parse_chapter_range_from_query,
    extract_prefix,
    get_prefix_key_from_entity_name,
    _get_prefix_section_order_and_labels,
    _filter_bible_by_chapter_range,
    format_bible_context_by_sections,
    get_bible_index,
    get_bible_entries,
    get_timeline_events,
)

try:
    from core.arc_service import ArcService
    from core.reverse_lookup import ReverseLookupAssembler
except ImportError:
    ArcService = None
    ReverseLookupAssembler = None


# ==========================================
# üìö CONTEXT MANAGER (V5 + V6 Arc & Reverse Lookup)
# ==========================================
class ContextManager:
    """Qu·∫£n l√Ω context cho AI v·ªõi kh·∫£ nƒÉng k·∫øt h·ª£p nhi·ªÅu ngu·ªìn. V6: Arc scoping + Triangle assembler."""

    @staticmethod
    def _build_arc_scope_context(project_id: str, current_arc_id: Optional[str], session_state: Optional[Dict] = None) -> Tuple[str, int]:
        """
        V6 MODULE 1 & 3: Build [Past Arc Summaries] + [Current Arc] for Sequential/Standalone.
        Global Bible is still injected via get_mandatory_rules and search_bible below.
        Returns (text, estimated_tokens).
        """
        if not ArcService or not current_arc_id:
            return "", 0
        arc = ArcService.get_arc(current_arc_id)
        if not arc:
            return "", 0
        parts = []
        scope = ArcService.get_scope_for_search(project_id, current_arc_id)
        if scope.get("scope_type") == ArcService.ARC_TYPE_SEQUENTIAL and scope.get("arc_summaries"):
            parts.append("[PAST ARC SUMMARIES - Timeline Inheritance]")
            for a in scope["arc_summaries"]:
                parts.append("- ARC: %s\n  Summary: %s" % (a.get("name", ""), (a.get("summary") or "").strip() or "(none)"))
            parts.append("")
        parts.append("[MACRO CONTEXT - ARC: %s]" % (arc.get("name") or "Current"))
        parts.append("Summary: %s" % ((arc.get("summary") or "").strip() or "(none)"))
        text = "\n".join(parts)
        return text, AIService.estimate_tokens(text)

    @staticmethod
    def build_context_with_chunk_reverse_lookup(
        project_id: str,
        chunk_ids: List[str],
        current_arc_id: Optional[str],
        token_limit: int = 12000,
    ) -> Tuple[str, List[str], int]:
        """
        V6 MODULE 3: Assemble context from chunk IDs using Triangle (Macro/Meso/Micro).
        Optionally prepend arc scope. Returns (full_context, sources, total_tokens).
        """
        context_parts = []
        sources = []
        total_tokens = 0
        if ArcService and current_arc_id:
            arc_scope, t = ContextManager._build_arc_scope_context(project_id, current_arc_id, None)
            if arc_scope:
                context_parts.append(arc_scope)
                total_tokens += t
        if ReverseLookupAssembler and chunk_ids:
            assembled, chunk_sources = ReverseLookupAssembler.assemble_from_chunks(chunk_ids, token_limit=token_limit)
            if assembled:
                context_parts.append("[REVERSE LOOKUP - Micro to Macro Evidence]\n" + assembled)
                total_tokens += AIService.estimate_tokens(assembled)
                sources.extend(chunk_sources)
        return "\n\n".join(context_parts), sources, total_tokens

    @staticmethod
    def get_entity_relations(entity_id: Any, project_id: str) -> str:
        """·ª¶y quy·ªÅn cho ai.context_helpers.get_entity_relations."""
        return _get_entity_relations(entity_id, project_id)

    # Gi·ªõi h·∫°n token khi load nhi·ªÅu ch∆∞∆°ng (∆∞u ti√™n summary n·∫øu v∆∞·ª£t)
    DEFAULT_CHAPTER_TOKEN_LIMIT = 60000

    @staticmethod
    def _resolve_chapter_range(
        project_id: str,
        chapter_range_mode: Optional[str],
        chapter_range_count: int,
        chapter_range: Optional[List[int]],
    ) -> Optional[Tuple[int, int]]:
        """·ª¶y quy·ªÅn cho ai.context_helpers.resolve_chapter_range."""
        return _resolve_chapter_range(project_id, chapter_range_mode, chapter_range_count, chapter_range)

    @staticmethod
    def load_chapters_by_range(
        project_id: str,
        start: int,
        end: int,
        token_limit: int = 60000,
    ) -> Tuple[str, List[str]]:
        """Load ch∆∞∆°ng theo kho·∫£ng chapter_number; c√≥ summary v√† art_style; n·∫øu v∆∞·ª£t token_limit th√¨ ∆∞u ti√™n summary cho ch∆∞∆°ng c≈©, full content cho ch∆∞∆°ng ƒëang b√†n (cu·ªëi)."""
        try:
            services = init_services()
            if not services:
                return "", []
            supabase = services["supabase"]
            r = supabase.table("chapters").select("*").eq(
                "story_id", project_id
            ).gte("chapter_number", start).lte("chapter_number", end).order(
                "chapter_number"
            ).execute()
            rows = r.data if r.data else []
        except Exception as e:
            print(f"load_chapters_by_range error: {e}")
            return "", []

        full_text = ""
        loaded_sources = []
        total_tokens = 0
        focus_idx = len(rows) - 1 if rows else -1

        for i, item in enumerate(rows):
            title = item.get("title") or f"Ch∆∞∆°ng {item.get('chapter_number', i+1)}"
            content = item.get("content") or ""
            summary = item.get("summary") or ""
            art_style = item.get("art_style") or ""
            use_full = (token_limit <= 0 or total_tokens < token_limit) or (i == focus_idx)
            block = f"\n\n=== üìÑ {title} ===\n"
            if summary:
                block += f"[Summary]: {summary}\n"
            if art_style:
                block += f"[Art style]: {art_style}\n"
            if use_full and content:
                block += f"[Content]:\n{content}\n"
            elif summary and not use_full:
                block += f"(Ch·ªâ t√≥m t·∫Øt do gi·ªõi h·∫°n token.)\n"
            full_text += block
            loaded_sources.append(f"üìÑ {title}")
            total_tokens += AIService.estimate_tokens(block)

        return full_text, loaded_sources

    @staticmethod
    def load_full_content(
        file_names: List[str],
        project_id: str,
        token_limit: int = 60000,
        focus_chapter_name: Optional[str] = None,
    ) -> Tuple[str, List[str]]:
        """Load n·ªôi dung file/ch∆∞∆°ng; th√™m summary v√† art_style; n·∫øu v∆∞·ª£t token_limit th√¨ ∆∞u ti√™n summary, full content cho ch∆∞∆°ng focus."""
        if not file_names:
            return "", []

        try:
            services = init_services()
            supabase = services["supabase"]
        except Exception:
            return "", []

        full_text = ""
        loaded_sources = []
        total_tokens = 0
        rows_with_meta = []

        for name in file_names:
            try:
                res = supabase.table("chapters").select("*").eq(
                    "story_id", project_id
                ).ilike("title", f"%{name}%").execute()
            except Exception:
                res = type("Res", (), {"data": None})()

            if res.data and len(res.data) > 0:
                item = res.data[0]
                item["_name"] = name
                item["_is_focus"] = (focus_chapter_name and focus_chapter_name in (item.get("title") or ""))
                rows_with_meta.append(item)
            else:
                try:
                    res_bible = supabase.table("story_bible").select(
                        "entity_name, description"
                    ).eq("story_id", project_id).ilike("entity_name", f"%{name}%").execute()
                    if res_bible.data and len(res_bible.data) > 0:
                        item = res_bible.data[0]
                        full_text += f"\n\n=== ‚ö†Ô∏è BIBLE SUMMARY: {item.get('entity_name', name)} ===\n{item.get('description', '')}\n"
                        loaded_sources.append(f"üóÇÔ∏è {item.get('entity_name', name)} (Summary)")
                except Exception:
                    pass

        for item in rows_with_meta:
            title = item.get("title") or f"Ch∆∞∆°ng {item.get('chapter_number')}"
            content = item.get("content") or ""
            summary = item.get("summary") or ""
            art_style = item.get("art_style") or ""
            is_focus = item.get("_is_focus", False)
            use_full = token_limit <= 0 or total_tokens + AIService.estimate_tokens(content) <= token_limit or is_focus
            block = f"\n\n=== üìÑ SOURCE FILE/CHAP: {title} ===\n"
            if summary:
                block += f"[Summary]: {summary}\n"
            if art_style:
                block += f"[Art style]: {art_style}\n"
            if use_full and content:
                block += f"[Content]:\n{content}\n"
            elif summary:
                block += "(Ch·ªâ t√≥m t·∫Øt do gi·ªõi h·∫°n token.)\n"
            full_text += block
            loaded_sources.append(f"üìÑ {title}")
            total_tokens += AIService.estimate_tokens(block)

        return full_text, loaded_sources

    @staticmethod
    def get_mandatory_rules(project_id: str) -> str:
        """·ª¶y quy·ªÅn cho ai.context_helpers.get_mandatory_rules."""
        return _get_mandatory_rules(project_id)

    @staticmethod
    def build_context(
        router_result: Dict,
        project_id: str,
        persona: Dict,
        strict_mode: bool = False,
        current_arc_id: Optional[str] = None,
        session_state: Optional[Dict] = None,
        free_chat_mode: bool = False,
        max_context_tokens: Optional[int] = None,
    ) -> Tuple[str, List[str], int]:
        """X√¢y d·ª±ng context t·ª´ router result. max_context_tokens: gi·ªõi h·∫°n ƒë·ªô d√†i (t·ª´ Settings Context Size); None = kh√¥ng gi·ªõi h·∫°n."""
        context_parts = []
        sources = []
        total_tokens = 0

        persona_text = f"üé≠ PERSONA: {persona['role']}\n{persona['core_instruction']}\n"
        context_parts.append(persona_text)
        total_tokens += AIService.estimate_tokens(persona_text)

        if free_chat_mode:
            rules_text = ContextManager.get_mandatory_rules(project_id)
            if rules_text:
                context_parts.append(rules_text)
                total_tokens += AIService.estimate_tokens(rules_text)
            free_instruction = "[CH·∫æ ƒê·ªò CHAT T·ª∞ DO / CHAT PHI·∫æM]\nTr·∫£ l·ªùi nh∆∞ chatbot th√¥ng th∆∞·ªùng, d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t. Kh√¥ng b·∫Øt bu·ªôc d·ª±a v√†o d·ªØ li·ªáu d·ª± √°n (Bible/chunk/file); c√≥ th·ªÉ tr·∫£ l·ªùi m·ªçi ch·ªß ƒë·ªÅ."
            context_parts.append(free_instruction)
            total_tokens += AIService.estimate_tokens(free_instruction)
            sources.append("üåê Chat t·ª± do")
            return "\n".join(context_parts), sources, total_tokens

        # V6 MODULE 1: Arc scope (Past Arc Summaries + Current Arc)
        if current_arc_id and ArcService:
            arc_scope, arc_tokens = ContextManager._build_arc_scope_context(project_id, current_arc_id, session_state)
            if arc_scope:
                context_parts.append(arc_scope)
                total_tokens += arc_tokens
                sources.append("üìê Arc Scope")

        if strict_mode:
            strict_text = """
            \n\n‚ÄºÔ∏è CH·∫æ ƒê·ªò NGHI√äM NG·∫∂T (STRICT MODE) ƒêANG B·∫¨T:
            1. CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong [CONTEXT].
            2. TUY·ªÜT ƒê·ªêI KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c d√πng ki·∫øn th·ª©c b√™n ngo√†i ƒë·ªÉ ƒëi·ªÅn v√†o ch·ªó tr·ªëng.
            3. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong Context, h√£y tr·∫£ l·ªùi: "D·ªØ li·ªáu d·ª± √°n ch∆∞a c√≥ th√¥ng tin n√†y."
            4. N·∫øu User h·ªèi v·ªÅ "l·ªãch s·ª≠", "c·ªët truy·ªán", h√£y ∆∞u ti√™n tr√≠ch xu·∫•t t·ª´ [KNOWLEDGE BASE].
            5. Kh√¥ng t·ª´ ch·ªëi tr·∫£ l·ªùi c√°c d·ªØ li·ªáu th·ª±c t·∫ø (fact) ch·ªâ v√¨ t√≠nh c√°ch Persona.
            """
            context_parts.append(strict_text)
            total_tokens += AIService.estimate_tokens(strict_text)

        rules_text = ContextManager.get_mandatory_rules(project_id)
        if rules_text:
            context_parts.append(rules_text)
            total_tokens += AIService.estimate_tokens(rules_text)

        intent = router_result.get("intent", "chat_casual")
        target_files = router_result.get("target_files", [])
        target_bible_entities = router_result.get("target_bible_entities", [])
        chapter_range_mode = router_result.get("chapter_range_mode")
        chapter_range_count = router_result.get("chapter_range_count", 5)
        chapter_range = router_result.get("chapter_range")
        context_needs = normalize_context_needs(router_result.get("context_needs"))
        # Chu·∫©n h√≥a intent c≈© -> search_context
        if intent in ("read_full_content", "search_bible", "mixed_context", "manage_timeline", "search_chunks"):
            if not context_needs:
                if intent == "read_full_content":
                    context_needs = ["chapter"]
                elif intent == "manage_timeline":
                    context_needs = ["timeline"]
                elif intent == "search_chunks":
                    context_needs = ["chunk"]
                elif intent == "search_bible":
                    context_needs = ["bible", "relation"]
                else:
                    context_needs = ["bible", "relation", "chapter", "timeline", "chunk"]
            intent = "search_context"
        if not context_needs and intent == "search_context":
            context_needs = infer_default_context_needs(router_result)
        context_priority = normalize_context_priority(
            router_result.get("context_priority"), context_needs
        ) or list(context_needs)

        if intent == "web_search":
            try:
                from utils.web_search import web_search as do_web_search
                search_text = do_web_search(router_result.get("rewritten_query") or "", max_results=5)
            except Exception as ex:
                search_text = f"[WEB SEARCH] L·ªói: {ex}. Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c√≥ s·∫µn."
            context_parts.append(search_text)
            total_tokens += AIService.estimate_tokens(search_text)
            sources.append("üåê Web Search")

        elif intent == "ask_user_clarification":
            clarification_question = router_result.get("clarification_question", "") or "B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n c√¢u h·ªèi ho·∫∑c ch·ªß ƒë·ªÅ b·∫°n mu·ªën h·ªèi?"
            context_parts.append(f"[C·∫¶N L√ÄM R√ï]\nH·ªá th·ªëng c·∫ßn th√™m th√¥ng tin: {clarification_question}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn, l·ªãch s·ª± y√™u c·∫ßu user l√†m r√µ theo g·ª£i √Ω tr√™n (kh√¥ng ƒëo√°n b·ª´a).")
            sources.append("‚ùì Clarification")

        elif intent == "update_data":
            op_type = router_result.get("data_operation_type") or ""
            op_target = router_result.get("data_operation_target") or ""
            if op_target in ("bible", "relation", "timeline", "chunking"):
                ch_range = router_result.get("chapter_range")
                ch_desc = f"ch∆∞∆°ng {ch_range[0]}" if (ch_range and len(ch_range) >= 1) else "ch∆∞∆°ng"
                context_parts.append(
                    f"[C·∫¨P NH·∫¨T D·ªÆ LI·ªÜU - C·∫¶N X√ÅC NH·∫¨N]\n"
                    f"User y√™u c·∫ßu: {op_type} {op_target} cho {ch_desc}. "
                    "Thao t√°c n√†y ch·ªâ th·ª±c hi·ªán sau khi user x√°c nh·∫≠n. Tr·∫£ l·ªùi ng·∫Øn g·ªçn: n√™u r√µ thao t√°c v√† ƒë·ªëi t∆∞·ª£ng c√πng ch∆∞∆°ng, nh·∫Øc user x√°c nh·∫≠n (s·∫Ω ch·∫°y ng·∫ßm v√† xem nh∆∞ ƒë√£ ch·∫•p nh·∫≠n)."
                )
                sources.append("üì¶ Update data (thao t√°c theo ch∆∞∆°ng, pending confirm)")
            else:
                update_summary = router_result.get("update_summary", "") or "Ghi nh·ªõ / c·∫≠p nh·∫≠t d·ªØ li·ªáu theo y√™u c·∫ßu user."
                context_parts.append(f"[C·∫¨P NH·∫¨T D·ªÆ LI·ªÜU - C·∫¶N X√ÅC NH·∫¨N]\n{update_summary}\n\nThao t√°c n√†y ch·ªâ th·ª±c hi·ªán sau khi user x√°c nh·∫≠n. Tr·∫£ l·ªùi t√≥m t·∫Øt n·ªôi dung s·∫Ω ƒë∆∞·ª£c ghi v√† nh·∫Øc user x√°c nh·∫≠n tr∆∞·ªõc khi th·ª±c hi·ªán.")
                sources.append("‚úèÔ∏è Update data (ghi nh·ªõ quy t·∫Øc, pending confirm)")

        elif intent == "query_Sql":
            arc_id = (session_state or {}).get("current_arc_id")
            block, source_label = build_query_sql_context(router_result, project_id, arc_id=arc_id)
            if block:
                context_parts.append(block)
                total_tokens += AIService.estimate_tokens(block)
                sources.append(source_label)
            else:
                intent = "search_context"
                context_needs = ["bible", "relation"]

        elif intent == "check_chapter_logic":
            try:
                from core.chapter_logic_check import run_chapter_logic_check
                ch_range = router_result.get("chapter_range")
                ch_num = int(ch_range[0]) if (ch_range and len(ch_range) >= 1) else None
                if ch_num is None:
                    context_parts.append("[SO√ÅT LOGIC CH∆Ø∆†NG] Ch∆∞a x√°c ƒë·ªãnh ƒë∆∞·ª£c ch∆∞∆°ng. H√£y n√™u r√µ s·ªë ch∆∞∆°ng (vd: ch∆∞∆°ng 3).")
                    sources.append("üîç Logic check")
                else:
                    services = init_services()
                    supabase = services.get("supabase") if services else None
                    if not supabase:
                        context_parts.append("[SO√ÅT LOGIC CH∆Ø∆†NG] Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c d·ªãch v·ª•.")
                        sources.append("üîç Logic check")
                    else:
                        r = supabase.table("chapters").select("id, chapter_number, title, content, arc_id").eq(
                            "story_id", project_id
                        ).eq("chapter_number", ch_num).limit(1).execute()
                        row = (r.data or [None])[0] if r.data else None
                        if not row:
                            context_parts.append("[SO√ÅT LOGIC CH∆Ø∆†NG] Kh√¥ng t√¨m th·∫•y ch∆∞∆°ng %s." % ch_num)
                            sources.append("üîç Logic check")
                        else:
                            issues, resolved_count, _check_id, err = run_chapter_logic_check(
                                project_id,
                                row["id"],
                                row.get("chapter_number") or ch_num,
                                row.get("title") or ("Ch∆∞∆°ng %s" % ch_num),
                                row.get("content") or "",
                                arc_id=row.get("arc_id"),
                            )
                            if err:
                                context_parts.append("[SO√ÅT LOGIC CH∆Ø∆†NG] L·ªói: %s" % err)
                            else:
                                lines = ["[K·∫æT QU·∫¢ SO√ÅT LOGIC - Ch∆∞∆°ng %s]" % ch_num]
                                if not issues:
                                    lines.append("Kh√¥ng ph√°t hi·ªán l·ªói logic (timeline, bible, relation, chat crystallize, rule).")
                                else:
                                    for i, it in enumerate(issues, 1):
                                        lines.append("%s. [%s] %s" % (i, it.get("dimension", ""), it.get("message", "")))
                                if resolved_count:
                                    lines.append("(ƒê√£ ƒë√°nh d·∫•u kh·∫Øc ph·ª•c %s l·ªói c≈© t·ª´ l·∫ßn so√°t tr∆∞·ªõc.)" % resolved_count)
                                lines.append("Xem chi ti·∫øt t·∫°i **Data Health**.")
                                context_parts.append("\n".join(lines))
                            sources.append("üîç Logic check (Data Health)")
            except Exception as ex:
                context_parts.append("[SO√ÅT LOGIC CH∆Ø∆†NG] L·ªói: %s" % str(ex))
                sources.append("üîç Logic check")

        if intent == "search_context":
            range_bounds_bible = ContextManager._resolve_chapter_range(
                project_id, chapter_range_mode, chapter_range_count, chapter_range
            )
            def _over_budget() -> bool:
                if max_context_tokens is None:
                    return False
                return total_tokens >= max_context_tokens * 0.92

            for need in context_priority:
                if _over_budget():
                    break
                if need == "chapter":
                    full_text, source_names = "", []
                    if range_bounds_bible is not None:
                        cap = (min(ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT, (max_context_tokens - total_tokens) // max(1, len(context_priority))) if max_context_tokens else ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT)
                        full_text, source_names = ContextManager.load_chapters_by_range(
                            project_id, range_bounds_bible[0], range_bounds_bible[1],
                            token_limit=cap,
                        )
                    if not full_text and target_files:
                        full_text, source_names = ContextManager.load_full_content(
                            target_files, project_id,
                            token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
                        )
                    if full_text:
                        context_parts.append(f"\n--- TARGET CONTENT ---\n{full_text}")
                        sources.extend(source_names)
                        total_tokens += AIService.estimate_tokens(full_text)

            need_bible_or_relation = ("bible" in context_needs or "relation" in context_needs) and not _over_budget()
            if need_bible_or_relation:
                raw_inferred = router_result.get("inferred_prefixes") or []
                valid_keys = Config.get_valid_prefix_keys()
                inferred_prefixes = [
                    p for p in raw_inferred
                    if p and str(p).strip().upper().replace(" ", "_") in valid_keys
                ] if valid_keys else raw_inferred
                bible_context = ""
                for entity in target_bible_entities:
                    raw_list = HybridSearch.smart_search_hybrid_raw(
                        entity, project_id, top_k=7, inferred_prefixes=inferred_prefixes
                    )
                    if range_bounds_bible:
                        raw_list = _filter_bible_by_chapter_range(raw_list, range_bounds_bible, max_items=10)
                    if raw_list:
                        for item in raw_list:
                            try:
                                eid = item.get("id")
                                if eid is not None:
                                    HybridSearch.update_lookup_stats(eid)
                            except Exception:
                                pass
                        main_id = raw_list[0].get("id") if raw_list else None
                        rel_block = ""
                        if main_id:
                            rel_text = ContextManager.get_entity_relations(main_id, project_id)
                            if rel_text:
                                rel_block = f"> [RELATION]:\n{rel_text}\n\n"
                        part = format_bible_context_by_sections(raw_list)
                        bible_context += f"\n--- {entity.upper()} ---\n{rel_block}{part}\n"

                if not bible_context and router_result.get("rewritten_query"):
                    raw_list = HybridSearch.smart_search_hybrid_raw(
                        router_result["rewritten_query"],
                        project_id,
                        top_k=10,
                        inferred_prefixes=inferred_prefixes,
                    )
                    if range_bounds_bible:
                        raw_list = _filter_bible_by_chapter_range(raw_list, range_bounds_bible, max_items=12)
                    if raw_list:
                        for item in raw_list:
                            try:
                                eid = item.get("id")
                                if eid is not None:
                                    HybridSearch.update_lookup_stats(eid)
                            except Exception:
                                pass
                        main_id = raw_list[0].get("id") if raw_list else None
                        rel_block = ""
                        if main_id:
                            rel_text = ContextManager.get_entity_relations(main_id, project_id)
                            if rel_text:
                                rel_block = f"> [RELATION]:\n{rel_text}\n\n"
                        part = format_bible_context_by_sections(raw_list)
                        bible_context = f"\n--- KNOWLEDGE BASE ---\n{rel_block}{part}\n"

                if bible_context:
                    context_parts.append(bible_context)
                    total_tokens += AIService.estimate_tokens(bible_context)
                    sources.append("üìö Bible Search")

                try:
                    services = init_services()
                    supabase = services['supabase']
                    related_chapter_nums = set()

                    if target_bible_entities:
                        for entity in target_bible_entities:
                            res = supabase.table("story_bible") \
                                .select("source_chapter") \
                                .eq("story_id", project_id) \
                                .ilike("entity_name", f"%{entity}%") \
                                .execute()

                            if res.data:
                                for row in res.data:
                                    if row.get('source_chapter') and row['source_chapter'] > 0:
                                        related_chapter_nums.add(row['source_chapter'])

                    if related_chapter_nums:
                        chap_res = supabase.table("chapters") \
                            .select("title") \
                            .eq("story_id", project_id) \
                            .in_("chapter_number", list(related_chapter_nums)) \
                            .execute()

                        if chap_res.data:
                            auto_files = [c['title'] for c in chap_res.data if c.get('title')]

                            if auto_files:
                                extra_text, extra_sources = ContextManager.load_full_content(auto_files, project_id)

                                if extra_text:
                                    context_parts.append(f"\n--- üïµÔ∏è AUTO-DETECTED CONTEXT (REVERSE LOOKUP) ---\n{extra_text}")
                                    sources.extend([f"{s} (Auto)" for s in extra_sources])
                                    total_tokens += AIService.estimate_tokens(extra_text)

                except Exception as e:
                    print(f"Reverse lookup error: {e}")
                    pass

            if "timeline" in context_needs and not _over_budget():
                events = get_timeline_events(
                    project_id,
                    limit=20,
                    chapter_range=range_bounds_bible,
                    arc_id=current_arc_id,
                )
                if events:
                    lines = ["[TIMELINE EVENTS - Th·ª© t·ª± s·ª± ki·ªán / m·ªëc th·ªùi gian]"]
                    for e in events:
                        order = e.get("event_order", 0)
                        title = e.get("title", "")
                        desc = (e.get("description") or "")[:400]
                        raw_date = e.get("raw_date", "")
                        etype = e.get("event_type", "event")
                        lines.append(f"- #{order} [{etype}] {title}" + (f" (Th·ªùi ƒëi·ªÉm: {raw_date})" if raw_date else "") + f"\n  {desc}")
                    block = "\n".join(lines)
                    context_parts.append(block)
                    total_tokens += AIService.estimate_tokens(block)
                    sources.append("üìÖ Timeline Events")
                else:
                    context_parts.append("[TIMELINE] Ch∆∞a c√≥ d·ªØ li·ªáu timeline_events. Tr·∫£ l·ªùi d·ª±a tr√™n Bible/ch∆∞∆°ng n·∫øu c√≥.")
                    sources.append("üìÖ Timeline (empty)")

            if "chunk" in context_needs and not _over_budget():
                query_for_chunk = (router_result.get("rewritten_query") or "").strip() or "n·ªôi dung"
                chunk_rows = search_chunks_vector(query_for_chunk, project_id, arc_id=current_arc_id, top_k=10)
                if not chunk_rows and current_arc_id:
                    chunk_rows = search_chunks_vector(query_for_chunk, project_id, arc_id=None, top_k=10)
                if chunk_rows and ReverseLookupAssembler:
                    chunk_ids = [str(c.get("id")) for c in chunk_rows if c.get("id")]
                    if chunk_ids:
                        chunk_ctx, chunk_sources, chunk_tokens = ContextManager.build_context_with_chunk_reverse_lookup(
                            project_id, chunk_ids, current_arc_id, token_limit=5000
                        )
                        if chunk_ctx:
                            context_parts.append(chunk_ctx)
                            total_tokens += chunk_tokens
                            sources.extend(chunk_sources)
                            sources.append("üì¶ Chunks")
                # Fallback: c√≥ s·ªë ch∆∞∆°ng trong query m√† ch∆∞a load chapter t·ª´ context_needs
                chapter_range_from_query = parse_chapter_range_from_query(query_for_chunk or router_result.get("rewritten_query") or "")
                if chapter_range_from_query and "chapter" not in context_needs:
                    full_text, source_names = ContextManager.load_chapters_by_range(
                        project_id, chapter_range_from_query[0], chapter_range_from_query[1],
                        token_limit=8000,
                    )
                    if full_text:
                        context_parts.append(f"\n--- üìÑ N·ªòI DUNG CH∆Ø∆†NG (fallback) ---\n{full_text}")
                        total_tokens += AIService.estimate_tokens(full_text)
                        sources.extend(source_names)
                        sources.append("üìÑ Chapter fallback")

        context_str = "\n".join(context_parts)
        if max_context_tokens is not None and total_tokens > max_context_tokens:
            context_str, total_tokens = cap_context_to_tokens(context_str, max_context_tokens)
        return context_str, sources, total_tokens


