import threading
from datetime import datetime

import streamlit as st

from config import Config, init_services, CostManager
from ai_engine import (
    AIService,
    ContextManager,
    SmartAIRouter,
    RuleMiningSystem,
    HybridSearch,
    check_semantic_intent,
    get_v7_reminder_message,
)
from ai.evaluate import is_answer_sufficient
from ai.context_helpers import get_related_chapter_nums
from ai_verifier import run_verification_loop
from core.executor_v7 import execute_plan
from core.command_parser import is_command_message, parse_command, get_fallback_clarification
from persona import PersonaSystem
from utils.auth_manager import check_permission, submit_pending_change
from utils.python_executor import PythonExecutor


def _get_crystallize_count(project_id, user_id):
    """L·∫•y s·ªë tin nh·∫Øn t·ª´ l·∫ßn crystallize g·∫ßn nh·∫•t (schema v7.1). Tr·∫£ v·ªÅ 0 n·∫øu ch∆∞a c√≥ b·∫£ng."""
    try:
        services = init_services()
        if not services:
            return 0
        r = services["supabase"].table("chat_crystallize_state").select("messages_since_crystallize").eq(
            "story_id", project_id
        ).eq("user_id", str(user_id) or "").limit(1).execute()
        if r.data and len(r.data) > 0:
            return int(r.data[0].get("messages_since_crystallize", 0) or 0)
    except Exception:
        pass
    return 0


def _increment_crystallize_count(project_id, user_id):
    """TƒÉng messages_since_crystallize l√™n 1 (sau khi l∆∞u tin nh·∫Øn V Work)."""
    try:
        services = init_services()
        if not services:
            return
        sb = services["supabase"]
        now = datetime.utcnow().isoformat()
        r = sb.table("chat_crystallize_state").select("messages_since_crystallize").eq(
            "story_id", project_id
        ).eq("user_id", str(user_id) or "").limit(1).execute()
        if r.data and len(r.data) > 0:
            cur = int(r.data[0].get("messages_since_crystallize", 0) or 0)
            sb.table("chat_crystallize_state").update({
                "messages_since_crystallize": cur + 1,
                "updated_at": now,
            }).eq("story_id", project_id).eq("user_id", str(user_id) or "").execute()
        else:
            sb.table("chat_crystallize_state").upsert({
                "story_id": project_id,
                "user_id": str(user_id) or "",
                "messages_since_crystallize": 1,
                "updated_at": now,
            }, on_conflict="story_id,user_id").execute()
    except Exception:
        pass


def _reset_crystallize_count(project_id, user_id):
    """Reset v·ªÅ 0 sau khi crystallize (tr√°nh tr√πng)."""
    try:
        services = init_services()
        if not services:
            return
        now = datetime.utcnow().isoformat()
        services["supabase"].table("chat_crystallize_state").upsert({
            "story_id": project_id,
            "user_id": str(user_id) or "",
            "messages_since_crystallize": 0,
            "updated_at": now,
        }, on_conflict="story_id,user_id").execute()
    except Exception:
        pass


def _after_save_history_v_work(project_id, user_id, persona_role):
    """Sau khi l∆∞u tin nh·∫Øn V Work: tƒÉng counter, n·∫øu >= 30 th√¨ ch·∫°y crystallize (s·∫Ω reset v·ªÅ 0)."""
    if not project_id or not user_id:
        return
    _increment_crystallize_count(project_id, user_id)
    if _get_crystallize_count(project_id, user_id) >= 30:
        threading.Thread(
            target=_auto_crystallize_background,
            args=(project_id, user_id, persona_role),
            daemon=True,
        ).start()


def _start_data_operation_background(
    project_id,
    user_id,
    user_request,
    active_persona,
    now_timestamp,
    steps=None,
    single_op=None,
    insert_user_message=True,
    rerun_after=True,
):
    """
    Ch·∫°y thao t√°c d·ªØ li·ªáu ng·∫ßm (kh√¥ng x√°c nh·∫≠n): l∆∞u user + tin 'ƒêang ch·∫°y ng·∫ßm', start thread,
    toast, (optionally) rerun. Khi xong job s·∫Ω t·ª± ghi tin ho√†n th√†nh v√†o chat (data_operation_jobs).
    insert_user_message=False: ch·ªâ insert tin 'ƒêang ch·∫°y ng·∫ßm'. rerun_after=False: kh√¥ng rerun (e.g. sau execute_plan ƒë·ªÉ v·∫´n hi·ªÉn th·ªã response V7).
    """
    steps = steps if isinstance(steps, list) else []
    if steps:
        desc = f"{len(steps)} thao t√°c (extract/update/delete)."
    elif single_op:
        op = single_op.get("operation_type", "extract")
        t = single_op.get("target", "bible")
        ch = single_op.get("chapter_number", "")
        desc = f"{op} {t} ch∆∞∆°ng {ch}."
    else:
        return
    running_msg = f"‚è≥ Running in background: **{user_request[:100]}**. {desc} Check **Background Jobs** tab for status."
    try:
        services = init_services()
        if not services:
            st.toast("Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c d·ªãch v·ª•.")
            return
        supabase = services["supabase"]
        if st.session_state.get("enable_history", True):
            if insert_user_message:
                supabase.table("chat_history").insert([
                    {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "user", "content": user_request, "created_at": now_timestamp, "metadata": {"data_operation_background": True}},
                    {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "model", "content": running_msg, "created_at": now_timestamp, "metadata": {"data_operation_background": True}},
                ]).execute()
            else:
                supabase.table("chat_history").insert({
                    "story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "model", "content": running_msg, "created_at": now_timestamp, "metadata": {"data_operation_background": True},
                }).execute()
            _after_save_history_v_work(project_id, user_id, active_persona.get("role", ""))
        if steps:
            from core.background_jobs import create_job, run_job_worker
            label = (user_request[:200] if user_request else "Data operation batch")
            job_id = create_job(
                story_id=project_id,
                user_id=user_id,
                job_type="data_operation_batch",
                label=label,
                payload={"steps": steps, "user_request": user_request or label},
                post_to_chat=False,
            )
            if job_id:
                threading.Thread(target=run_job_worker, args=(job_id,), daemon=True).start()
        else:
            from core.data_operation_jobs import run_data_operation
            threading.Thread(
                target=run_data_operation,
                kwargs={
                    "project_id": project_id,
                    "user_id": user_id,
                    "operation_type": single_op.get("operation_type", "extract"),
                    "target": single_op.get("target", "bible"),
                    "chapter_number": single_op.get("chapter_number"),
                    "user_request": user_request,
                    "post_completion_message": False,
                },
                daemon=True,
            ).start()
        st.toast("Started in background. Check Background Jobs tab for status.")
        if rerun_after:
            st.rerun()
    except Exception as e:
        st.error(f"L·ªói khi b·∫Øt ƒë·∫ßu thao t√°c: {e}")


# --- V Home: l∆∞u/load theo topic (kh√¥ng d√πng chat_history) ---
V_HOME_CONTEXT_MESSAGES = 10


def _v_home_get_current_topic_start(user_id):
    """L·∫•y topic_start_at hi·ªán t·∫°i c·ªßa user. N·∫øu ch∆∞a c√≥ th√¨ t·∫°o m·ªõi (now). Tr·∫£ v·ªÅ chu·ªói ISO."""
    if not user_id:
        return datetime.utcnow().isoformat()
    try:
        services = init_services()
        if not services:
            return datetime.utcnow().isoformat()
        r = services["supabase"].table("v_home_current_topic").select("topic_start_at").eq(
            "user_id", str(user_id)
        ).limit(1).execute()
        if r.data and len(r.data) > 0:
            raw = r.data[0].get("topic_start_at")
            if raw is not None:
                return raw if isinstance(raw, str) else getattr(raw, "isoformat", lambda: str(raw))()
        now = datetime.utcnow().isoformat()
        services["supabase"].table("v_home_current_topic").upsert(
            {"user_id": str(user_id), "topic_start_at": now},
            on_conflict="user_id",
        ).execute()
        return now
    except Exception:
        return datetime.utcnow().isoformat()


def _v_home_load_messages(user_id):
    """L·∫•y tin nh·∫Øn thu·ªôc topic hi·ªán t·∫°i (ƒë·ªÉ hi·ªÉn th·ªã v√† l√†m context)."""
    if not user_id:
        return []
    try:
        services = init_services()
        if not services:
            return []
        topic_start = _v_home_get_current_topic_start(user_id)
        r = (
            services["supabase"]
            .table("v_home_messages")
            .select("id, role, content, created_at, topic_start_at")
            .eq("user_id", str(user_id))
            .order("created_at", desc=True)
            .limit(200)
            .execute()
        )
        out = []
        for m in (r.data or []):
            ts = m.get("topic_start_at")
            ts_str = ts if isinstance(ts, str) else (getattr(ts, "isoformat", lambda: str(ts))() if ts else "")
            if ts_str == topic_start:
                out.append(m)
        out.reverse()
        return out
    except Exception:
        return []


def _v_home_reset_topic(user_id):
    """Reset topic: ƒë·∫∑t topic_start_at = now. Tin nh·∫Øn sau ch·ªâ thu·ªôc topic m·ªõi."""
    if not user_id:
        return
    try:
        services = init_services()
        if not services:
            return
        now = datetime.utcnow().isoformat()
        services["supabase"].table("v_home_current_topic").upsert(
            {"user_id": str(user_id), "topic_start_at": now},
            on_conflict="user_id",
        ).execute()
    except Exception:
        pass


def _v_home_save_message(user_id, role, content, topic_start_at):
    """L∆∞u 1 tin nh·∫Øn V Home (kh√¥ng ghi chat_history)."""
    if not user_id:
        return
    try:
        services = init_services()
        if not services:
            return
        services["supabase"].table("v_home_messages").insert({
            "user_id": str(user_id),
            "role": role,
            "content": content,
            "created_at": datetime.utcnow().isoformat(),
            "topic_start_at": topic_start_at,
        }).execute()
    except Exception:
        pass


def _auto_crystallize_background(project_id, user_id, persona_role):
    """Ch·∫°y ng·∫ßm: crystallize 25 tin (30 - 5) v√† l∆∞u v√†o Bible [CHAT] (ng√†y-stt). Reset counter v7.1 v·ªÅ 0."""
    try:
        services = init_services()
        if not services:
            return
        supabase = services["supabase"]
        q = supabase.table("chat_history").select("id, role, content, created_at").eq("story_id", project_id)
        if user_id:
            q = q.eq("user_id", str(user_id))
        r = q.order("created_at", desc=True).limit(35).execute()
        data = list(r.data)[::-1] if r.data else []
        if len(data) < 25:
            return
        to_crystallize = data[:-5]
        chat_text = "\n".join([f"{m['role']}: {m['content']}" for m in to_crystallize])
        summary = RuleMiningSystem.crystallize_session(to_crystallize, persona_role)
        if not summary or summary == "NO_INFO":
            return
        vec = AIService.get_embedding(summary)
        if not vec:
            return
        today = datetime.utcnow().strftime("%Y-%m-%d")
        try:
            log_r = supabase.table("chat_crystallize_log").select("serial_in_day").eq(
                "story_id", project_id
            ).eq("user_id", str(user_id) or "").eq("crystallize_date", today).execute()
            serial = len(log_r.data) + 1 if log_r.data else 1
        except Exception:
            serial = 1
        entity_name = f"[CHAT] {today} chat-{serial}"
        payload = {
            "story_id": project_id,
            "entity_name": entity_name,
            "description": summary,
            "embedding": vec,
            "source_chapter": 0,
        }
        ins = supabase.table("story_bible").insert(payload).execute()
        bible_id = ins.data[0].get("id") if ins.data else None
        try:
            supabase.table("chat_crystallize_log").insert({
                "story_id": project_id,
                "user_id": str(user_id) if user_id else None,
                "crystallize_date": today,
                "serial_in_day": serial,
                "message_count": len(to_crystallize),
                "bible_entry_id": bible_id,
            }).execute()
        except Exception:
            pass
        _reset_crystallize_count(project_id, user_id)
        try:
            from ai_engine import suggest_relations
            suggestions = suggest_relations(summary, project_id)
            for s in (suggestions or []):
                if s.get("kind") == "relation":
                    try:
                        supabase.table("entity_relations").insert({
                            "source_entity_id": s["source_entity_id"],
                            "target_entity_id": s["target_entity_id"],
                            "relation_type": s.get("relation_type", "li√™n quan"),
                            "description": s.get("description", ""),
                            "story_id": project_id,
                        }).execute()
                    except Exception:
                        pass
        except Exception:
            pass
    except Exception as e:
        print(f"auto_crystallize_background error: {e}")


def render_chat_tab(project_id, persona, chat_mode=None):
    """Tab Chat. chat_mode: 'v_work' (d·ª± √°n, persona, router, crystallize) ho·∫∑c 'v_home' (chat t·ª± do, kh√¥ng context d·ª± √°n)."""
    if chat_mode is None:
        chat_mode = "v_work"
    is_v_home = chat_mode == "v_home"

    st.header("üè† V Home" if is_v_home else "üîß V Work")

    col_chat, col_memory = st.columns([3, 1])

    user = st.session_state.get("user")
    user_id = getattr(user, "id", None) if user else None
    user_email = getattr(user, "email", None) if user else None
    can_write = bool(
        project_id
        and user_id
        and check_permission(str(user_id), user_email or "", project_id, "write")
    )
    can_request = bool(
        project_id
        and user_id
        and check_permission(str(user_id), user_email or "", project_id, "request_write")
    )

    with col_memory:
        st.write("### üß† Memory & Settings")
        if is_v_home:
            active_persona = {"icon": "üè†", "role": "Assistant", "core_instruction": "B·∫°n l√† tr·ª£ l√Ω th√¢n thi·ªán. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, h·ªØu √≠ch. Ng√¥n ng·ªØ: ∆∞u ti√™n Ti·∫øng Vi·ªát.", "system_prompt": "", "max_tokens": 4000}
            st.session_state['enable_history'] = False
            st.caption("Chat t·ª± do ‚Äî kh√¥ng l∆∞u v√†o DB d·ª± √°n. Context = 10 tin cu·ªëi c·ªßa topic.")
            if st.button("üîÑ Reset topic", use_container_width=True, key=f"chat_btn_reset_topic_{chat_mode}", help="B·∫Øt ƒë·∫ßu topic m·ªõi: t·ª´ gi·ªù ch·ªâ ƒë∆∞a tin nh·∫Øn sau th·ªùi ƒëi·ªÉm n√†y v√†o context."):
                _v_home_reset_topic(user_id)
                st.toast("ƒê√£ b·∫Øt ƒë·∫ßu topic m·ªõi.")
                st.rerun()
        else:
            available = PersonaSystem.get_available_personas()
            default_key = st.session_state.get("persona", "Writer")
            idx = available.index(default_key) if default_key in available else 0
            selected_persona_key = st.selectbox(
                "Persona tr·∫£ l·ªùi",
                available,
                index=idx,
                key=f"chat_persona_key_{chat_mode}",
                help="Ch·ªçn persona ƒë·ªÉ AI tr·∫£ l·ªùi theo phong c√°ch n√†y."
            )
            active_persona = PersonaSystem.get_persona(selected_persona_key)
            st.session_state['enable_history'] = True

            if st.button("üßπ Clear Screen", use_container_width=True, key=f"chat_btn_clear_{chat_mode}"):
                st.session_state['chat_cutoff'] = datetime.utcnow().isoformat()
                st.rerun()

            if st.button("üîÑ Show All", use_container_width=True, key=f"chat_btn_show_all_{chat_mode}"):
                st.session_state['chat_cutoff'] = "1970-01-01"
                st.rerun()

        if not is_v_home:
            st.session_state['strict_mode'] = st.toggle(
                "üö´ Strict Mode",
                value=st.session_state.get('strict_mode', False),
                help="ON: AI only answers based on found data. No fabrication. (Temp = 0)",
                key=f"chat_toggle_strict_{chat_mode}",
            )
            st.session_state['use_v7_planner'] = st.toggle(
                "üìê V7 Planner",
                value=st.session_state.get('use_v7_planner', False),
                help="V s·∫Ω t∆∞ duy ƒë·ªÉ t√¨m c√¢u tr·∫£ l·ªùi t·ªët nh·∫•t.",
                key=f"chat_toggle_v7_{chat_mode}",
            )
            st.session_state['auto_extract_rules_chat'] = st.toggle(
                "üßê T·ª± ƒë·ªông tr√≠ch xu·∫•t lu·∫≠t t·ª´ chat",
                value=st.session_state.get('auto_extract_rules_chat', False),
                help="B·∫≠t: sau m·ªói tin nh·∫Øn, AI s·∫Ω t√¨m lu·∫≠t m·ªõi trong h·ªôi tho·∫°i v√† h·ªèi b·∫°n x√°c nh·∫≠n. M·∫∑c ƒë·ªãnh t·∫Øt.",
                key=f"chat_toggle_auto_rules_{chat_mode}",
            )
            st.divider()
            st.write("### üï∞Ô∏è Context cho Router / Planner")
            st.session_state["history_depth"] = st.slider(
                "S·ªë tin nh·∫Øn c≈© ƒë∆∞a v√†o Router & V7 Planner",
                min_value=0,
                max_value=50,
                value=st.session_state.get("history_depth", 5),
                step=1,
                help="Bao nhi√™u tin g·∫ßn nh·∫•t ƒë∆∞·ª£c ƒë∆∞a v√†o Router v√† V7 Planner ƒë·ªÉ ch·ªçn intent v√† l√™n k·∫ø ho·∫°ch. Tr·∫£ l·ªùi cu·ªëi d·ª±a tr√™n context t·ª´ Bible/ch∆∞∆°ng ƒë√£ thu th·∫≠p, kh√¥ng nh·ªìi th√™m l·ªãch s·ª≠ chat.",
                key=f"chat_history_depth_{chat_mode}",
            )
            crystallize_count = _get_crystallize_count(project_id, user_id) if project_id and user_id else 0
            st.caption(f"üíé Crystallize: **{crystallize_count} / 30** tin (sau 30 ‚Üí t√≥m t·∫Øt & l∆∞u Bible [CHAT], xem t·∫°i **Knowledge > Bible** ho·∫∑c **Memory**).")
        else:
            st.session_state["history_depth"] = st.session_state.get("history_depth", 5)

    def _chat_messages_fragment():
        if is_v_home:
            visible_msgs = _v_home_load_messages(user_id)
            for m in visible_msgs:
                role_icon = active_persona["icon"] if m["role"] == "model" else None
                with st.chat_message(m["role"], avatar=role_icon):
                    st.markdown(m.get("content", ""))
        else:
            visible_msgs = []
            try:
                services = init_services()
                supabase = services["supabase"]
                q = (
                    supabase.table("chat_history")
                    .select("*")
                    .eq("story_id", project_id)
                )
                if user_id:
                    q = q.eq("user_id", str(user_id))
                msgs_data = (
                    q.order("created_at", desc=True)
                    .limit(50)
                    .execute()
                )
                msgs = msgs_data.data[::-1] if msgs_data.data else []
                visible_msgs = [m for m in msgs if m["created_at"] > st.session_state.get("chat_cutoff", "1970-01-01")]
                # Hi·ªÉn th·ªã c√†ng m·ªõi c√†ng ·ªü tr√™n cao (newest first)
                for m in reversed(visible_msgs):
                    role_icon = active_persona["icon"] if m["role"] == "model" else None
                    with st.chat_message(m["role"], avatar=role_icon):
                        st.markdown(m["content"])
                        if m.get("metadata"):
                            with st.expander("üìä Details"):
                                st.json(m["metadata"], expanded=False)
            except Exception as e:
                st.error(f"Error loading history: {e}")
        history_depth = st.session_state.get("history_depth", 5)
        chat_input_key = "chat_input_v_home" if is_v_home else "chat_input_main"
        if prompt := st.chat_input(f"Ask {active_persona['icon']} AI Assistant...", key=chat_input_key):
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.spinner("Thinking..."):
                now_timestamp = datetime.utcnow().isoformat()
                v7_handled = False
                router_out = None
                free_chat_mode = is_v_home or st.session_state.get('free_chat_mode', False)

                # S·ªë tin ƒë∆∞a v√†o Router/Planner theo slider (0 = kh√¥ng d√πng l·ªãch s·ª≠).
                depth = history_depth if not is_v_home else 0
                if depth > 0 and visible_msgs:
                    recent_history_text = "\n".join([
                        f"{m['role']}: {m['content']}"
                        for m in visible_msgs[-depth:]
                    ])
                else:
                    recent_history_text = "" if not is_v_home else "\n".join([
                        f"{m.get('role', 'user')}: {m.get('content', '')}"
                        for m in visible_msgs[-V_HOME_CONTEXT_MESSAGES:]
                    ])

                if free_chat_mode:
                    router_out = {"intent": "chat_casual", "target_files": [], "target_bible_entities": [], "rewritten_query": prompt, "chapter_range": None, "chapter_range_mode": None, "chapter_range_count": 5}
                    debug_notes = ["Intent: chat_casual", "üåê Chat t·ª± do"]
                else:
                    debug_notes = []
                    # Ch·ªâ l·ªánh @: parse tr∆∞·ªõc; fallback ask_user_clarification n·∫øu thi·∫øu/sai (kh√¥ng ƒëo√°n √Ω)
                    if not is_v_home and is_command_message(prompt):
                        parse_result = parse_command(prompt, project_id, str(user_id) if user_id else None)
                        if parse_result.status in ("incomplete", "unknown"):
                            clarification_message = get_fallback_clarification(parse_result)
                            with st.chat_message("assistant", avatar=active_persona['icon']):
                                st.caption("üìå Ch·ªâ l·ªánh (@@) ‚Äî c·∫ßn l√†m r√µ")
                                st.info(clarification_message)
                            if st.session_state.get('enable_history', True):
                                try:
                                    services = init_services()
                                    supabase = services['supabase']
                                    supabase.table("chat_history").insert([
                                        {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "user", "content": prompt, "created_at": now_timestamp, "metadata": {"source": "command_fallback", "intent": "ask_user_clarification"}},
                                        {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "model", "content": f"[C·∫ßn l√†m r√µ] {clarification_message}", "created_at": now_timestamp, "metadata": {"intent": "ask_user_clarification"}},
                                    ]).execute()
                                    _after_save_history_v_work(project_id, user_id, active_persona.get("role", ""))
                                except Exception:
                                    pass
                            v7_handled = True
                        elif parse_result.status == "ok":
                            router_out = parse_result.parsed.router_out
                            debug_notes = ["üìå Ch·ªâ l·ªánh", f"Intent: {parse_result.parsed.intent}"]
                    if router_out is None:
                        semantic_match = None
                        try:
                            svc = init_services()
                            if svc:
                                r = svc["supabase"].table("settings").select("value").eq("key", "semantic_intent_no_use").execute()
                                no_use = r.data and r.data[0] and int(r.data[0].get("value", 0)) == 1
                                if not no_use:
                                    semantic_match = check_semantic_intent(prompt, project_id)
                        except Exception:
                            semantic_match = check_semantic_intent(prompt, project_id)
                    if router_out is None and semantic_match:
                        router_out = {"intent": "chat_casual", "target_files": [], "target_bible_entities": [], "rewritten_query": prompt, "chapter_range": None, "chapter_range_mode": None, "chapter_range_count": 5}
                        if semantic_match.get("related_data"):
                            router_out["_semantic_data"] = semantic_match["related_data"]
                        debug_notes.append(f"üéØ Semantic match {int(semantic_match.get('similarity',0)*100)}%")
                    elif router_out is None and not is_v_home and st.session_state.get('use_v7_planner', False):
                        plan_result = SmartAIRouter.get_plan_v7(prompt, recent_history_text, project_id)
                        plan = plan_result.get("plan") or []
                        first_intent = (plan[0].get("intent", "") if plan else "") or "chat_casual"
                        if first_intent == "ask_user_clarification":
                            clarification_question = (plan[0].get("args") or {}).get("clarification_question", "") or "B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n c√¢u h·ªèi ho·∫∑c ch·ªß ƒë·ªÅ b·∫°n mu·ªën h·ªèi?"
                            with st.chat_message("assistant", avatar=active_persona['icon']):
                                st.caption("üß† V7 Planner ‚Äî C·∫ßn l√†m r√µ")
                                st.info(f"**ƒê·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c, t√¥i c·∫ßn b·∫°n l√†m r√µ:**\n\n{clarification_question}")
                                st.text_input("B·∫°n c√≥ th·ªÉ g√µ l·∫°i ho·∫∑c b·ªï sung t·∫°i ƒë√¢y (g·ª≠i b·∫±ng √¥ chat ph√≠a d∆∞·ªõi):", key="clarification_followup", placeholder="V√≠ d·ª•: T√¥i mu·ªën h·ªèi v·ªÅ nh√¢n v·∫≠t A trong ch∆∞∆°ng 3")
                            if st.session_state.get('enable_history', True):
                                try:
                                    services = init_services()
                                    supabase = services['supabase']
                                    supabase.table("chat_history").insert([
                                        {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "user", "content": prompt, "created_at": now_timestamp, "metadata": {"intent": first_intent, "v7_plan": plan_result}},
                                        {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "model", "content": f"[C·∫ßn l√†m r√µ] {clarification_question}", "created_at": now_timestamp, "metadata": {"intent": first_intent}},
                                    ]).execute()
                                    if not is_v_home:
                                        _after_save_history_v_work(project_id, user_id, active_persona.get("role", ""))
                                except Exception:
                                    pass
                            v7_handled = True
                        elif first_intent == "update_data" and not is_v_home and can_write and (plan or []) and all((s.get("intent") or "") == "update_data" for s in (plan or [])):
                            # Ch·ªâ x·ª≠ l√Ω "ch·ªâ update_data" khi to√†n b·ªô plan l√† update_data; n·∫øu c√≥ b∆∞·ªõc kh√°c th√¨ ch·∫°y execute_plan b√™n d∆∞·ªõi.
                            data_steps = []
                            for s in (plan or []):
                                if (s.get("intent") or "") != "update_data":
                                    continue
                                a = s.get("args") or {}
                                t = (a.get("data_operation_target") or "").strip()
                                if t not in ("bible", "relation", "timeline", "chunking"):
                                    continue
                                op_type = a.get("data_operation_type") or "extract"
                                ch_range = a.get("chapter_range")
                                if ch_range and isinstance(ch_range, (list, tuple)) and len(ch_range) >= 2:
                                    try:
                                        start, end = int(ch_range[0]), int(ch_range[1])
                                        start, end = min(start, end), max(start, end)
                                        if start == end:
                                            data_steps.append({"operation_type": op_type, "target": t, "chapter_number": start})
                                        else:
                                            data_steps.append({"operation_type": op_type, "target": t, "chapter_range": [start, end]})
                                    except (ValueError, TypeError):
                                        if ch_range and len(ch_range) >= 1:
                                            data_steps.append({"operation_type": op_type, "target": t, "chapter_number": int(ch_range[0])})
                                elif ch_range and len(ch_range) >= 1:
                                    data_steps.append({"operation_type": op_type, "target": t, "chapter_number": int(ch_range[0])})
                                else:
                                    continue
                            if data_steps:
                                _start_data_operation_background(
                                    project_id, user_id, prompt, active_persona, now_timestamp, steps=data_steps,
                                )
                                v7_handled = True
                        if not v7_handled:
                            retries_used = 0
                            status_label = "V7 Multi-step"
                            with st.status(f"üìê {status_label}", expanded=True) as status:
                                st.write("üß† Planning...")
                                if plan_result.get("analysis"):
                                    st.caption(plan_result["analysis"][:500] + ("..." if len(plan_result.get("analysis", "")) > 500 else ""))
                                cumulative_context = ""
                                sources = []
                                step_results = []
                                replan_events = []
                                try:
                                    st.write(f"‚öôÔ∏è Executing {len(plan)} step(s)...")
                                    cumulative_context, sources, step_results, replan_events, data_operation_steps = execute_plan(
                                        plan,
                                        project_id,
                                        active_persona,
                                        prompt,
                                        st.session_state.get('strict_mode', False),
                                        st.session_state.get('current_arc_id'),
                                        dict(st.session_state),
                                        free_chat_mode=False,
                                        max_context_tokens=Config.CONTEXT_SIZE_TOKENS.get(st.session_state.get("context_size", "medium")),
                                        run_numerical_executor=True,
                                    )
                                    if data_operation_steps:
                                        _start_data_operation_background(
                                            project_id, user_id, prompt, active_persona, now_timestamp,
                                            steps=data_operation_steps, insert_user_message=False, rerun_after=False,
                                        )
                                    if replan_events:
                                        for ev in replan_events:
                                            st.caption(f"üîÑ Re-plan (sau step {ev.get('step_id')}): {ev.get('reason', '')[:80]}... ‚Üí {ev.get('action', '')}")
                                    st.write("üìù Generating draft...")
                                    system_content = (active_persona.get("system_prompt") or "") + "\n\n--- CONTEXT (C√°c b∆∞·ªõc ƒë√£ th·ª±c thi) ---\n" + cumulative_context
                                    user_content = prompt
                                    draft_resp = AIService.call_openrouter(
                                        messages=[
                                            {"role": "system", "content": system_content},
                                            {"role": "user", "content": user_content},
                                        ],
                                        model=st.session_state.get('selected_model', Config.DEFAULT_MODEL),
                                        temperature=0.0 if st.session_state.get('strict_mode') else 0.7,
                                        max_tokens=4096,
                                        stream=False,
                                    )
                                    draft_response = (draft_resp.choices[0].message.content or "").strip()
                                    st.write("üõ°Ô∏è Verifying...")
                                    verification_required = plan_result.get("verification_required", True)

                                    def _llm_generate(system_content: str, user_content: str) -> str:
                                        r = AIService.call_openrouter(
                                            messages=[
                                                {"role": "system", "content": system_content},
                                                {"role": "user", "content": user_content},
                                            ],
                                            model=st.session_state.get('selected_model', Config.DEFAULT_MODEL),
                                            temperature=0.0,
                                            max_tokens=4096,
                                            stream=False,
                                        )
                                        return (r.choices[0].message.content or "").strip()

                                    plan_for_verifier = [{"intent": r.get("intent", "chat_casual")} for r in step_results]
                                    final_response, retries_used = run_verification_loop(
                                        draft_response,
                                        cumulative_context,
                                        plan_for_verifier,
                                        step_results,
                                        _llm_generate,
                                        verification_required=verification_required,
                                    )
                                    if retries_used > 0:
                                        st.warning("‚ö†Ô∏è Detecting error, auto-correcting...")
                                    status.update(label=f"‚úÖ {status_label} ‚Äî Done", state="complete")
                                except Exception as ex:
                                    status.update(label=f"‚ùå {status_label} ‚Äî Error", state="error")
                                    final_response = f"L·ªói khi ch·∫°y V7: {ex}"
                                    import traceback
                                    st.exception(ex)

                            with st.chat_message("assistant", avatar=active_persona['icon']):
                                # Stream hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi cu·ªëi (typewriter effect)
                                _placeholder = st.empty()
                                import time
                                _chunk = 25
                                for _i in range(0, len(final_response), _chunk):
                                    _placeholder.markdown(final_response[:_i + _chunk] + "‚ñå")
                                    time.sleep(0.02)
                                _placeholder.markdown(final_response)
                                with st.expander("üìä V7 Details"):
                                    st.caption(f"Steps: {len(step_results)} | Verification retries: {retries_used}")
                                    if replan_events:
                                        st.caption("üîÑ Re-plan: " + "; ".join([f"Step {e.get('step_id')} ‚Üí {e.get('action')}" for e in replan_events]))
                                    st.json({
                                        "plan": plan_result.get("plan"),
                                        "verification_required": plan_result.get("verification_required"),
                                        "replan_events": replan_events,
                                    }, expanded=False)

                            if st.session_state.get('enable_history', True):
                                try:
                                    services = init_services()
                                    supabase = services['supabase']
                                    supabase.table("chat_history").insert([
                                        {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "user", "content": prompt, "created_at": now_timestamp, "metadata": {"v7": True, "plan": plan_result.get("plan")}},
                                        {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "model", "content": final_response, "created_at": now_timestamp, "metadata": {"v7": True, "verification_required": plan_result.get("verification_required")}},
                                    ]).execute()
                                    if not is_v_home:
                                        _after_save_history_v_work(project_id, user_id, active_persona.get("role", ""))
                                except Exception:
                                    pass
                            v7_handled = True
                    elif router_out is None:
                        router_out = SmartAIRouter.ai_router_pro_v2(prompt, recent_history_text, project_id)
                    if router_out is not None:
                        debug_notes = [f"Intent: {router_out.get('intent', 'chat_casual')}"] + debug_notes

                if not v7_handled:
                    intent = router_out.get('intent', 'chat_casual')
                    targets = router_out.get('target_files', [])
                    rewritten_query = router_out.get('rewritten_query', prompt)

                    # ask_user_clarification: d·ª´ng l·∫°i, hi·ªán popup h·ªèi user thay v√¨ g·ªçi LLM
                    if intent == "ask_user_clarification":
                        clarification_question = router_out.get("clarification_question", "") or "B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n c√¢u h·ªèi ho·∫∑c ch·ªß ƒë·ªÅ b·∫°n mu·ªën h·ªèi?"
                        with st.chat_message("assistant", avatar=active_persona['icon']):
                            st.caption("üß† Intent: ask_user_clarification ‚Äî C·∫ßn l√†m r√µ")
                            st.info(f"**ƒê·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c, t√¥i c·∫ßn b·∫°n l√†m r√µ:**\n\n{clarification_question}")
                            st.text_input("B·∫°n c√≥ th·ªÉ g√µ l·∫°i ho·∫∑c b·ªï sung t·∫°i ƒë√¢y (g·ª≠i b·∫±ng √¥ chat ph√≠a d∆∞·ªõi):", key="clarification_followup", placeholder="V√≠ d·ª•: T√¥i mu·ªën h·ªèi v·ªÅ nh√¢n v·∫≠t A trong ch∆∞∆°ng 3")
                        if st.session_state.get('enable_history', True):
                            try:
                                services = init_services()
                                supabase = services['supabase']
                                supabase.table("chat_history").insert([
                                    {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "user", "content": prompt, "created_at": now_timestamp, "metadata": {"intent": intent, "router_output": router_out}},
                                    {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "model", "content": f"[C·∫ßn l√†m r√µ] {clarification_question}", "created_at": now_timestamp, "metadata": {"intent": intent}},
                                ]).execute()
                                if not is_v_home:
                                    _after_save_history_v_work(project_id, user_id, active_persona.get("role", ""))
                            except Exception:
                                pass
                    elif intent == "suggest_v7":
                        reason = (router_out.get("reason") or "").strip()
                        with st.chat_message("assistant", avatar=active_persona['icon']):
                            st.caption("üß† V6 ‚Äî G·ª£i √Ω d√πng V7 Planner")
                            st.warning(get_v7_reminder_message())
                            if reason:
                                st.caption(f"*L√Ω do: {reason}*")
                        if st.session_state.get('enable_history', True):
                            try:
                                services = init_services()
                                supabase = services['supabase']
                                model_msg = "C√¢u h·ªèi c·∫ßn nhi·ªÅu b∆∞·ªõc x·ª≠ l√Ω (nhi·ªÅu intent ho·∫∑c nhi·ªÅu thao t√°c). Vui l√≤ng b·∫≠t V7 Planner ƒë·ªÉ th·ª±c hi·ªán ƒë·ªß trong m·ªôt l·∫ßn."
                                supabase.table("chat_history").insert([
                                    {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "user", "content": prompt, "created_at": now_timestamp, "metadata": {"intent": intent, "router_output": router_out}},
                                    {"story_id": project_id, "user_id": str(user_id) if user_id else None, "role": "model", "content": model_msg, "created_at": now_timestamp, "metadata": {"intent": intent}},
                                ]).execute()
                                if not is_v_home:
                                    _after_save_history_v_work(project_id, user_id, active_persona.get("role", ""))
                            except Exception:
                                pass
                    elif intent == "update_data" and not is_v_home and can_write:
                        ch_range = router_out.get("chapter_range")
                        # @data_analyze: 4 b∆∞·ªõc (bible, relation, timeline, chunking)
                        if router_out.get("_data_analyze_full") and ch_range and len(ch_range) >= 2:
                            start, end = int(ch_range[0]), int(ch_range[1])
                            start, end = min(start, end), max(start, end)
                            data_steps = [
                                {"operation_type": "extract", "target": "bible", "chapter_range": [start, end]},
                                {"operation_type": "extract", "target": "relation", "chapter_range": [start, end]},
                                {"operation_type": "extract", "target": "timeline", "chapter_range": [start, end]},
                                {"operation_type": "extract", "target": "chunking", "chapter_range": [start, end]},
                            ]
                            _start_data_operation_background(
                                project_id, user_id, prompt, active_persona, now_timestamp, steps=data_steps,
                            )
                        elif (router_out.get("data_operation_target") or "") in ("bible", "relation", "timeline", "chunking"):
                            op_type = router_out.get("data_operation_type") or "extract"
                            op_target = router_out.get("data_operation_target") or "bible"
                            ch_num = int(ch_range[0]) if (ch_range and len(ch_range) >= 1) else None
                            if ch_range and len(ch_range) >= 2:
                                start, end = int(ch_range[0]), int(ch_range[1])
                                ch_num = min(start, end)
                            op_label = {"extract": "Tr√≠ch xu·∫•t", "update": "C·∫≠p nh·∫≠t", "delete": "X√≥a"}.get(op_type, op_type)
                            target_label = {"bible": "Bible", "relation": "Relation", "timeline": "Timeline", "chunking": "Chunking"}.get(op_target, op_target)
                            if ch_num is None:
                                with st.chat_message("assistant", avatar=active_persona['icon']):
                                    st.caption("üß† Intent: update_data (thao t√°c theo ch∆∞∆°ng)")
                                    st.warning("Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ch∆∞∆°ng. Vui l√≤ng n√≥i r√µ s·ªë ch∆∞∆°ng ho·∫∑c t√™n ch∆∞∆°ng (v√≠ d·ª•: ch∆∞∆°ng 1, ch∆∞∆°ng Kh·ªüi ƒë·∫ßu).")
                            else:
                                _start_data_operation_background(
                                    project_id, user_id, prompt, active_persona, now_timestamp,
                                    single_op={"operation_type": op_type, "target": op_target, "chapter_number": ch_num},
                                )
                    else:
                        max_context_tokens = Config.CONTEXT_SIZE_TOKENS.get(st.session_state.get("context_size", "medium"))
                        exec_result = None
                        if intent == "numerical_calculation" and not free_chat_mode:
                            context_text, sources, context_tokens = ContextManager.build_context(
                                router_out, project_id, active_persona,
                                st.session_state.get('strict_mode', False),
                                current_arc_id=st.session_state.get('current_arc_id'),
                                session_state=dict(st.session_state),
                                max_context_tokens=max_context_tokens,
                            )
                            code_prompt = f"""User h·ªèi: "{prompt}"
Context c√≥ s·∫µn:
{context_text[:6000]}

Nhi·ªám v·ª•: T·∫°o code Python (pandas/numpy) ƒë·ªÉ tr·∫£ l·ªùi. G√°n k·∫øt qu·∫£ cu·ªëi v√†o bi·∫øn result.
Ch·ªâ tr·∫£ v·ªÅ code trong block ```python ... ```, kh√¥ng gi·∫£i th√≠ch."""
                            try:
                                code_resp = AIService.call_openrouter(
                                    messages=[{"role": "user", "content": code_prompt}],
                                    model=st.session_state.get('selected_model', Config.DEFAULT_MODEL),
                                    temperature=0.1,
                                    max_tokens=2000,
                                )
                                raw = (code_resp.choices[0].message.content or "").strip()
                                import re
                                m = re.search(r'```(?:python)?\s*(.*?)```', raw, re.DOTALL)
                                code = m.group(1).strip() if m else raw
                                if code:
                                    val, err = PythonExecutor.execute(code, result_variable="result")
                                    if err:
                                        exec_result = f"(Executor l·ªói: {err})"
                                    else:
                                        exec_result = str(val) if val is not None else "null"
                                        debug_notes.append("üßÆ Python Executor OK")
                            except Exception as ex:
                                exec_result = f"(L·ªói: {ex})"
                            if exec_result:
                                context_text += f"\n\n--- K·∫æT QU·∫¢ T√çNH TO√ÅN (Python Executor) ---\n{exec_result}"

                        if is_v_home:
                            context_text = "\n".join([
                                f"{m.get('role', 'user')}: {m.get('content', '')}"
                                for m in visible_msgs[-V_HOME_CONTEXT_MESSAGES:]
                            ])
                            sources = []
                        elif exec_result is None:
                            context_text, sources, context_tokens = ContextManager.build_context(
                                router_out,
                                project_id,
                                active_persona,
                                st.session_state.get('strict_mode', False),
                                current_arc_id=st.session_state.get('current_arc_id'),
                                session_state=dict(st.session_state),
                                free_chat_mode=free_chat_mode,
                                max_context_tokens=max_context_tokens,
                            )
                            if not free_chat_mode and router_out.get("_semantic_data"):
                                context_text = f"[SEMANTIC INTENT - Data]\n{router_out['_semantic_data']}\n\n{context_text}"
                                sources.append("üéØ Semantic Intent")

                        debug_notes.extend(sources)

                        final_prompt = f"CONTEXT:\n{context_text}\n\nUSER QUERY: {prompt}"

                        run_instruction = active_persona['core_instruction']
                        run_temperature = st.session_state.get('temperature', 0.7)

                        if st.session_state.get('strict_mode') and not free_chat_mode:
                            run_temperature = 0.0

                        messages = []
                        system_message = f"""{run_instruction}

            TH√îNG TIN NG·ªÆ C·∫¢NH (CONTEXT):
            {context_text}

            H∆Ø·ªöNG D·∫™N:
            - Tr·∫£ l·ªùi d·ª±a tr√™n Context n·∫øu c√≥.
            - H·ªØu √≠ch, s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.
            - Ch·∫ø ƒë·ªô hi·ªán t·∫°i: {active_persona['role']}
            - Ng√¥n ng·ªØ: ∆Øu ti√™n Ti·∫øng Vi·ªát (tr·ª´ khi User y√™u c·∫ßu kh√°c ho·∫∑c code).
            """

                        messages.append({"role": "system", "content": system_message})

                        # Tr·∫£ l·ªùi ch·ªâ d·ª±a tr√™n context ƒë√£ thu th·∫≠p (Bible, ch∆∞∆°ng, timeline...); kh√¥ng nh·ªìi l·ªãch s·ª≠ chat v√†o LLM.
                        messages.append({"role": "user", "content": prompt})

                        try:
                            model = st.session_state.get('selected_model', Config.DEFAULT_MODEL)

                            response = AIService.call_openrouter(
                                messages=messages,
                                model=model,
                                temperature=run_temperature,
                                max_tokens=active_persona.get('max_tokens', 4000),
                                stream=True
                            )

                            with st.chat_message("assistant", avatar=active_persona['icon']):
                                if debug_notes:
                                    st.caption(f"üß† {', '.join(debug_notes)}")
                                if st.session_state.get('strict_mode'):
                                    st.caption("üîí Strict Mode: ON")

                                full_response_text = ""
                                placeholder = st.empty()

                                for chunk in response:
                                    if chunk.choices[0].delta.content is not None:
                                        content = chunk.choices[0].delta.content
                                        full_response_text += content
                                        placeholder.markdown(full_response_text + "‚ñå")

                                placeholder.markdown(full_response_text)

                            # search_context: th·∫©m ƒë·ªãnh c√¢u tr·∫£ l·ªùi; n·∫øu ch∆∞a ƒë·ªß √Ω th√¨ fallback ƒë·ªçc full content c√°c ch∆∞∆°ng reverse lookup
                            if (
                                not is_v_home
                                and intent == "search_context"
                                and full_response_text
                                and not is_answer_sufficient(
                                    prompt,
                                    full_response_text,
                                    (context_text or "")[:1000],
                                    router_out.get("context_needs"),
                                )
                            ):
                                ch_range = router_out.get("chapter_range")
                                start, end = None, None
                                if ch_range and len(ch_range) >= 2:
                                    start, end = int(ch_range[0]), int(ch_range[1])
                                    start, end = min(start, end), max(start, end)
                                if start is None or end is None:
                                    related_nums = get_related_chapter_nums(
                                        project_id, router_out.get("target_bible_entities") or []
                                    )
                                    if related_nums:
                                        start, end = min(related_nums), max(related_nums)
                                if start is not None and end is not None:
                                    fallback_text, _ = ContextManager.load_chapters_by_range(
                                        project_id, start, end,
                                        token_limit=ContextManager.DEFAULT_CHAPTER_TOKEN_LIMIT,
                                    )
                                    if fallback_text:
                                        extended_context = (context_text or "") + "\n\n--- N·ªòI DUNG CH∆Ø∆†NG (FALLBACK - ƒë·ªçc ƒë·∫ßy ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi ƒë·ªß √Ω) ---\n" + fallback_text[:8000]
                                        retry_messages = [
                                            {"role": "system", "content": run_instruction + "\n\nTH√îNG TIN NG·ªÆ C·∫¢NH (CONTEXT):\n" + extended_context + "\n\nTr·∫£ l·ªùi ƒê·∫¶Y ƒê·ª¶ d·ª±a tr√™n context, ƒë·∫∑c bi·ªát n·ªôi dung ch∆∞∆°ng v·ª´a b·ªï sung."},
                                            {"role": "user", "content": prompt},
                                        ]
                                        try:
                                            retry_resp = AIService.call_openrouter(
                                                messages=retry_messages,
                                                model=model,
                                                temperature=run_temperature,
                                                max_tokens=active_persona.get("max_tokens", 4000),
                                            )
                                            new_answer = (retry_resp.choices[0].message.content or "").strip()
                                            if new_answer:
                                                full_response_text = new_answer
                                                placeholder.markdown(full_response_text)
                                                debug_notes.append("üìÑ Fallback read full content")
                                        except Exception:
                                            pass

                            input_tokens = AIService.estimate_tokens(system_message + prompt)
                            output_tokens = AIService.estimate_tokens(full_response_text)
                            cost = AIService.calculate_cost(input_tokens, output_tokens, model)

                            if 'user' in st.session_state:
                                CostManager.update_budget(st.session_state.user.id, cost)

                            if full_response_text:
                                if is_v_home:
                                    topic_start = _v_home_get_current_topic_start(user_id)
                                    _v_home_save_message(user_id, "user", prompt, topic_start)
                                    _v_home_save_message(user_id, "model", full_response_text, topic_start)
                                elif st.session_state.get('enable_history', True):
                                    services = init_services()
                                    supabase = services['supabase']

                                    supabase.table("chat_history").insert([
                                        {
                                            "story_id": project_id,
                                            "user_id": str(user_id) if user_id else None,
                                            "role": "user",
                                            "content": prompt,
                                            "created_at": now_timestamp,
                                            "metadata": {
                                                "intent": intent,
                                                "router_output": router_out,
                                                "model": model,
                                                "temperature": run_temperature
                                            }
                                        },
                                        {
                                            "story_id": project_id,
                                            "user_id": str(user_id) if user_id else None,
                                            "role": "model",
                                            "content": full_response_text,
                                            "created_at": now_timestamp,
                                            "metadata": {
                                                "model": model,
                                                "cost": f"${cost:.6f}",
                                                "tokens": input_tokens + output_tokens
                                            }
                                        }
                                    ]).execute()

                                # update_data (ghi nh·ªõ quy t·∫Øc): l∆∞u pending x√°c nh·∫≠n tr∆∞·ªõc khi ghi Bible (ch·ªâ V Work; thao t√°c theo ch∆∞∆°ng x·ª≠ l√Ω ·ªü nh√°nh kh√°c)
                                op_t = (router_out.get("data_operation_target") or "").strip()
                                if not is_v_home and intent == "update_data" and can_write and op_t not in ("bible", "relation", "timeline", "chunking"):
                                    st.session_state["pending_update_confirm"] = {
                                        "project_id": project_id,
                                        "prompt": prompt,
                                        "response": full_response_text,
                                        "update_summary": router_out.get("update_summary", ""),
                                        "user_id": user_id,
                                    }

                                # V Work: tƒÉng counter crystallize v√† trigger n·∫øu >= 30 (reset v·ªÅ 0 sau crystallize)
                                if not is_v_home and can_write and user_id:
                                    _after_save_history_v_work(project_id, user_id, active_persona.get("role", ""))

                                # Rule mining (ch·ªâ V Work, ch·ªâ khi b·∫≠t toggle)
                                if not is_v_home and can_write and st.session_state.get('auto_extract_rules_chat', False):
                                    new_rules = RuleMiningSystem.extract_rules_raw(prompt, full_response_text)
                                    if new_rules:
                                        st.session_state['pending_new_rules'] = [{"content": r, "analysis": None} for r in new_rules]
                                    # Offer add to Semantic Intent (n·∫øu b·∫≠t auto-create v√† kh√¥ng ph·∫£i chat phi·∫øm)
                                    try:
                                        r = init_services()["supabase"].table("settings").select("value").eq("key", "semantic_intent_no_auto_create").execute()
                                        no_auto = r.data and r.data[0] and int(r.data[0].get("value", 0)) == 1
                                    except Exception:
                                        no_auto = False
                                    if not no_auto and intent != "chat_casual":
                                        st.session_state["pending_semantic_add"] = {"prompt": prompt, "response": full_response_text, "context": context_text, "intent": intent}

                            elif not st.session_state.get('enable_history', True):
                                st.caption("üëª Anonymous mode: History not saved & Rule mining disabled.")

                        except Exception as e:
                            st.error(f"Generation error: {str(e)}")

    with col_chat:
        _chat_messages_fragment()

    # Offer add to Semantic Intent (ch·ªâ V Work)
    if not is_v_home and "pending_semantic_add" in st.session_state and can_write:
        p = st.session_state["pending_semantic_add"]
        with st.expander("üéØ Th√™m v√†o Semantic Intent?", expanded=True):
            st.caption("C√¢u h·ªèi v·ª´a r·ªìi kh√¥ng ph·∫£i chat phi·∫øm. Th√™m l√†m m·∫´u ƒë·ªÉ l·∫ßn sau kh·ªõp nhanh?")
            st.write("**C√¢u h·ªèi:**", p.get("prompt", "")[:100])
            col_a, col_b = st.columns(2)
            with col_a:
                if st.button("‚úÖ Th√™m v√†o Semantic", key=f"chat_semantic_add_btn_{chat_mode}"):
                    def _add_semantic():
                        try:
                            svc = init_services()
                            if not svc:
                                return
                            sb = svc["supabase"]
                            vec = AIService.get_embedding(p.get("prompt", ""))
                            ctx = p.get("context", "") or ""
                            resp = p.get("response", "") or ""
                            related_data = (ctx.rstrip() + "\n\n--- C√¢u tr·∫£ l·ªùi ---\n" + resp) if ctx else resp
                            payload = {"story_id": project_id, "question_sample": p.get("prompt", ""), "intent": "chat_casual", "related_data": related_data}
                            if vec:
                                payload["embedding"] = vec
                            try:
                                sb.table("semantic_intent").insert(payload).execute()
                            except Exception:
                                payload.pop("embedding", None)
                                sb.table("semantic_intent").insert(payload).execute()
                        except Exception:
                            pass
                    threading.Thread(target=_add_semantic, daemon=True).start()
                    del st.session_state["pending_semantic_add"]
                    st.toast("ƒê√£ th√™m v√†o Semantic Intent (ch·∫°y ng·∫ßm).")
                    st.rerun()
            with col_b:
                if st.button("‚ùå B·ªè qua", key=f"chat_semantic_skip_btn_{chat_mode}"):
                    del st.session_state["pending_semantic_add"]
                    st.rerun()

    # update_data: X√°c nh·∫≠n cu·ªëi c√πng tr∆∞·ªõc khi ghi Bible / c·∫≠p nh·∫≠t (ch·ªâ V Work)
    if not is_v_home and "pending_update_confirm" in st.session_state and can_write:
        pu = st.session_state["pending_update_confirm"]
        if pu.get("project_id") == project_id:
            with st.expander("‚úèÔ∏è X√°c nh·∫≠n th·ª±c hi·ªán c·∫≠p nh·∫≠t?", expanded=True):
                st.caption("B·∫°n ƒë√£ y√™u c·∫ßu ghi nh·ªõ / c·∫≠p nh·∫≠t d·ªØ li·ªáu. Ch·ªâ th·ª±c hi·ªán khi b·∫°n x√°c nh·∫≠n.")
                st.write("**T√≥m t·∫Øt:**", pu.get("update_summary", "") or "(Theo n·ªôi dung AI tr·∫£ l·ªùi)")
                st.write("**N·ªôi dung s·∫Ω ghi:**", (pu.get("response", "") or "")[:500])
                col_ok, col_no = st.columns(2)
                with col_ok:
                    if st.button("‚úÖ X√°c nh·∫≠n th·ª±c hi·ªán", key=f"update_confirm_ok_{chat_mode}"):
                        try:
                            services = init_services()
                            supabase = services["supabase"]
                            content_to_save = (pu.get("response", "") or pu.get("update_summary", "") or "").strip()
                            if content_to_save:
                                vec = AIService.get_embedding(content_to_save[:8000])
                                payload = {
                                    "story_id": project_id,
                                    "entity_name": f"[RULE] {datetime.now().strftime('%Y%m%d_%H%M%S')}",
                                    "description": content_to_save,
                                    "source_chapter": 0,
                                }
                                if vec:
                                    payload["embedding"] = vec
                                supabase.table("story_bible").insert(payload).execute()
                                st.toast("ƒê√£ ghi nh·ªõ / c·∫≠p nh·∫≠t v√†o Bible.")
                            del st.session_state["pending_update_confirm"]
                            st.rerun()
                        except Exception as e:
                            st.error(f"L·ªói khi ghi: {e}")
                with col_no:
                    if st.button("‚ùå H·ªßy", key=f"update_confirm_no_{chat_mode}"):
                        del st.session_state["pending_update_confirm"]
                        st.rerun()

    # Rule Mining UI (ch·ªâ V Work; danh s√°ch lu·∫≠t tr√≠ch t·ª´ 1 c√¢u chat, x√°c nh·∫≠n t·ª´ng c√°i ho·∫∑c t·∫•t c·∫£)
    if not is_v_home and can_write:
        if 'pending_new_rule' in st.session_state and 'pending_new_rules' not in st.session_state:
            st.session_state['pending_new_rules'] = [{"content": st.session_state['pending_new_rule'], "analysis": st.session_state.get('rule_analysis')}]
            del st.session_state['pending_new_rule']
            if 'rule_analysis' in st.session_state:
                del st.session_state['rule_analysis']
    if not is_v_home and 'pending_new_rules' in st.session_state and can_write:
        pending_list = st.session_state['pending_new_rules']
        if not isinstance(pending_list, list):
            pending_list = []

        with st.expander("üßê AI ph√°t hi·ªán lu·∫≠t t·ª´ chat", expanded=True):
            st.caption("Lu·∫≠t l∆∞u v√†o **Knowledge > Bible** (prefix [RULE]). X√°c nh·∫≠n t·ª´ng lu·∫≠t ho·∫∑c t·∫•t c·∫£.")
            for i, item in enumerate(pending_list):
                rule_content = item.get("content") or ""
                analysis = item.get("analysis")
                rule_key = f"rule_{i}_{chat_mode}"

                with st.container():
                    st.write(f"**Lu·∫≠t {i + 1}:** {rule_content[:200]}{'‚Ä¶' if len(rule_content) > 200 else ''}")
                    if analysis is None:
                        with st.spinner("ƒêang ki·ªÉm tra tr√πng..."):
                            item["analysis"] = RuleMiningSystem.analyze_rule_conflict(rule_content, project_id)
                            analysis = item["analysis"]
                    if analysis:
                        st.info(f"**{analysis.get('status', 'NEW')}** ‚Äî {analysis.get('reason', '')}")
                        similar_rules = analysis.get("similar_rules") or []
                        if similar_rules:
                            for sr in similar_rules:
                                pct = sr.get("similarity_pct", 0)
                                st.caption(f"‚ö†Ô∏è Nghi ng·ªù tr√πng ({pct}% gi·ªëng): _{sr.get('content', '')[:150]}‚Ä¶_")
                        if analysis.get("status") == "CONFLICT":
                            st.warning(f"Xung ƒë·ªôt v·ªõi: {analysis.get('existing_rule_summary', '')[:200]}")
                        elif analysis.get("status") == "MERGE":
                            st.info(f"üí° G·ª£i √Ω g·ªôp: { (analysis.get('merged_content') or '')[:200] }‚Ä¶")

                    col_a, col_b = st.columns(2)
                    with col_a:
                        if st.button("‚úÖ L∆∞u", key=f"rule_save_one_{rule_key}"):
                            final_content = (analysis.get('merged_content') if analysis and analysis.get('status') == "MERGE" else rule_content) or rule_content
                            vec = AIService.get_embedding(final_content)
                            services = init_services()
                            supabase = services.get("supabase")
                            if supabase:
                                payload = {"story_id": project_id, "entity_name": f"[RULE] {datetime.now().strftime('%Y%m%d_%H%M%S')}", "description": final_content, "embedding": vec, "source_chapter": 0}
                                try:
                                    supabase.table("story_bible").insert(payload).execute()
                                    st.toast("ƒê√£ l∆∞u lu·∫≠t.")
                                except Exception as e:
                                    st.error(str(e))
                            pending_list.pop(i)
                            if not pending_list:
                                del st.session_state['pending_new_rules']
                            st.rerun()
                    with col_b:
                        if st.button("‚ùå B·ªè qua", key=f"rule_ignore_one_{rule_key}"):
                            pending_list.pop(i)
                            if not pending_list:
                                del st.session_state['pending_new_rules']
                            st.rerun()
                    st.divider()

            if pending_list:
                col_all_a, col_all_b = st.columns(2)
                with col_all_a:
                    if st.button("‚úÖ L∆∞u t·∫•t c·∫£", key=f"rule_save_all_{chat_mode}"):
                        services = init_services()
                        supabase = services.get("supabase") if services else None
                        for item in pending_list:
                            rule_content = item.get("content") or ""
                            analysis = item.get("analysis")
                            final_content = (analysis.get('merged_content') if analysis and analysis.get('status') == "MERGE" else rule_content) or rule_content
                            vec = AIService.get_embedding(final_content)
                            if supabase:
                                try:
                                    supabase.table("story_bible").insert({
                                        "story_id": project_id, "entity_name": f"[RULE] {datetime.now().strftime('%Y%m%d_%H%M%S')}",
                                        "description": final_content, "embedding": vec, "source_chapter": 0
                                    }).execute()
                                except Exception:
                                    pass
                        st.toast("ƒê√£ l∆∞u t·∫•t c·∫£ lu·∫≠t.")
                        del st.session_state['pending_new_rules']
                        st.rerun()
                with col_all_b:
                    if st.button("‚ùå B·ªè qua t·∫•t c·∫£", key=f"rule_ignore_all_{chat_mode}"):
                        del st.session_state['pending_new_rules']
                        st.rerun()
