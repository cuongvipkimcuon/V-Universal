# views/data_analyze.py - Tab Data Analyze: ch·ªçn ch∆∞∆°ng, g·ª≠i t√°c v·ª• ch·∫°y ng·∫ßm (Extract Bible / Relation / Timeline / Chunk)
import json
import threading

import streamlit as st

from config import Config, init_services
from ai_engine import (
    AIService,
    analyze_split_strategy,
    execute_split_logic,
    suggest_relations,
    extract_timeline_events_from_content,
    _get_default_tool_model,
)
from utils.auth_manager import check_permission
from utils.cache_helpers import get_chapters_cached
from persona import PersonaSystem
from core.background_jobs import create_job, run_job_worker


def _get_existing_bible_entity_names_for_chapter(project_id, chap_num, supabase):
    """L·∫•y set entity_name (v√† t√™n kh√¥ng prefix) ƒë√£ c√≥ trong Bible cho ch∆∞∆°ng n√†y."""
    try:
        r = supabase.table("story_bible").select("entity_name").eq(
            "story_id", project_id
        ).eq("source_chapter", chap_num).execute()
        names = set()
        for row in (r.data or []):
            en = (row.get("entity_name") or "").strip()
            if en:
                names.add(en)
                # Th√™m ph·∫ßn sau prefix [XXX] ƒë·ªÉ match khi extract tr·∫£ v·ªÅ t√™n kh√¥ng prefix
                if en.startswith("[") and "]" in en:
                    rest = en[en.index("]") + 1:].strip()
                    if rest:
                        names.add(rest)
        return names
    except Exception:
        return set()


def _get_entity_ids_for_chapter(project_id, chap_num, supabase):
    """L·∫•y list id c√°c entity Bible c√≥ source_chapter = ch∆∞∆°ng n√†y."""
    try:
        r = supabase.table("story_bible").select("id").eq(
            "story_id", project_id
        ).eq("source_chapter", chap_num).execute()
        return [row["id"] for row in (r.data or []) if row.get("id")]
    except Exception:
        return []


def _run_extract_on_content(content, ext_persona, project_id, chap_num, exclude_existing=False, supabase=None):
    """Ch·∫°y extract Bible tr√™n content; n·∫øu exclude_existing th√¨ lo·∫°i item tr√πng v·ªõi Bible hi·ªán c√≥ c·ªßa ch∆∞∆°ng."""
    from ai_engine import AIService
    strategy = analyze_split_strategy(content, file_type="story", context_hint="")
    parts = execute_split_logic(content, strategy.get("split_type", "by_length"), strategy.get("split_value", "50000"))
    if not parts:
        parts = execute_split_logic(content, "by_length", "50000")
    MAX_CHARS = 55000
    chunks = []
    for p in parts:
        c = (p.get("content") or "").strip()
        if not c:
            continue
        if len(c) <= MAX_CHARS:
            chunks.append(c)
        else:
            for s in execute_split_logic(c, "by_length", "50000"):
                sc = (s.get("content") or "").strip()
                if sc:
                    chunks.append(sc)
    all_items = []
    allowed_keys = Config.get_allowed_prefix_keys_for_extract()
    prefix_list_str = ", ".join(allowed_keys) + ", OTHER" if allowed_keys else "OTHER"
    for i, chunk_content in enumerate(chunks):
        ext_prompt = f"""
N·ªòI DUNG (Ph·∫ßn {i+1}/{len(chunks)}):
{chunk_content}

NHI·ªÜM V·ª§: {ext_persona.get('extractor_prompt', 'Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ quan tr·ªçng t·ª´ n·ªôi dung tr√™n.')}

‚õîÔ∏è Y√äU C·∫¶U: Tr·∫£ v·ªÅ JSON v·ªõi key "items". Tr∆∞·ªùng "type" ph·∫£i l√† ƒë√∫ng M·ªòT trong: {prefix_list_str}. "description": t√≥m t·∫Øt d∆∞·ªõi 50 t·ª´.
N·∫øu kh√¥ng t√¨m th·∫•y: {{ "items": [] }}. Ch·ªâ tr·∫£ v·ªÅ JSON."""
        try:
            resp = AIService.call_openrouter(
                messages=[{"role": "user", "content": ext_prompt}],
                model=_get_default_tool_model(),
                temperature=0.0,
                max_tokens=16000,
                response_format={"type": "json_object"},
            )
            if resp and resp.choices:
                raw = resp.choices[0].message.content.strip()
                obj = json.loads(AIService.clean_json_text(raw))
                items_chunk = obj.get("items", []) if isinstance(obj, dict) else (obj if isinstance(obj, list) else [])
                all_items.extend(items_chunk)
        except Exception:
            pass
    if exclude_existing and supabase:
        existing = _get_existing_bible_entity_names_for_chapter(project_id, chap_num, supabase)
        def _norm(s):
            return (s or "").strip().lower()
        new_items = []
        for item in all_items:
            name = (item.get("entity_name") or "").strip()
            if not name:
                continue
            if _norm(name) in {_norm(n) for n in existing}:
                continue
            if name in existing:
                continue
            # Check without prefix
            if "]" in name and name.startswith("["):
                rest = name[name.index("]") + 1:].strip()
                if _norm(rest) in {_norm(n) for n in existing}:
                    continue
            new_items.append(item)
        return new_items
    unique_dict = {}
    for item in all_items:
        name = item.get("entity_name", "").strip()
        if name and (name not in unique_dict or len(item.get("description", "")) > len(unique_dict[name].get("description", ""))):
            unique_dict[name] = item
    return list(unique_dict.values())


def render_data_analyze_tab(project_id):
    if not project_id:
        st.info("üìÅ Vui l√≤ng ch·ªçn Project ·ªü thanh b√™n tr√°i.")
        return

    st.session_state.setdefault("update_trigger", 0)
    file_list = get_chapters_cached(project_id, st.session_state.get("update_trigger", 0))
    file_options = {}
    for f in file_list:
        display_name = f"üìÑ #{f['chapter_number']}: {f.get('title') or f'Chapter {f['chapter_number']}'}"
        file_options[display_name] = f["chapter_number"]

    if not file_list:
        st.info("Ch∆∞a c√≥ ch∆∞∆°ng n√†o. T·∫°o ch∆∞∆°ng trong Workstation tr∆∞·ªõc.")
        return

    services = init_services()
    if not services:
        st.warning("Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c d·ªãch v·ª•.")
        return
    supabase = services["supabase"]

    selected_file = st.selectbox(
        "Ch·ªçn ch∆∞∆°ng ƒë·ªÉ ph√¢n t√≠ch",
        list(file_options.keys()),
        key="da_chapter_select",
    )
    chap_num = file_options.get(selected_file, 1)
    res = supabase.table("chapters").select("*").eq("story_id", project_id).eq("chapter_number", chap_num).limit(1).execute()
    selected_row = res.data[0] if res.data and len(res.data) > 0 else None
    content = (selected_row.get("content") or "").strip() if selected_row else ""

    if not content:
        st.warning("Ch∆∞∆°ng n√†y ch∆∞a c√≥ n·ªôi dung. Th√™m n·ªôi dung trong Workstation.")
        st.stop()

    st.caption(f"N·ªôi dung ch∆∞∆°ng: {len(content)} k√Ω t·ª±.")

    _render_extract_bible_relations_chunking(
        project_id, content, chap_num, selected_row, file_options, selected_file, supabase
    )
    _render_timeline_section(project_id, content, chap_num, selected_row, supabase)

    st.session_state.setdefault("update_trigger", st.session_state.get("update_trigger", 0))


def _render_timeline_section(project_id, content, chap_num, selected_row, supabase):
    """Timeline: g·ª≠i job ch·∫°y ng·∫ßm; AI tr√≠ch xu·∫•t v√† l∆∞u tr·ª±c ti·∫øp. V Work th√¥ng b√°o khi xong."""
    st.markdown("---")
    st.subheader("üìÖ Timeline (tr√≠ch xu·∫•t t·ª´ ch∆∞∆°ng)")
    try:
        supabase.table("timeline_events").select("id").limit(1).execute()
    except Exception:
        st.warning("B·∫£ng timeline_events ch∆∞a t·ªìn t·∫°i. Ch·∫°y schema_v7_migration.sql tr√™n Supabase ƒë·ªÉ d√πng t√≠nh nƒÉng n√†y.")
        return
    chapter_label = selected_row.get("title") or f"Ch∆∞∆°ng {chap_num}"
    st.caption(f"Ch∆∞∆°ng: {chapter_label}. AI tr√≠ch xu·∫•t s·ª± ki·ªán v√† l∆∞u v√†o Timeline (x√≥a events c≈© c·ªßa ch∆∞∆°ng). Ch·∫°y ng·∫ßm.")
    st.checkbox(
        "‚ö†Ô∏è T√¥i hi·ªÉu: Tr√≠ch xu·∫•t Timeline s·∫Ω **x√≥a to√†n b·ªô** timeline_events ƒë√£ g·∫Øn v·ªõi ch∆∞∆°ng n√†y tr∆∞·ªõc khi l∆∞u m·ªõi.",
        key="da_confirm_delete_timeline_chapter",
    )
    uid = getattr(st.session_state.get("user"), "id", None) or ""
    uem = getattr(st.session_state.get("user"), "email", None) or ""
    can_write = check_permission(uid, uem, project_id, "write")
    if st.session_state.get("da_confirm_delete_timeline_chapter") and can_write:
        if st.button("ü§ñ AI tr√≠ch xu·∫•t timeline t·ª´ ch∆∞∆°ng n√†y", type="primary", key="da_timeline_extract_btn"):
            job_id = create_job(
                story_id=project_id,
                user_id=uid or None,
                job_type="data_analyze_timeline",
                label=f"Timeline ch∆∞∆°ng {chap_num}",
                payload={"chapter_number": chap_num, "chapter_label": chapter_label},
                post_to_chat=True,
            )
            if job_id:
                threading.Thread(target=run_job_worker, args=(job_id,), daemon=True).start()
                st.toast("ƒê√£ g·ª≠i v√†o h√†ng ƒë·ª£i. Xem tab T√°c v·ª• ng·∫ßm. V Work s·∫Ω th√¥ng b√°o khi xong.")
                st.session_state["update_trigger"] = st.session_state.get("update_trigger", 0) + 1
                st.rerun()
            else:
                st.error("Kh√¥ng t·∫°o ƒë∆∞·ª£c job.")


def _render_extract_bible_relations_chunking(project_id, content, chap_num, selected_row, file_options, selected_file, supabase):
    """N·ªôi dung tab Extract Bible / Relations / Chunking (gi·ªØ nguy√™n logic c≈©)."""
    # --- Section 1: Extract Bible ---
    st.markdown("---")
    st.subheader("üì• Extract Bible")
    personas_avail = PersonaSystem.get_available_personas()
    da_persona_key = st.selectbox("üé≠ Persona cho Extract", personas_avail, key="da_persona_select")
    ext_persona = PersonaSystem.get_persona(da_persona_key)

    st.checkbox(
        "‚ö†Ô∏è T√¥i hi·ªÉu: B·∫Øt ƒë·∫ßu ph√¢n t√≠ch s·∫Ω **x√≥a to√†n b·ªô** Bible entries ƒë√£ g·∫Øn v·ªõi ch∆∞∆°ng n√†y (source_chapter = ch∆∞∆°ng ƒëang ch·ªçn) tr∆∞·ªõc khi ch·∫°y extract.",
        key="da_confirm_delete_bible_chapter",
    )
    uid = getattr(st.session_state.get("user"), "id", None) or ""
    uem = getattr(st.session_state.get("user"), "email", None) or ""
    can_write = check_permission(uid, uem, project_id, "write")
    if st.session_state.get("da_confirm_delete_bible_chapter") and can_write:
        if st.button("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu ph√¢n t√≠ch", type="primary", key="da_extract_start_btn"):
            job_id = create_job(
                story_id=project_id,
                user_id=uid or None,
                job_type="data_analyze_bible",
                label=f"Extract Bible ch∆∞∆°ng {chap_num}",
                payload={"chapter_number": chap_num, "persona_key": da_persona_key, "exclude_existing": False},
                post_to_chat=True,
            )
            if job_id:
                threading.Thread(target=run_job_worker, args=(job_id,), daemon=True).start()
                st.toast("ƒê√£ g·ª≠i v√†o h√†ng ƒë·ª£i. Xem tab T√°c v·ª• ng·∫ßm. V Work s·∫Ω th√¥ng b√°o khi xong.")
                st.session_state["update_trigger"] = st.session_state.get("update_trigger", 0) + 1
                st.rerun()
            else:
                st.error("Kh√¥ng t·∫°o ƒë∆∞·ª£c job.")
    if can_write:
        if st.button("üîÑ C·∫≠p nh·∫≠t (ch·ªâ g·ª£i √Ω m·ªõi)", key="da_extract_update_btn"):
            job_id = create_job(
                story_id=project_id,
                user_id=uid or None,
                job_type="data_analyze_bible",
                label=f"Extract Bible ch∆∞∆°ng {chap_num} (ch·ªâ m·ªõi)",
                payload={"chapter_number": chap_num, "persona_key": da_persona_key, "exclude_existing": True},
                post_to_chat=True,
            )
            if job_id:
                threading.Thread(target=run_job_worker, args=(job_id,), daemon=True).start()
                st.toast("ƒê√£ g·ª≠i v√†o h√†ng ƒë·ª£i. Xem tab T√°c v·ª• ng·∫ßm. V Work s·∫Ω th√¥ng b√°o khi xong.")
                st.rerun()
            else:
                st.error("Kh√¥ng t·∫°o ƒë∆∞·ª£c job.")
    if not can_write:
        st.warning("Ch·ªâ th√†nh vi√™n c√≥ quy·ªÅn ghi m·ªõi ƒë∆∞·ª£c th·ª±c hi·ªán.")

    # --- Section 2: Relation ---
    st.markdown("---")
    st.subheader("üîó Relation")
    st.info("üí° N√™n th·ª±c hi·ªán Extract Bible tr∆∞·ªõc ƒë·ªÉ g·ª£i √Ω relation ch√≠nh x√°c. T√°c v·ª• ch·∫°y ng·∫ßm; xem tab T√°c v·ª• ng·∫ßm.")
    st.checkbox(
        "‚ö†Ô∏è T√¥i hi·ªÉu: G·ª£i √Ω quan h·ªá s·∫Ω **x√≥a c√°c quan h·ªá** gi·ªØa c√°c th·ª±c th·ªÉ thu·ªôc ch∆∞∆°ng n√†y tr∆∞·ªõc khi g·ª£i √Ω l·∫°i.",
        key="da_confirm_delete_relation_chapter",
    )
    if st.session_state.get("da_confirm_delete_relation_chapter") and can_write:
        if st.button("üîÑ G·ª£i √Ω quan h·ªá t·ª´ n·ªôi dung ch∆∞∆°ng", key="da_suggest_relations"):
            job_id = create_job(
                story_id=project_id,
                user_id=uid or None,
                job_type="data_analyze_relation",
                label=f"G·ª£i √Ω quan h·ªá ch∆∞∆°ng {chap_num}",
                payload={"chapter_number": chap_num, "only_new": False},
                post_to_chat=True,
            )
            if job_id:
                threading.Thread(target=run_job_worker, args=(job_id,), daemon=True).start()
                st.toast("ƒê√£ g·ª≠i v√†o h√†ng ƒë·ª£i. Xem tab T√°c v·ª• ng·∫ßm. V Work s·∫Ω th√¥ng b√°o khi xong.")
                st.rerun()
            else:
                st.error("Kh√¥ng t·∫°o ƒë∆∞·ª£c job.")
    if can_write:
        if st.button("üîÑ C·∫≠p nh·∫≠t (ch·ªâ g·ª£i √Ω quan h·ªá m·ªõi)", key="da_relation_update_btn"):
            job_id = create_job(
                story_id=project_id,
                user_id=uid or None,
                job_type="data_analyze_relation",
                label=f"C·∫≠p nh·∫≠t quan h·ªá ch∆∞∆°ng {chap_num} (ch·ªâ m·ªõi)",
                payload={"chapter_number": chap_num, "only_new": True},
                post_to_chat=True,
            )
            if job_id:
                threading.Thread(target=run_job_worker, args=(job_id,), daemon=True).start()
                st.toast("ƒê√£ g·ª≠i v√†o h√†ng ƒë·ª£i. Xem tab T√°c v·ª• ng·∫ßm. V Work s·∫Ω th√¥ng b√°o khi xong.")
                st.rerun()
            else:
                st.error("Kh√¥ng t·∫°o ƒë∆∞·ª£c job.")

    # --- Section 3: Chunking ---
    st.markdown("---")
    st.subheader("‚úÇÔ∏è Chunking")
    st.caption("Chunks t·ª´ ch∆∞∆°ng ƒë∆∞·ª£c g·∫Øn chapter_id + arc_id, meta_json.source = data_analyze. L∆∞u m·ªõi s·∫Ω x√≥a chunks c≈© c·ªßa ch∆∞∆°ng. Ch·∫°y ng·∫ßm.")
    st.checkbox(
        "‚ö†Ô∏è T√¥i hi·ªÉu: Ph√¢n t√≠ch Chunk s·∫Ω **x√≥a to√†n b·ªô** chunks ƒë√£ g·∫Øn v·ªõi ch∆∞∆°ng n√†y tr∆∞·ªõc khi l∆∞u m·ªõi.",
        key="da_confirm_delete_chunks_chapter",
    )
    if st.session_state.get("da_confirm_delete_chunks_chapter") and can_write:
        if st.button("üìÑ Ph√¢n t√≠ch Chunk", type="primary", key="da_chunk_analyze"):
            job_id = create_job(
                story_id=project_id,
                user_id=uid or None,
                job_type="data_analyze_chunk",
                label=f"Ph√¢n t√≠ch Chunk ch∆∞∆°ng {chap_num}",
                payload={"chapter_number": chap_num},
                post_to_chat=True,
            )
            if job_id:
                threading.Thread(target=run_job_worker, args=(job_id,), daemon=True).start()
                st.toast("ƒê√£ g·ª≠i v√†o h√†ng ƒë·ª£i. Xem tab T√°c v·ª• ng·∫ßm. V Work s·∫Ω th√¥ng b√°o khi xong.")
                st.session_state["update_trigger"] = st.session_state.get("update_trigger", 0) + 1
                st.rerun()
            else:
                st.error("Kh√¥ng t·∫°o ƒë∆∞·ª£c job.")
